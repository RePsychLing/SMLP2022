[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMLP2022 - Advanced frequentist methods",
    "section": "",
    "text": "Useful packages\nSleepstudy\nSleepstudy2\nArrow\nModel-building\nGLMM\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, but one further motivation is that Julia is at a broader ‚Äútechnical computing‚Äù audience (like MATLAB or perhaps Python) and less at a ‚Äústatistical analysis‚Äù audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. Pluto helpfully installs and manages for you, but the package-manager REPL mode (activated by typing ] at the julia> prompt) is very useful."
  },
  {
    "objectID": "useful_packages.html#data-wrangling",
    "href": "useful_packages.html#data-wrangling",
    "title": "Useful packages",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nReading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather ‚Äúcolumn tables‚Äù ‚Äì named tuples of column vectors.\n\n\nDataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R‚Äôs dplyr and much of the tidyverse. The DataFrames.jl documentation is the place to for looking at how to e.g.¬†read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations.\nDataFrame.jl‚Äôs mini language can be a bit daunting, if you‚Äôre used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we‚Äôe had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or ‚Äúpipe‚Äù together successive operations. It‚Äôs your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl."
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "Regression",
    "text": "Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don‚Äôt have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e.¬†the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia > lm(@formula(y ~ 1 + x), data)\njulia > glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia > fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia > fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don‚Äôt recommend this as it‚Äôs easy to mess up in really subtle but very probelmatic ways.)\n\n@formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that‚Äôs because formulas are essentially their own domain-specific language (a variant of Wilkinson-Roger notation) and macros are used for manipulating the language itself ‚Äì or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.\n\n\nExtensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the ‚Äúextras‚Äù available in R, e.g.¬†RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.\n\n\nStandardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e.¬†on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g.¬†centering new data around the mean computed during fitting the original data) correctly and automatically.\n\n\nEffects\nJohn Fox‚Äôs effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model‚Äôs overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we‚Äôre working on it!\n\n\nEstimated Marginal / Least Square Means\nTo our knowledge, There is currently nothing like the R package emmeans package in Julia. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. üòÉ"
  },
  {
    "objectID": "useful_packages.html#hypothesis-testing",
    "href": "useful_packages.html#hypothesis-testing",
    "title": "Useful packages",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nClassical statistical tests such as the t-test can be found in the package HypothesisTests.jl."
  },
  {
    "objectID": "useful_packages.html#plotting-ecosystem",
    "href": "useful_packages.html#plotting-ecosystem",
    "title": "Useful packages",
    "section": "Plotting ecosystem",
    "text": "Plotting ecosystem\nThroughtout this course, we have used the Makie ecosystem for plotting, but there are several alternatives in Julia.\n\nMakie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g.¬†MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Pluto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It‚Äôs a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly.\n\n\nPlots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX‚Äôs tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.\n\n\nGadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the ‚Äúgg‚Äù in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).\n\n\nOthers\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite."
  },
  {
    "objectID": "useful_packages.html#connecting-to-other-languages",
    "href": "useful_packages.html#connecting-to-other-languages",
    "title": "Useful packages",
    "section": "Connecting to Other Languages",
    "text": "Connecting to Other Languages\nUsing Julia doesn‚Äôt mean you have to leave all the packages you knew in other languages behind. In Julia, it‚Äôs often possible to even easily and quickly invoke code from other languages from within Julia.\nRCall.jl provides a very convenient interface for interacting with R. JellyMe4.jl add support for moving MixedModels.jl and lme4 models back and forth between the languages (which means that you can use emmeans, sjtools, DHARMa, car, etc. to examine MixedModels.jl models!). RData.jl provides support for reading .rds and .rda files from Julia, while RDatasets.jl provides convenient access to many of the standard datasets provided by R and various R packages.\nPyCall.jl provides a very convenient way for interacting with Python code and packages. PyPlot.jl builds upon this foundation to provide support for Python‚Äôs matplotlib. Similarly, PyMNE.jl and PyFOOOF.jl provide some additional functionality to make interacting with MNE-Python and FOOOF from within Julia even easier than with vanilla PyCall.\nFor MATLAB users, there is also MATLAB.jl\nCxx.jl provides interoperability with C++. It also provides a C++ REPL mode, making it possible to treating C++ much more like a dynamic language than the traditional compiler toolchain would allow.\nSupport for calling C and Fortran is part of the Julia standard library."
  },
  {
    "objectID": "useful_packages.html#notebooks-and-webpages",
    "href": "useful_packages.html#notebooks-and-webpages",
    "title": "Useful packages",
    "section": "Notebooks and Webpages",
    "text": "Notebooks and Webpages\nWe haved Pluto.jl throughout this course because it provides a nice, easy way to interact with Julia, including automatic package management. PlutoUI.jl provides some nice tools for interacting with Pluto notebooks and making them more dynamic (including the TableOfContents() in this one!). We also like that Pluto notebooks are essentially normal Julia code stored in plain text under the hood, which means that they play quite nice with version control systems such as Git. Sometimes, however, the reactive nature of Pluto (update any one cell and the changes propogate to all impacted cells) is not desireable.\nAn alternative notebook interface is provide by Jupyter, formerly IPython. Indeed, the ‚ÄúJu‚Äù actually stands for Julia. Jupyter support is provided by the package IJulia.jl.\nThe Weave.jl package provides ‚ÄúJulia markdown‚Äù, which is similar to knitr/RMarkdown or the the Sweave formats in R. Notably, Weave also provides support for converting between jmd files and Jupyter notebooks.\nSimilarly, Literate.jl is a simple package for literate programming (i.e.¬†programming where documentation and code are ‚Äúwoven‚Äù together) and can generate Markdown, plain code and Jupyter notebook output. It is designed for simplicity and speed and is less extensive than Weave.jl and Documenter.jl.\nDocumenter.jl is the standard tool for building webpages from Julia documentation\nBooks.jl is a package designed to offer somewhat similar functionality to the bookdown package in R."
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule."
  },
  {
    "objectID": "sleepstudy.html#loading-the-data",
    "href": "sleepstudy.html#loading-the-data",
    "title": "Analysis of the sleepstudy data",
    "section": "Loading the data",
    "text": "Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrameMacros  # simplified dplyr-like data wrangling \nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\n\nProgressMeter.ijulia_behavior(:clear);\nCairoMakie.activate!(; type=\"svg\");\n\nThe sleepstudy data are one of the datasets available with the MixedModels package.\n\nsleepstudy = MixedModels.dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nFigure¬†1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nprint(xyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n))\n\"\"\";\n\n\n\n\n\nFigure 1: Average response time versus days of sleep deprivation by subject\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject‚Äôs data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic."
  },
  {
    "objectID": "sleepstudy.html#fitting-an-initial-model",
    "href": "sleepstudy.html#fitting-an-initial-model",
    "title": "Analysis of the sleepstudy data",
    "section": "Fitting an initial model",
    "text": "Fitting an initial model\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\nMinimizing 58    Time: 0:00:00 ( 6.51 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not ‚Äúparameters‚Äù in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1)\n\n\n\n\n\nFigure 2: Prediction intervals on random effects for model m1\n\n\n\n\nFigure¬†2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope."
  },
  {
    "objectID": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "href": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "title": "Analysis of the sleepstudy data",
    "section": "A model with uncorrelated random effects",
    "text": "A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\nm2 = let\n  form = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\nAgain, the default display doesn‚Äôt show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\nVarCorr(m2)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nreaction ~ 1 + days + MixedModels.ZeroCorr((1 + days | subj))\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  DataFrame(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ‚àò leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows √ó 6 columnsmodelparsgeomdofAICBICAICcSymbolInt64Float64Float64Float64Float641m2529.0451762.01777.971762.352m1628.61151763.941783.11764.42\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a ‚Äúsmaller is better‚Äù scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of ‚Äúdegrees of freedom‚Äù, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We‚Äôre not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor."
  },
  {
    "objectID": "sleepstudy.html#some-diagnostic-plots",
    "href": "sleepstudy.html#some-diagnostic-plots",
    "title": "Analysis of the sleepstudy data",
    "section": "Some diagnostic plots",
    "text": "Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure¬†3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\nFigure 3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)"
  },
  {
    "objectID": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "href": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "title": "Analysis of the sleepstudy data",
    "section": "Assessing variability by bootstrapping",
    "text": "Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1; hide_progress=true)\nallpars = DataFrame(m1bstp.allpars)\n\n\n30,000 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)260.71221Œ≤missingdays9.8497531œÉsubj(Intercept)15.331741œÉsubjdays6.4029651œÅsubj(Intercept), days-0.025958261œÉresidualmissing23.409272Œ≤missing(Intercept)262.25382Œ≤missingdays12.300892œÉsubj(Intercept)16.3183102œÉsubjdays5.54688112œÅsubj(Intercept), days0.552607122œÉresidualmissing25.7047133Œ≤missing(Intercept)253.149143Œ≤missingdays12.879153œÉsubj(Intercept)25.4787163œÉsubjdays6.1444173œÅsubj(Intercept), days0.0691545183œÉresidualmissing22.2753194Œ≤missing(Intercept)263.376204Œ≤missingdays11.5798214œÉsubj(Intercept)18.8039224œÉsubjdays4.6557234œÅsubj(Intercept), days0.103361244œÉresidualmissing23.3128255Œ≤missing(Intercept)248.429265Œ≤missingdays9.39444275œÉsubj(Intercept)20.1411285œÉsubjdays5.27358295œÅsubj(Intercept), days-0.163603305œÉresidualmissing25.4355‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure¬†4, shows the normal distribution, ‚Äúbell-curve‚Äù, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"days\").value,\n  )\n  f1\nend\n\n\n\n\n\nFigure 4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-Œ± shortestcovint to be the shortest interval that contains a proportion 1-Œ± (defaults to 95%) of the bootstrap estimates of the parameter.\n\nDataFrame(shortestcovint(m1bstp))\n\n\n6 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)239.64265.2282Œ≤missingdays7.4234713.16073œÉsubj(Intercept)10.172233.08764œÉsubjdays2.994767.661215œÅsubj(Intercept), days-0.4013531.06œÉresidualmissing22.70128.5016\n\n\nThe intervals look reasonable except that the upper bound on the interval for œÅ, the correlation coefficient, is 1.0 . It turns out that the estimates of œÅ have a great deal of variability.\nEven more alarming, some of these œÅ values are undefined (denoted NaN) because the way œÅ is calculated can create a division by zero.\n\ndescribe(@select(@subset(allpars, :type == \"œÅ\"), :value))\n\n\n1 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolFloat64Float64NothingFloat64Int64DataType1valueNaNNaNNaN0Float64\n\n\nBecause there are several values on the boundary (œÅ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure¬†5.\n\n\nCode\nhist(\n  @subset(allpars, :type == \"œÅ\", isfinite(:value)).value;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\nFigure 5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure¬†6, show reasonable symmetry.\n\n\nCode\nbegin\n  œÉs = @subset(allpars, :type == \"œÉ\")\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual œÉ\"),\n    @subset(œÉs, :group == \"residual\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"days\").value,\n  )\n  f2\nend\n\n\n\n\n\nFigure 6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, Œ≤‚ÇÅ and Œ≤‚ÇÇ, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure¬†7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2√ó2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  vals = disallowmissing(\n    Array(\n      select(\n        unstack(DataFrame(m1bstp.Œ≤), :iter, :coefname, :Œ≤),\n        Not(:iter),\n      ),\n    ),\n  )\n  scatter(\n    vals;\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde(vals))\n  current_figure()\nend\n\n\n\n\n\nFigure 7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours"
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ‚ÅÑ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\nDesign\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\nTest schedule within days\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\nStatistical analyses\nThe authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\nCurrent data\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#setup",
    "href": "sleepstudy_speed.html#setup",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Setup",
    "text": "Setup\nFirst we attach the various packages needed, define a few helper functions, read the data, and get everything in the desired shape.\n\nCode\nusing CairoMakie         # device driver for static (SVG, PDF, PNG) plots\nusing Chain              # like pipes but cleaner\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie   # plots specific to mixed-effects models using Makie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "sleepstudy_speed.html#preprocessing",
    "href": "sleepstudy_speed.html#preprocessing",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe sleepstudy data are one of the datasets available with recent versions of the MixedModels package. We carry out some preprocessing to have the dataframe in the desired shape:\n\nCapitalize random factor Subj\nCompute speed as an alternative dependent variable from reaction, warranted by a ‚Äòboxcox‚Äô check of residuals.\nCreate a GroupedDataFrame by levels of Subj (the original dataframe is available as gdf.parent, which we name df)\n\n\ngdf = @chain MixedModels.dataset(:sleepstudy) begin\n  DataFrame\n  rename!(:subj => :Subj, :days => :day)\n  @transform!(:speed = 1000 / :reaction)\n  groupby(:Subj)\nend\n\n\nGroupedDataFrame with 18 groups based on key: SubjFirst Group (10 rows): Subj = \"S308\"SubjdayreactionspeedStringInt8Float64Float641S3080249.564.007052S3081258.7053.865413S3082250.8013.987234S3083321.443.1115S3084356.8522.802286S3085414.692.411447S3086382.2042.616418S3087290.1493.446519S3088430.5852.3224210S3089466.3532.1443‚ãÆLast Group (10 rows): Subj = \"S372\"SubjdayreactionspeedStringInt8Float64Float641S3720269.4123.711792S3721273.4743.656653S3722297.5973.360254S3723310.6323.219255S3724287.1733.482236S3725329.6083.033917S3726334.4822.98978S3727343.222.913589S3728369.1422.7089910S3729364.1242.74632\n\n\n\ndf = gdf.parent\ndescribe(df)\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS308S3720String2day4.504.590Int83reaction298.508194.332288.651466.3530Float644speed3.466342.14433.464435.145830Float64"
  },
  {
    "objectID": "sleepstudy_speed.html#estimates-for-pooled-data",
    "href": "sleepstudy_speed.html#estimates-for-pooled-data",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Estimates for pooled data",
    "text": "Estimates for pooled data\nIn the first analysis we ignore the dependency of observations due to repeated measures from the same subjects. We pool all the data and estimate the regression of 180 speed scores on the nine days of the experiment.\n\npooledcoef = simplelinreg(df.day, df.speed)  # produces a Tuple\n\n(3.96581197478315, -0.11099359232199725)"
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-effects",
    "href": "sleepstudy_speed.html#within-subject-effects",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Within-subject effects",
    "text": "Within-subject effects\nIn the second analysis we estimate coefficients for each Subj without regard of the information available from the complete set of data. We do not ‚Äúborrow strength‚Äù to adjust for differences due to between-Subj variability and due to being far from the population mean.\n\nWithin-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] => simplelinreg => :coef)\n\n\n18 rows √ó 2 columnsSubjcoefStringTuple‚Ä¶1S308(3.94806, -0.194812)2S309(4.87022, -0.0475185)3S310(4.90606, -0.120054)4S330(3.4449, -0.0291309)5S331(3.47647, -0.0498047)6S332(3.84436, -0.105511)7S333(3.60159, -0.0917378)8S334(4.04528, -0.133527)9S335(3.80451, 0.0455771)10S337(3.34374, -0.137744)11S349(4.46855, -0.170885)12S350(4.21414, -0.20151)13S351(3.80469, -0.0728582)14S352(3.68634, -0.144957)15S369(3.85384, -0.120531)16S370(4.52679, -0.215965)17S371(3.853, -0.0936243)18S372(3.69208, -0.113292)\n\n\nFigure¬†1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n‚îå Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n‚îÇ   caller = top-level scope at In[7]:8\n‚îî @ Core ./In[7]:8\n\n\n\n\n\nFigure 1: Reaction speed (s‚Åª¬π) versus days of sleep deprivation by subject"
  },
  {
    "objectID": "sleepstudy_speed.html#basic-lmm",
    "href": "sleepstudy_speed.html#basic-lmm",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Basic LMM",
    "text": "Basic LMM\n\ncontrasts = Dict(:Subj => Grouping())\nm1 = let\n  form = @formula speed ~ 1 + day + (1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\nMinimizing 70    Time: 0:00:00 ( 4.77 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1056\n37.55\n<1e-99\n0.4190\n\n\nday\n-0.1110\n0.0151\n-7.37\n<1e-12\n0.0566\n\n\nResidual\n0.2698\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept which estimates the average speed on the baseline day of the experiment prior to sleep deprivation, and the slowing per day of sleep deprivation. In this case about -0.11/second.\nThe random effects represent shifts from the typical behavior for each subject.The shift in the intercept has a standard deviation of about 0.42/s.\nThe within-subject correlation of the random effects for intercept and slope is small, -0.18, indicating that a simpler model with a correlation parameter (CP) forced to/ assumed to be zero may be sufficient."
  },
  {
    "objectID": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "href": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "No correlation parameter: zcp LMM",
    "text": "No correlation parameter: zcp LMM\nThe zerocorr function applied to a random-effects term estimates one parameter less than LMM m1‚Äì the CP is now fixed to zero.\nm2 = let\n  form = @formula speed ~ 1 + day + zerocorr(1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1033\n38.38\n<1e-99\n0.4085\n\n\nday\n-0.1110\n0.0147\n-7.53\n<1e-13\n0.0550\n\n\nResidual\n0.2706\n\n\n\n\n\n\n\nLMM m2 has a slghtly lower log-likelihood than LMM m1 but also one fewer parameters. A likelihood-ratio test is used to compare these nested models.\n\nCode\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nspeed ~ 1 + day + MixedModels.ZeroCorr((1 + day | Subj))\n5\n125\n\n\n\n\n\nspeed ~ 1 + day + (1 + day | Subj)\n6\n125\n0\n1\n0.5192\n\n\n\nAlternatively, the AIC, AICc, and BIC values can be compared. They are on a scale where ‚Äúsmaller is better‚Äù. All three model-fit statistics prefer the zcpLMM m2.\n\n\nCode\nlet\n  mods = [m2, m1]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n\n2 rows √ó 5 columnsdofdevianceAICAICcBICInt64Float64Float64Float64Float6415125.379135.379135.724151.34426124.964136.964137.45156.122"
  },
  {
    "objectID": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "href": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Conditional modes of the random effects",
    "text": "Conditional modes of the random effects\nThe third set of estimates are their conditional modes. They represent a compromise between their own data and the model parameters. When distributional assumptions hold, predictions based on these estimates are more accurate than either the pooled or the within-subject estimates. Here we ‚Äúborrow strength‚Äù to improve the accuracy of prediction."
  },
  {
    "objectID": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "href": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Caterpillar plots (effect profiles)",
    "text": "Caterpillar plots (effect profiles)\n\n\nCode\ncaterpillar(m2)\n\n\n\n\n\nFigure 2: Prediction intervals on the random effects in model m2"
  },
  {
    "objectID": "sleepstudy_speed.html#shrinkage-plot",
    "href": "sleepstudy_speed.html#shrinkage-plot",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m2)\n\n\n\n\n\nFigure 3: Shrinkage plot of the means of the random effects in model m2"
  },
  {
    "objectID": "Arrow.html",
    "href": "Arrow.html",
    "title": "Notes on saved data files",
    "section": "",
    "text": "The Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means ‚Äúsomething that looks like a data frame in R‚Äù. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather."
  },
  {
    "objectID": "Arrow.html#the-emotikon-data",
    "href": "Arrow.html#the-emotikon-data",
    "title": "Notes on saved data files",
    "section": "The Emotikon data",
    "text": "The Emotikon data\nThe SMLP2021 repository contains a version of the data from F√ºhner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing PyCall  # show reading Arrow in Python\nusing RCall   # show reading Arrow in R\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density"
  },
  {
    "objectID": "Arrow.html#downloading-and-importing-the-rds-file",
    "href": "Arrow.html#downloading-and-importing-the-rds-file",
    "title": "Notes on saved data files",
    "section": "Downloading and importing the RDS file",
    "text": "Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\ndfrm = rcopy(R\"readRDS($fn)\")\n\n\n525,126 rows √ó 7 columnsCohortSchoolChildSexageTestscoreCat‚Ä¶Cat‚Ä¶Cat‚Ä¶Cat‚Ä¶Float64Cat‚Ä¶Float6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3077850\n\n\n\ndf = DataFrame(tbl)\n\n\n525,126 rows √ó 7 columnsCohortSchoolChildSexageTestscoreStringStringStringStringFloat64StringFloat6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ"
  },
  {
    "objectID": "Arrow.html#avoiding-needless-repetition",
    "href": "Arrow.html#avoiding-needless-repetition",
    "title": "Notes on saved data files",
    "section": "Avoiding needless repetition",
    "text": "Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design."
  },
  {
    "objectID": "Arrow.html#creating-the-smaller-table",
    "href": "Arrow.html#creating-the-smaller-table",
    "title": "Notes on saved data files",
    "section": "Creating the smaller table",
    "text": "Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n\n108,295 rows √ó 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1774946\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794058\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n\n525,126 rows √ó 3 columnsChildTestscoreStringStringFloat641C002352S20_r5.263162C002352BPT3.73C002352SLJ125.04C002352Star_r2.471465C002352Run1053.06C002353S20_r5.07C002353BPT4.18C002353SLJ116.09C002353Star_r1.7677810C002353Run1089.011C002354S20_r4.5454512C002354BPT3.913C002354SLJ111.014C002354Star_r1.9887515C002354Run864.016C002355S20_r4.5454517C002355BPT3.018C002355SLJ114.019C002355Star_r1.8446420C002355Run835.021C002356S20_r4.3478322C002356BPT3.323C002356SLJ118.024C002356Star_r1.9068225C002356Run860.026C002357S20_r4.3478327C002357BPT4.328C002357SLJ130.029C002357Star_r1.9965530C002357Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius‚Äôs presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @c zscore(:score))\n\n\n525,126 rows √ó 4 columnsChildTestscorezScoreStringStringFloat64Float641C002352S20_r5.263161.79132C002352BPT3.7-0.06223173C002352SLJ125.0-0.03365674C002352Star_r2.471461.468745C002352Run1053.00.3310586C002353S20_r5.01.154717C002353BPT4.10.4983548C002353SLJ116.0-0.4988229C002353Star_r1.76778-0.977310C002353Run1089.00.57405611C002354S20_r4.545450.055148112C002354BPT3.90.21806113C002354SLJ111.0-0.75724814C002354Star_r1.98875-0.20918615C002354Run864.0-0.94468116C002355S20_r4.545450.055148117C002355BPT3.0-1.0432618C002355SLJ114.0-0.60219319C002355Star_r1.84464-0.7101320C002355Run835.0-1.1404321C002356S20_r4.34783-0.42292122C002356BPT3.3-0.62281723C002356SLJ118.0-0.39545224C002356Star_r1.90682-0.49399225C002356Run860.0-0.9716826C002357S20_r4.34783-0.42292127C002357BPT4.30.77864628C002357SLJ130.00.22476929C002357Star_r1.99655-0.18207630C002357Run960.0-0.296686‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n\n108,295 rows √ó 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n\n525,126 rows √ó 8 columnsChildTestscorezScoreSchoolCohortSexageStringStringFloat64Float64StringStringStringFloat641C002352S20_r5.263161.7913S1000672013male7.994522C002352BPT3.7-0.0622317S1000672013male7.994523C002352SLJ125.0-0.0336567S1000672013male7.994524C002352Star_r2.471461.46874S1000672013male7.994525C002352Run1053.00.331058S1000672013male7.994526C002353S20_r5.01.15471S1000672013male7.994527C002353BPT4.10.498354S1000672013male7.994528C002353SLJ116.0-0.498822S1000672013male7.994529C002353Star_r1.76778-0.9773S1000672013male7.9945210C002353Run1089.00.574056S1000672013male7.9945211C002354S20_r4.545450.0551481S1000672013male7.9945212C002354BPT3.90.218061S1000672013male7.9945213C002354SLJ111.0-0.757248S1000672013male7.9945214C002354Star_r1.98875-0.209186S1000672013male7.9945215C002354Run864.0-0.944681S1000672013male7.9945216C002355S20_r4.545450.0551481S1001222013female7.9945217C002355BPT3.0-1.04326S1001222013female7.9945218C002355SLJ114.0-0.602193S1001222013female7.9945219C002355Star_r1.84464-0.71013S1001222013female7.9945220C002355Run835.0-1.14043S1001222013female7.9945221C002356S20_r4.34783-0.422921S1001462013male7.9945222C002356BPT3.3-0.622817S1001462013male7.9945223C002356SLJ118.0-0.395452S1001462013male7.9945224C002356Star_r1.90682-0.493992S1001462013male7.9945225C002356Run860.0-0.97168S1001462013male7.9945226C002357S20_r4.34783-0.422921S1001462013male7.9945227C002357BPT4.30.778646S1001462013male7.9945228C002357SLJ130.00.224769S1001462013male7.9945229C002357Star_r1.99655-0.182076S1001462013male7.9945230C002357Run960.0-0.296686S1001462013male7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created."
  },
  {
    "objectID": "Arrow.html#discovering-patterns-in-the-data",
    "href": "Arrow.html#discovering-patterns-in-the-data",
    "title": "Notes on saved data files",
    "section": "Discovering patterns in the data",
    "text": "Discovering patterns in the data\nOne of the motivations for creating the Child table was be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow => :ntest)\n\n\n108,295 rows √ó 2 columnsChildntestStringInt641C00235252C00235353C00235454C00235555C00235656C00235757C00235858C00235949C002360510C002361411C002362512C002363513C002364514C002365515C002366516C002367517C002368518C002369519C002370520C002371421C002372522C002373523C002374524C002375525C002376526C002377527C002378528C002379529C002380530C0023815‚ãÆ‚ãÆ‚ãÆ\n\n\nNow create a table of the number of children with 1, 2, ‚Ä¶, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n\n5 rows √ó 2 columnsntestnrowInt64Int6411462227293317394488365596529\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002452S1011752013male7.9945212C002625S1033292013male7.9945213C002754S1048142013female7.9945214C003269S1022582012female7.9972615C003599S1058432012female7.9972616C003807S1007542011male8.017C003985S1029452011male8.018C004086S1042552011male8.019C004657S1014002014male8.03833110C005036S1059092014male8.03833111C005440S1010232019male8.05202112C005523S1018252019female8.05202113C005697S1036152019male8.05202114C005759S1046322019female8.05202115C005810S1049542019female8.05202116C005835S1050532019male8.05202117C005854S1054052019male8.05202118C006550S1033292013male8.0794119C006760S1051812013female8.0794120C007031S1132442013male8.0794121C007050S1001952012female8.08214122C007305S1023502012male8.08214123C007828S1114052012female8.08214124C008698S1049172016female8.09309125C008707S1022712016male8.09582126C009596S1034212014female8.1232127C009651S1037062014female8.1232128C009879S1059092014female8.1232129C010203S1026602016male8.12594130C010204S1026602016male8.125941‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆLast Group (96529 rows): ntest = 5ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002360S1001952013female7.9945259C002362S1002372013female7.99452510C002363S1002372013female7.99452511C002364S1002502013female7.99452512C002365S1003042013male7.99452513C002366S1003042013male7.99452514C002367S1003162013female7.99452515C002368S1003652013male7.99452516C002369S1003652013male7.99452517C002370S1003652013female7.99452518C002372S1004322013male7.99452519C002373S1004812013male7.99452520C002374S1004812013male7.99452521C002375S1004812013female7.99452522C002376S1004932013female7.99452523C002377S1004932013female7.99452524C002378S1005472013male7.99452525C002379S1005472013male7.99452526C002380S1005472013male7.99452527C002381S1005472013female7.99452528C002382S1005472013female7.99452529C002383S1005842013female7.99452530C002384S1005962013male7.994525‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow => :nchild)\n\n\n2 rows √ó 2 columnsSexnchildStringInt641male2302female232\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow => :nchild)\n\n\n2 rows √ó 2 columnsSexnchildStringInt641male475522female48977\n\n\nWhat about the distribution of ages?\n\"\"\"\n    ridgeplot!(ax::Axis, df::AbstractDataFrame, densvar::Symbol, group::Symbol; normalize=false)\n    ridgeplot!(f::Figure, args...; pos=(1,1) kwargs...)\n    ridgeplot(args...; kwargs...)\nCreate a \"ridge plot\".\nA ridge plot is stacked plot of densities for a given variable (`densvar`) grouped by a different variable (`group`). Because densities can very widely in scale, it is sometimes useful to `normalize` the densities so that each density has a maximum of 1.\nThe non-mutating method creates a Figure before calling the method for Figure.\nThe method for Figure places the ridge plot in the grid position specified by `pos`, default is (1,1).\n\"\"\"\nfunction ridgeplot!(\n  ax::Axis,\n  df::AbstractDataFrame,\n  densvar::Symbol,\n  group::Symbol;\n  normalize=false,\n)\n  # `normalize` makes it so that the max density is always 1\n  # `normalize` works on the density not the area/mass\n  gdf = groupby(df, group)\n  dens = combine(gdf, densvar => kde => :kde)\n  sort!(dens, group)\n  spacing = normalize ? 1.0 : 0.9 * maximum(dens[!, :kde]) do val\n    return maximum(val.density)\n  end\n\n  nticks = length(gdf)\n\n  for (idx, row) in enumerate(eachrow(dens))\n    dd = if normalize\n      row.kde.density ./ maximum(row.kde.density)\n    else\n      row.kde.density\n    end\n\n    offset = idx * spacing\n\n    lower = Node(Point2f.(row.kde.x, offset))\n    upper = Node(Point2f.(row.kde.x, dd .+ offset))\n    band!(ax, lower, upper; color=(:black, 0.3))\n    lines!(ax, upper; color=(:black, 1.0))\n  end\n\n  ax.yticks[] = (\n    1:spacing:(nticks * spacing), string.(dens[!, group])\n  )\n  ylims!(ax, 0, (nticks + 2) * spacing)\n  ax.xlabel[] = string(densvar)\n  ax.ylabel[] = string(group)\n\n  return ax\nend\nfunction ridgeplot!(f::Figure, args...; pos=(1, 1), kwargs...)\n  ridgeplot!(Axis(f[pos...]), args...; kwargs...)\n  return f\nend\n\"\"\"\n    ridgeplot(args...; kwargs...)\nSee [ridgeplot!](@ref).\n\"\"\"\nfunction ridgeplot(args...; kwargs...)\n  return ridgeplot!(Figure(), args...; kwargs...)\nend\nridgeplot(parent(gdf), :age, :ntest)\n\nparent(gdf)\n\n\n108,295 rows √ó 6 columnsChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002359S1001832013female7.9945249C002360S1001952013female7.99452510C002361S1002132013male7.99452411C002362S1002372013female7.99452512C002363S1002372013female7.99452513C002364S1002502013female7.99452514C002365S1003042013male7.99452515C002366S1003042013male7.99452516C002367S1003162013female7.99452517C002368S1003652013male7.99452518C002369S1003652013male7.99452519C002370S1003652013female7.99452520C002371S1004322013female7.99452421C002372S1004322013male7.99452522C002373S1004812013male7.99452523C002374S1004812013male7.99452524C002375S1004812013female7.99452525C002376S1004932013female7.99452526C002377S1004932013female7.99452527C002378S1005472013male7.99452528C002379S1005472013male7.99452529C002380S1005472013male7.99452530C002381S1005472013female7.994525‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ"
  },
  {
    "objectID": "Arrow.html#reading-arrow-files-in-other-languages",
    "href": "Arrow.html#reading-arrow-files-in-other-languages",
    "title": "Notes on saved data files",
    "section": "Reading Arrow files in other languages",
    "text": "Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow). They can be accessed within Julia using the RCall and PyCall packages.\nfeather = pyimport(\"pyarrow.feather\");\n\nfeather.read_table(\"./data/fggk21.arrow\")\n\nPyObject pyarrow.Table\nCohort: dictionary<values=string, indices=int8, ordered=0> not null\nSchool: dictionary<values=string, indices=int16, ordered=0> not null\nChild: dictionary<values=string, indices=int32, ordered=0> not null\nSex: dictionary<values=string, indices=int8, ordered=0> not null\nage: double not null\nTest: dictionary<values=string, indices=int8, ordered=0> not null\nscore: double not null\n----\nCohort: [  -- dictionary:\n[\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"]  -- indices:\n[2,2,2,2,2,...,7,7,7,7,7]]\nSchool: [  -- dictionary:\n[\"S100572\",\"S100833\",\"S100912\",\"S100948\",\"S100997\",...,\"S400713\",\"S400865\",\"S700010\",\"S401031\",\"S401055\"]  -- indices:\n[419,419,419,419,419,...,275,275,275,275,275]]\nChild: [  -- dictionary:\n[\"C002352\",\"C002353\",\"C002354\",\"C002355\",\"C002356\",...,\"C117959\",\"C117962\",\"C117964\",\"C117965\",\"C117966\"]  -- indices:\n[0,0,0,0,0,...,108294,108294,108294,108294,108294]]\nSex: [  -- dictionary:\n[\"female\",\"male\"]  -- indices:\n[1,1,1,1,1,...,1,1,1,1,1]]\nage: [[7.994524298425736,7.994524298425736,7.994524298425736,7.994524298425736,7.994524298425736,...,9.10609171800137,9.10609171800137,9.10609171800137,9.10609171800137,9.10609171800137]]\nTest: [  -- dictionary:\n[\"Run\",\"Star_r\",\"S20_r\",\"SLJ\",\"BPT\"]  -- indices:\n[2,4,3,1,0,...,2,4,3,1,0]]\nscore: [[5.2631578947368425,3.7,125,2.4714563106796112,1053,...,4.545454545454545,3.8,100,2.1850643776824032,990]]\n\n\n\nR\"\"\"\nfggk21 <- arrow::read_feather(\"./data/fggk21.arrow\")\nnrow(fggk21)\n\"\"\"\n\nRObject{IntSxp}\n[1] 525126\n\n\n\nR\"tibble::glimpse(fggk21)\";\n\nRows: 525,126\nColumns: 7\n\n\n$ Cohort <fct> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201‚Ä¶\n$ School <fct> S100067, S100067, S100067, S100067, S100067, S100067, S100067, ‚Ä¶\n$ Child  <fct> C002352, C002352, C002352, C002352, C002352, C002353, C002353, ‚Ä¶\n$ Sex    <fct> male, male, male, male, male, male, male, male, male, male, mal‚Ä¶\n$ age    <dbl> 7.994524, 7.994524, 7.994524, 7.994524, 7.994524, 7.994524, 7.9‚Ä¶\n$ Test   <fct> S20_r, BPT, SLJ, Star_r, Run, S20_r, BPT, SLJ, Star_r, Run, S20‚Ä¶\n$ score  <dbl> 5.263158, 3.700000, 125.000000, 2.471456, 1053.000000, 5.000000‚Ä¶"
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used"
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "Matrix notation for the sleepstudy model",
    "text": "Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n\n180 rows √ó 3 columnssubjdaysreactionStringInt8Float641S3080249.562S3081258.7053S3082250.8014S3083321.445S3084356.8526S3085414.697S3086382.2048S3087290.1499S3088430.58510S3089466.35311S3090222.73412S3091205.26613S3092202.97814S3093204.70715S3094207.71616S3095215.96217S3096213.6318S3097217.72719S3098224.29620S3099237.31421S3100199.05422S3101194.33223S3102234.3224S3103232.84225S3104229.30726S3105220.45827S3106235.42128S3107255.75129S3108261.01230S3109247.515‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nMinimizing 58    Time: 0:00:00 ( 5.83 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n\n\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n\n\n\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51068 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                Coef.  Std. Error      z  Pr(>|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, Œ≤, has 2 elements and the fixed-effects model matrix, X, is of size 180 √ó 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ‚ãÆ\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.Œ≤\n\n2-element Vector{Float64}:\n 251.40510484848417\n  10.467285959595715\n\n\n\nm1.X\n\n180√ó2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ‚ãÆ    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1‚Äôs.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The ‚Äúestimates‚Äù (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158188821572576 -40.04844169634357 ‚Ä¶ 0.7232620708173191 12.118907835512482; 9.075511758123813 -8.644079444480187 ‚Ä¶ -0.9710526399373302 1.310698060178722]\n\n\n\nfirst(m1.b)   # only one grouping factor\n\n2√ó18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   ‚Ä¶  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180√ó36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ‚ãÆ              ‚ãÆ              ‚ãÆ        ‚ã±     ‚ãÆ              ‚ãÆ              ‚ãÆ\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.Œ≤ + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725613\n 215.00307618237164\n   ‚ãÆ\n 328.09823347656857\n 337.5944667962269\n 263.52401268399666\n 275.3019967037711\n 287.07998072354553\n 298.85796474331994\n 310.6359487630944\n 322.41393278286887\n 334.1919168026433\n 345.9699008224177\n 357.74788484219215\n 369.5258688619666\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725616\n 215.00307618237167\n   ‚ãÆ\n 328.09823347656857\n 337.594466796227\n 263.52401268399666\n 275.30199670377107\n 287.0799807235455\n 298.85796474331994\n 310.6359487630944\n 322.4139327828688\n 334.1919168026432\n 345.9699008224177\n 357.74788484219215\n 369.52586886196656\n\n\nIn symbols we would write the linear predictor expression as\n\\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\]\nwhere \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 √ó 2 and \\(\\bf Z\\) is of size 180 √ó 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 √ó 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 √ó 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.Œ∏\n\n3-element Vector{Float64}:\n 0.9292213219958606\n 0.018168376087695465\n 0.22264487473562505\n\n\n\nŒª = only(m1.Œª)  # with multiple grouping factors there will be multiple Œª's\n\n2√ó2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ‚ãÖ \n 0.0181684  0.222645\n\n\n\nŒ£ = varest(m1) * (Œª * Œª')\n\n2√ó2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "Linear predictors in LMMs and GLMMs",
    "text": "Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as\n\\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\]\nmay seem like over-mathematization (or ‚Äúoverkill‚Äù, if you prefer) relative to expressions like\n\\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\]\nbut this more abstract form is necessary for generalizations.\nThe way that I read the first form is :::{.callout} The conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\nSo the only things that differ in the distributions of the \\(y_i\\)‚Äôs are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "Generalized Linear Mixed Models",
    "text": "Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e.¬†yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i‚Äôth response is again determined by the i‚Äôth element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of ‚Äúsuccess‚Äù for the i‚Äôth response and must be between 0 and 1. We can‚Äôt guarantee that the i‚Äôth component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren‚Äôt as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function,\n\\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\]\n(it‚Äôs called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic\n\\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\]\nThis is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(Œ∑) = inv(increment(exp(-Œ∑)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don‚Äôt have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet ‚ÄúBeauty is truth, truth beauty - that is all ye know on earth and all ye need to know.‚Äù)\n\nAn example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n\n1,934 rows √ó 5 columnsdisturbanlivchageuseStringStringStringFloat64String1D01Y3+18.44N2D01Y0-5.56N3D01Y21.44N4D01Y3+8.44N5D01Y0-13.56N6D01Y0-11.56N7D01Y3+18.44N8D01Y3+-3.56N9D01Y1-5.56N10D01Y3+1.44N11D01Y0-11.56Y12D01Y0-2.56N13D01Y1-4.56N14D01Y3+5.44N15D01Y3+-0.56N16D01Y3+4.44Y17D01Y0-5.56N18D01Y3+-0.56Y19D01Y1-6.56Y20D01Y2-3.56N21D01Y0-4.56N22D01Y0-9.56N23D01Y3+2.44N24D01Y22.44Y25D01Y1-4.56Y26D01Y3+14.44N27D01Y0-6.56Y28D01Y1-3.56Y29D01Y1-5.56Y30D01Y1-1.56Y‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n\n60 rows √ó 2 columnsdistnrowStringInt641D011172D02203D0324D04305D05396D06657D07188D08379D092310D101311D112112D122913D132414D1411815D152216D162017D172418D184719D192620D201521D211822D222023D231524D241425D256726D261327D274428D284929D293230D3061‚ãÆ‚ãÆ‚ãÆ\n\n\nThe information recorded included woman‚Äôs age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, drawn using lattice graphics in R, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20‚Äôs to early 30‚Äôs) and low again for older women (late 30‚Äôs to early 40‚Äôs in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn‚Äôt mean that there is no ‚Äúage effect‚Äù, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist => Grouping(),\n  :urban => HelmertCoding(),\n  :children => HelmertCoding(),\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 186   Time: 0:00:00 ( 1.86 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n<1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7021\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n<1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n<1e-08\n\n\n\nlivch: 1\n0.8151\n0.1622\n5.02\n<1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n<1e-06\n\n\n\nlivch: 3+\n0.9153\n0.1858\n4.93\n<1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\nVarCorr(gm1)\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229100\n0.478644\n\n\n\nNotice that there is no ‚Äúresidual‚Äù variance being estimated. This is because the Bernoulli distribution doesn‚Äôt have a scale parameter.\n\n\nConvert livch to a binary factor\n\n@transform!(contra, :children = :livch ‚â† \"0\")\n\n\n1,934 rows √ó 6 columnsdisturbanlivchageusechildrenStringStringStringFloat64StringBool1D01Y3+18.44N12D01Y0-5.56N03D01Y21.44N14D01Y3+8.44N15D01Y0-13.56N06D01Y0-11.56N07D01Y3+18.44N18D01Y3+-3.56N19D01Y1-5.56N110D01Y3+1.44N111D01Y0-11.56Y012D01Y0-2.56N013D01Y1-4.56N114D01Y3+5.44N115D01Y3+-0.56N116D01Y3+4.44Y117D01Y0-5.56N018D01Y3+-0.56Y119D01Y1-6.56Y120D01Y2-3.56N121D01Y0-4.56N022D01Y0-9.56N023D01Y3+2.44N124D01Y22.44Y125D01Y1-4.56Y126D01Y3+14.44N127D01Y0-6.56Y028D01Y1-3.56Y129D01Y1-5.56Y130D01Y1-1.56Y1‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 138   Time: 0:00:00 ( 0.87 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows √ó 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm272364.922379.182418.152379.242gm182372.462388.732433.272388.81\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\nUsing urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 143   Time: 0:00:00 ( 0.87 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm372353.822368.482407.462368.542gm272364.922379.182418.152379.243gm182372.462388.732433.272388.81\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\ngm2\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age¬≤ (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\nPredictions for some subgroups\nFor a ‚Äútypical‚Äù district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e.¬†centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children => [true, false], :urban => [\"Y\", \"N\"], :age => [0.0]\n)\npreds = effects(design, gm3)\n\n\n4 rows √ó 7 columnschildrenageurbanuse: YerrlowerupperBoolFloat64StringFloat64Float64Float64Float64110.0Y0.6585760.150530.5080460.809106200.0Y-0.5543250.230477-0.784802-0.323848310.0N-0.1286160.113017-0.241633-0.0155985400.0N-1.341520.221575-1.56309-1.11994\n\n\nConverting these Œ∑ values to probabilities yields\n\nlogistic.(preds[!, \"use: Y\"])\n\n4-element Vector{Float64}:\n 0.6589404445793394\n 0.3648614994485519\n 0.46789030652047003\n 0.2072606855141492"
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "Summarizing the results",
    "text": "Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\nProvide a short alias for AlgebraOfGraphics."
  },
  {
    "objectID": "kb07.html#data-set-and-model",
    "href": "kb07.html#data-set-and-model",
    "title": "Bootstrapping a fitted model",
    "section": "Data set and model",
    "text": "Data set and model\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n\n7 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1subjS030S1030String2itemI01I320String3spkrnewold0String4precbreakmaintain0String5loadnoyes0String6rt_trunc2182.25791940.051710Int167rt_raw2226.245791940.0159230Int16\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\ncontrasts = merge(\n  Dict(nm => EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm => Grouping() for nm in (:subj, :item)),\n);\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 723   Time: 0:00:01 ( 1.70 ms/it)\n  objective:  28637.1393507629\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n\n\n(Intercept)\n2181.6750\n77.3042\n28.22\n<1e-99\n301.8457\n362.1769\n\n\nspkr: old\n67.7490\n18.2846\n3.71\n0.0002\n43.0723\n40.5401\n\n\nprec: maintain\n-333.9206\n47.1549\n-7.08\n<1e-11\n62.1055\n246.8926\n\n\nload: yes\n78.7681\n19.5218\n4.03\n<1e-04\n65.1378\n42.1405\n\n\nspkr: old & prec: maintain\n-21.9634\n15.8063\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3838\n15.8063\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5334\n15.8063\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6051\n15.8063\n1.49\n0.1353\n\n\n\n\nResidual\n668.5074\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91110.8185\n301.8457\n\n\n\n\n\n\nspkr: old\n1855.2224\n43.0723\n+0.78\n\n\n\n\n\nprec: maintain\n3857.0961\n62.1055\n-0.59\n+0.02\n\n\n\n\nload: yes\n4242.9381\n65.1378\n+0.36\n+0.82\n+0.53\n\n\nitem\n(Intercept)\n131172.1058\n362.1769\n\n\n\n\n\n\nspkr: old\n1643.4957\n40.5401\n+0.44\n\n\n\n\n\nprec: maintain\n60955.9375\n246.8926\n-0.69\n+0.35\n\n\n\n\nload: yes\n1775.8252\n42.1405\n+0.32\n+0.23\n-0.15\n\n\nResidual\n\n446902.1445\n668.5074\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.240\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n27\n20\n0.1436\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, kbm02, to the more complex, kbm01."
  },
  {
    "objectID": "kb07.html#a-bootstrap-sample",
    "href": "kb07.html#a-bootstrap-sample",
    "title": "Bootstrapping a fitted model",
    "section": "A bootstrap sample",
    "text": "A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\nRandom.seed!(1234321)\nhide_progress = true\nkbm02samp = parametricbootstrap(2000, kbm02; hide_progress);\nOne of the uses of such a sample is to form ‚Äúconfidence intervals‚Äù on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nDataFrame(shortestcovint(kbm02samp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2028.012337.922Œ≤missingspkr: old38.43199.59443Œ≤missingprec: maintain-439.321-245.8644Œ≤missingload: yes46.0262107.5115œÉitem(Intercept)261.196448.516œÉitemprec: maintain175.489312.0387œÅitem(Intercept), prec: maintain-0.89799-0.4455958œÉsubj(Intercept)228.099357.7899œÉresidualmissing655.249701.497\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure¬†1.\n\n\nCode\ndraw(\n  data(kbm02samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\ndraw(\n  data(\n    filter(\n      :column => ==(Symbol(\"(Intercept)\")), DataFrame(kbm02samp.œÉs)\n    ),\n  ) *\n  mapping(:œÉ; color=:group) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(filter(:type => ==(\"œÅ\"), DataFrame(kbm02samp.allpars))) *\n  mapping(:value => \"Correlation\"; color=:names) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 3: Density plot of correlation parameters in bootstrap sample from model kbm02"
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "",
    "text": "We take the kwdyz11.arrow dataset (Kliegl et al., 2010) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15.arrow from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl."
  },
  {
    "objectID": "kwdyz11.html#packages",
    "href": "kwdyz11.html#packages",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Packages",
    "text": "Packages\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\nusing Random\nusing StatsBase\nusing Statistics\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "href": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Read data, compute and plot densities and means",
    "text": "Read data, compute and plot densities and means\n\n\nCode\ndat = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 2, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt,\n    :rt => (x -> log.(x)) => :lrt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS01S610CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3rt370.426150.1358.6705.70Float644lrt5.8865.01135.882216.559190Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results and exhibited the same problems of model identification (see Kliegl et al., 2010).\nComparative density plots of all response times by cue-target relation, Figure¬†1, show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density(),\n)\n\n\n\n\n\nFigure 1: Comparative density plots of log reaction time for different cue-target relations.\n\n\n\n\nAn alternative visualization without overlap of the conditions can be accomplished with ridge plots.\nTo be done\nFor the next set of plots we average subjects‚Äô data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\n\n\n244 rows √ó 5 columnsSubjCTRnrt_mlrt_mCat‚Ä¶Cat‚Ä¶Int64Float64Float641S01val330413.3326.012722S01sod48437.7216.06823S01dos47443.3666.082694S01dod45434.3166.060795S02val333365.8995.87516S02sod47396.1495.959167S02dos46439.4876.069498S02dod48441.0426.068839S03val336371.4465.9048910S03sod46446.8546.0946411S03dos48471.3026.1428712S03dod47476.5326.1570613S04val336403.1815.9912414S04sod48446.0756.0949915S04dos48458.3046.1200516S04dod48441.0546.0820517S05val310358.7145.8411718S05sod44409.155.9493419S05dos45457.4936.1005320S05dod45472.1386.1336121S06val334362.8645.8795322S06sod48368.1235.8961723S06dos48383.0945.9360124S06dod48367.9735.8897925S07val326407.4975.9844726S07sod48459.06.1092727S07dos42486.5026.1772928S07dod47457.6176.1047129S08val333308.1025.7181530S08sod48323.8755.76388‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\n\n\nFigure 2: Comparative boxplots of log response time by cue-target relation.\n\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nA better alternative to the boxplot is a dotplot. It also displays subjects‚Äô condition means.\nTo be done"
  },
  {
    "objectID": "kwdyz11.html#linear-mixed-model",
    "href": "kwdyz11.html#linear-mixed-model",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Linear mixed model",
    "text": "Linear mixed model\n\ncontrasts = Dict(\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n  :Subj => Grouping(),\n)\nm1 = let\n  form = @formula(lrt ~ 1 + CTR + (1 + CTR | Subj))\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 319   Time: 0:00:00 ( 1.44 ms/it)\n  objective:  -12782.373740637588\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n5.9358\n0.0185\n320.53\n<1e-99\n0.1441\n\n\nCTR: sod\n0.0878\n0.0084\n10.48\n<1e-24\n0.0582\n\n\nCTR: dos\n0.0366\n0.0062\n5.92\n<1e-08\n0.0274\n\n\nCTR: dod\n-0.0086\n0.0060\n-1.43\n0.1515\n0.0249\n\n\nResidual\n0.1920\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.0207652\n0.1441013\n\n\n\n\n\n\nCTR: sod\n0.0033847\n0.0581778\n+0.48\n\n\n\n\n\nCTR: dos\n0.0007530\n0.0274414\n-0.24\n-0.15\n\n\n\n\nCTR: dod\n0.0006221\n0.0249410\n+0.30\n+0.93\n-0.43\n\n\nResidual\n\n0.0368543\n0.1919748\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nLMM m1 is not fully supported by the data; it is overparameterized. This is also visible in the PCA: only three, not four PCS are needed to account for all the variance and covariance in the random-effect structure. The problem is the +.93 CP for spatial sod and attraction dod effects.\n\nfirst(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.48   1.0     .      .\n CTR: dos     -0.24  -0.15   1.0     .\n CTR: dod      0.3    0.93  -0.43   1.0\n\nNormalized cumulative variances:\n[0.5886, 0.8095, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2    PC3    PC4\n (Intercept)  -0.4   0.04   0.9    0.17\n CTR: sod     -0.6   0.4   -0.16  -0.68\n CTR: dos      0.33  0.91   0.06   0.23\n CTR: dod     -0.61  0.08  -0.41   0.68"
  },
  {
    "objectID": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "href": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Diagnostic plots of LMM residuals",
    "text": "Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\nResidual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f => \"Fitted values\", :r => \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\nFigure 3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted log response time\", :r => \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 4: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n\nQ-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)\n\n\n\n\n\n\nObserved and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure¬†5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value => \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure 5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian"
  },
  {
    "objectID": "kwdyz11.html#conditional-modes",
    "href": "kwdyz11.html#conditional-modes",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Conditional modes",
    "text": "Conditional modes\nNow we move on to visualizations that are based on model parameters and subjects‚Äô data, that is ‚Äúpredictions‚Äù of the LMM for subject‚Äôs GM and experimental effects. Three important plots are\n\nOverlay\nCaterpillar\nShrinkage\n\n\nOverlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done\n\n\nCaterpillar plot\nThe caterpillar plot, Figure¬†6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n\n\nFigure 6: Prediction intervals on the random effects for Subj in model m1\n\n\n\n\n\n\nShrinkage plot\nFigure¬†7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n\n\nFigure 7: Shrinkage plot of the conditional means of the random effects for model m1"
  },
  {
    "objectID": "kwdyz11.html#parametric-bootstrap",
    "href": "kwdyz11.html#parametric-bootstrap",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Parametric bootstrap",
    "text": "Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\nGenerate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1; hide_progress=true)\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n\n10 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)5.9339221Œ≤missingCTR: sod0.086421731Œ≤missingCTR: dos0.048889141Œ≤missingCTR: dod-0.012183551œÉSubj(Intercept)0.13293761œÉSubjCTR: sod0.049735871œÅSubj(Intercept), CTR: sod0.60494281œÉSubjCTR: dos0.027881991œÅSubj(Intercept), CTR: dos-0.254336101œÅSubjCTR: sod, CTR: dos0.0550768\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters \n\n37500\n\n\n\n\nShortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nDataFrame(shortestcovint(samp))\n\n\n\n15 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)5.899175.972562Œ≤missingCTR: sod0.07193110.1045883Œ≤missingCTR: dos0.02511790.04916734Œ≤missingCTR: dod-0.02071780.002683055œÉSubj(Intercept)0.1165650.168576œÉSubjCTR: sod0.04551810.07080337œÅSubj(Intercept), CTR: sod0.2437220.7129558œÉSubjCTR: dos0.009642950.04088249œÅSubj(Intercept), CTR: dos-0.9211810.24702610œÅSubjCTR: sod, CTR: dos-0.7247280.47764611œÉSubjCTR: dod0.01441330.038505712œÅSubj(Intercept), CTR: dod-0.132490.74037913œÅSubjCTR: sod, CTR: dod0.5763690.99999514œÅSubjCTR: dos, CTR: dod-0.8856910.43604115œÉresidualmissing0.1905590.193644\n\n\n\n\nComparative density plots of bootstrapped parameter estimates\n\nResidual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\", ismissing(:names))) *\n  mapping(:value => \"Residual standard deviation\") *\n  density();\n)\n\n\n\n\n\nFigure 8: ?(caption)\n\n\n\n\n\n\nFixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not inlcude its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept)\" => \"grand mean\",\n]\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => renamer(labels) => \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nVariance components (VCs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => renamer(labels) => \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 10: Comparative density plots of the variance components for model m1\n\n\n\n\nThe VC are all very nicely defined.\n\n\nCorrelation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" => \"GM, spatial\",\n    \"(Intercept), CTR: dos\" => \"GM, object\",\n    \"CTR: sod, CTR: dos\" => \"spatial, object\",\n    \"(Intercept), CTR: dod\" => \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" => \"object, attraction\",\n  ]\n  draw(\n    data(@subset(dat2, :type == \"œÅ\")) *\n    mapping(\n      :value => \"Correlation\";\n      color=:names => renamer(labels) => \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\nFigure 11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015)."
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2010) (see also script mmt_kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visiable in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2010). To keep matters somewhat simpler and comparable we ignore them in this script.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2010), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2010), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions. Therefore, the code in this script is largely the same as the one in kwdyz.jl.\nThere will be another vignette modelling the additional experimental manipulations of target size and orientation of cue rectangle. This analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); they were also used in a paper of GAMMs (Baayen et al., 2017). Data and R scripts are also available in R-package RePsychLing. Here we provide some of the corresponding analyses with MixedModels.jl and a much wider variety of visualizations of LMM results. A MixedModels.jl-based analysis focusing on the complex experimental design and the analysis of a complex random-effect structure for this design is in script kkl15_complex.jl."
  },
  {
    "objectID": "kkl15.html#packages",
    "href": "kkl15.html#packages",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Packages",
    "text": "Packages\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing Random\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\nusing MixedModelsMakie: qqnorm\nusing MixedModelsMakie: ridgeplot\nusing MixedModelsMakie: scatter\nconst datadir = joinpath(@__DIR__, \"data\")\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"png\")"
  },
  {
    "objectID": "kkl15.html#read-data-compute-and-plot-means",
    "href": "kkl15.html#read-data-compute-and-plot-means",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Read data, compute and plot means",
    "text": "Read data, compute and plot means\n\ndat = @chain \"kkl15.arrow\" begin\n  joinpath(datadir, _)\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 3, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt => (x -> log.(x)) => :lrt,\n    :rt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3lrt5.644245.01215.622556.619380Float644rt293.147150.22276.594749.4810Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results.\nComparative density plots of all response times by cue-target relation show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 1: Compartive density plots of log response time by condition\n\n\n\n\nBoxplots of the mean of log response time by subject under the different conditions show an outlier value under three of the four conditions; they are from the same subject.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\ndescribe(dat_subj)\n\n\n5 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3n156.2944964.04480Int644rt_m308.223208.194304.862584.710Float645lrt_m5.69085.332265.698486.361410Float64\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\n\n\nFigure 2: Comparative boxplots of mean response by subject under different conditions\n\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nWe remove the outlier subject and replot, but we model the data points in dat and check whether this subject appears as an outlier in the caterpillar plot of conditional modes.\n\n\nCode\nlet\n  dat_subj2 = @subset(dat_subj, :rt_m < 510)\n  boxplot(\n    dat_subj2.CTR.refs,\n    dat_subj2.lrt_m;\n    orientation=:horizontal,\n    show_notch=true,\n    axis=(;\n      yticks=(\n        1:4,\n        [\n          \"valid cue\",\n          \"same obj/diff pos\",\n          \"diff obj/same pos\",\n          \"diff obj/diff pos\",\n        ],\n      ),\n    ),\n    figure=(; resolution=(800, 300)),\n  )\nend\n\n\n\n\n\nFigure 3: Comparative boxplots of mean response by subject under different conditions without outlier\n\n\n\n\nA better alternative to the boxplot is often a dotplot, because it also displays subjects‚Äô condition means.\nTo be done\nFor the next set of plots we average subjects‚Äô data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_cond = combine(\n  groupby(dat_subj, :CTR),\n  :n => length => :N,\n  :lrt_m => mean => :lrt_M,\n  :lrt_m => std => :lrt_SD,\n  :lrt_m => (x -> std(x) / sqrt(length(x))) => :lrt_SE,\n)\n\n\n4 rows √ó 5 columnsCTRNlrt_Mlrt_SDlrt_SECat‚Ä¶Int64Float64Float64Float641val865.614430.158050.01704292sod865.688360.1913950.02063863dos865.729430.1910270.02059894dod865.730990.216280.023322\n\n\nWe can also look at correlations plots based on the four condition means. There are actually two correlation matrices which have correspondences in alternative parameterizatios of the LMM random-effect structure. One matrix is based on the four measures. If you think of the four measures as test scores, this matrix is the usual correlation matrix. The second matrix contains correlations between the Grand Mean (GM) and the three effects defined with the contrasts for the four levels of the condition factor in the next chunk.\nTo this end, we\n\nuse the unstack() command to convert data from long to wide format,\ncompute the GM and the three experimental effects.\nplot the correlation matrix for four measures/scores, and\nplot the correlation matrix for GM and three effects\n\n\ndat_subj_w = @chain dat_subj begin\n  unstack(:Subj, :CTR, :rt_m)\n  disallowmissing!\n  @transform(\n    :GM = (:val + :sod + :dos + :dod) ./ 4,\n    :spatial = :sod - :val,\n    :object = :dos - :sod,\n    :attraction = :dod - :dos,\n  )\nend\ndescribe(dat_subj_w)\n\n\n9 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2val283.688216.35286.376513.530Float643sod306.75213.444305.092562.0150Float644dos319.896215.787317.527584.710Float645dod322.561208.194315.511555.2060Float646GM308.223217.819309.381553.8650Float647spatial23.0621-47.203620.227287.78730Float648object13.1456-13.827311.790361.04240Float649attraction2.66497-43.8703-1.1188763.53470Float64\n\n\n#@df dat_subj_w StatsPlots.corrplot(cols(2:5), grid = false, compact=false)\n#@df dat_subj_w StatsPlots.corrplot(cols(6:9), grid = false, compact=false)\n\n\n\n\n\n\nNote\n\n\n\nTwo of the theoreticsally irrelevant within-subject effect correlations have a different sign than the corresponding, non-significant CPs in the LMM; they are negative here, numerically positive in the LMM. This occurs only very rarely in the case of ecological correlations. However, as they are not significant according to shortest coverage interval, it may not be that relevant either. It is the case both for effects based on log-transformed and raw reaction times."
  },
  {
    "objectID": "kkl15.html#linear-mixed-model",
    "href": "kkl15.html#linear-mixed-model",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Linear mixed model",
    "text": "Linear mixed model\n\ncontrasts = Dict(\n  :Subj => Grouping(),\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n)\nm1 = let\n  form = @formula lrt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 232   Time: 0:00:00 ( 1.60 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n5.6907\n0.0199\n286.42\n<1e-99\n0.1839\n\n\nCTR: sod\n0.0740\n0.0080\n9.30\n<1e-19\n0.0688\n\n\nCTR: dos\n0.0409\n0.0038\n10.74\n<1e-26\n0.0011\n\n\nCTR: dod\n0.0016\n0.0057\n0.28\n0.7771\n0.0387\n\n\nResidual\n0.1971\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.03382755\n0.18392268\n\n\n\n\n\n\nCTR: sod\n0.00472960\n0.06877208\n+0.56\n\n\n\n\n\nCTR: dos\n0.00000126\n0.00112159\n-0.05\n+0.80\n\n\n\n\nCTR: dod\n0.00149685\n0.03868912\n+0.60\n+0.66\n+0.36\n\n\nResidual\n\n0.03884575\n0.19709325\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\n\nonly(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.56   1.0     .      .\n CTR: dos     -0.05   0.8    1.0     .\n CTR: dod      0.6    0.66   0.36   1.0\n\nNormalized cumulative variances:\n[0.6327, 0.9136, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4\n (Intercept)  -0.41   0.66  -0.47   0.42\n CTR: sod     -0.6   -0.18  -0.34  -0.7\n CTR: dos     -0.43  -0.69  -0.07   0.58\n CTR: dod     -0.53   0.25   0.81   0.0\n\n\nWe note that the critical correlation parameter between spatial (sod) and attraction (dod) is now estimated at .66 ‚Äì not that close to the 1.0 boundary that caused singularity in Kliegl et al. (2010). However, the LMM based on log reaction times is still singular. Let‚Äôs check for untransformed reaction times.\nm1_rt = let\n  form = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4147\n48.05\n<1e-99\n59.3745\n\n\nCTR: sod\n23.0720\n2.6415\n8.73\n<1e-17\n22.8504\n\n\nCTR: dos\n13.0855\n1.4583\n8.97\n<1e-18\n6.8329\n\n\nCTR: dod\n2.6860\n2.0608\n1.30\n0.1925\n15.1026\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\nVarCorr(m1_rt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n3525.32539\n59.37445\n\n\n\n\n\n\nCTR: sod\n522.13947\n22.85037\n+0.66\n\n\n\n\n\nCTR: dos\n46.68842\n6.83289\n+0.35\n+0.15\n\n\n\n\nCTR: dod\n228.08774\n15.10257\n+0.53\n+0.65\n+0.30\n\n\nResidual\n\n4254.25039\n65.22461\n\n\n\n\n\n\n\nissingular(m1_rt)\n\nfalse\n\n\nFor untransformed reaction times, we see the model is not singular."
  },
  {
    "objectID": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "href": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Diagnostic plots of LMM residuals",
    "text": "Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\nResidual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nscatter(fitted(m1), residuals(m1))\n\n\n\n\n\nFigure 4: Residuals versus fitted values for model m1\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted values from m1\", :r => \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 5: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n\nQ-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nCode\nqqnorm(m1; qqline=:none)\n\n\nCode\nqqnorm(m1_rt; qqline=:none)\n\n\n\nObserved and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is narrower than expected from a normal distribution. We can see this in this plot. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = (;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=repeat([\"residual\", \"normal\"]; inner=n),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure 6: Kernel density plot of the standardized residuals for model m1 versus a standard normal"
  },
  {
    "objectID": "kkl15.html#conditional-modes",
    "href": "kkl15.html#conditional-modes",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Conditional modes",
    "text": "Conditional modes\n\nCaterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n\n\nFigure 7: Prediction intervals of the subject random effects in model m1\n\n\n\n\nWhen we order the conditional modes for GM, that is (Intercept), the outlier subject S113 becomes visible; the associated experimental effects are not unusual.\n\n\nCode\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=1)\n\n\n\n\n\nFigure 8: Prediction intervals of the subject random effects in model m1 ordered by mean response\n\n\n\n\nThe catepillar plot also reveals that credibilty intervals are much shorter for subjects‚Äô Grand Means, shown in (Intercept), than the subjects‚Äô experimental effects, because the latter are based on difference scores not means. Moreover, credibility intervals are shorter for the first spatial effect sod than the other two effects, because the spatial effect involves the valid condition which yielded three times as many trials than the other three conditions. Consequently, the spatial effect is more reliable. Unfortunately, due to differences in scaling of the x-axis of the panels this effect must be inferred. One option to reveal this difference is to reparameterize the LMM such model parameters estimate the conditional modes for the levels of condition rather than the contrast-based effects. This is accomplished by replacing the 1 in the random effect term with 0, as shown next.\nm1L = let\n  form = @formula rt ~ 1 + CTR + (0 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4146\n48.05\n<1e-99\n\n\n\nCTR: sod\n23.0720\n2.6415\n8.73\n<1e-17\n60.1729\n\n\nCTR: dos\n13.0855\n1.4583\n8.97\n<1e-18\n62.4723\n\n\nCTR: dod\n2.6860\n2.0609\n1.30\n0.1925\n71.5378\n\n\nCTR: val\n\n\n\n\n47.2871\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\nVarCorr(m1L)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\nCTR: val\n2236.0678\n47.2871\n\n\n\n\n\n\nCTR: sod\n3620.7745\n60.1729\n+0.94\n\n\n\n\n\nCTR: dos\n3902.7873\n62.4723\n+0.93\n+0.99\n\n\n\n\nCTR: dod\n5117.6611\n71.5378\n+0.89\n+0.98\n+0.98\n\n\nResidual\n\n4254.2488\n65.2246\n\n\n\n\n\n\nThe caterpillar plot for levels shows the effect of the number of trials on credibility intervals; they are obviously much shorter for the valid condition. Note that this effect is not visible in a repeated-measure ANOVA with four condition means per subject as input.\n\n\nCode\n@chain m1L begin\n  ranefinfo\n  only\n  caterpillar!(Figure(; resolution=(800, 1000)), _; orderby=1)\nend\n\n\n\n\n\nFigure 9: Prediction intervals of the subject random effects in model m1L\n\n\n\n\n\n\nShrinkage plot\n\nLog-transformed reaction times (LMM m1)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1)\n\n\n\n\n\nFigure 10: Shrinkage plots of the subject random effects in model m1L\n\n\n\n\nThree of the CPs are imploded, but not the theoretically critical ones. These implosions did not occur (or were not as visible) for raw reaction times.\n\n\nRaw reaction times (LMM m1_rt)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1_rt)\n\n\n\n\n\nFigure 11: Shrinkage plots of the subject random effects in model m1_rt\n\n\n\n\nThe implosion is for three CP visualizations is not observed for raw reaction times. Interesting."
  },
  {
    "objectID": "kkl15.html#parametric-bootstrap",
    "href": "kkl15.html#parametric-bootstrap",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Parametric bootstrap",
    "text": "Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\nGenerate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1);\n\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n10 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)5.6919121Œ≤missingCTR: sod0.07775131Œ≤missingCTR: dos0.040674441Œ≤missingCTR: dod-0.0024932651œÉSubj(Intercept)0.1825461œÉSubjCTR: sod0.062357271œÅSubj(Intercept), CTR: sod0.67746481œÉSubjCTR: dos0.0039113991œÅSubj(Intercept), CTR: dos-0.290374101œÅSubjCTR: sod, CTR: dos0.507145\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters \n\n37500\n\n\n\n\nShortest coverage interval\n\nDataFrame(shortestcovint(samp))\n\n\n15 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)5.64875.727752Œ≤missingCTR: sod0.0579340.08897943Œ≤missingCTR: dos0.03291840.0477164Œ≤missingCTR: dod-0.009701970.01267655œÉSubj(Intercept)0.1533530.2075776œÉSubjCTR: sod0.05657480.07997687œÅSubj(Intercept), CTR: sod0.388260.716538œÉSubjCTR: dos0.001016460.01795729œÅSubj(Intercept), CTR: dos-0.9999990.92042210œÅSubjCTR: sod, CTR: dos-0.8843110.99998811œÉSubjCTR: dod0.02772640.048685912œÅSubj(Intercept), CTR: dod0.3928010.8300813œÅSubjCTR: sod, CTR: dod0.4451640.89084214œÅSubjCTR: dos, CTR: dod-0.8345880.93672615œÉresidualmissing0.1958950.198225\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\nFigure 12: Ridge plot of fixed-effects bootstrap samples from model m1L\n\n\n\n\n\n\nComparative density plots of bootstrapped parameter estimates\n\nResidual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"residual\")) *\n  mapping(:value => \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\nFigure 13: Kernel density estimate from bootstrap samples of the residual standard deviation for model m1L\n\n\n\n\n\n\nFixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not include its density here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" => \"GM\",\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept), CTR: sod\" => \"GM, spatial\",\n  \"(Intercept), CTR: dos\" => \"GM, object\",\n  \"CTR: sod, CTR: dos\" => \"spatial, object\",\n  \"(Intercept), CTR: dod\" => \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" => \"object, attraction\",\n])\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => rn => \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 14: Kernel density estimate from bootstrap samples of the fixed effects for model m1L\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => rn => \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 15: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThe VC are all very nicely defined.\n\n\nCorrelation parameters (CPs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÅ\")) *\n  mapping(\n    :value => \"Correlation\";\n    color=:names => rn => \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 16: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl.\nThe three remaining CPs are not well defined for log-transformed reaction times; they only fit noise and should be removed. It is also possible that fitting the complex experimental design (including target size and rectangle orientation) will lead to more acceptable estimates. The corresponding plot based on LMM m1_rt for raw reaction times still shows them with very wide distributions, but acceptable."
  },
  {
    "objectID": "shrinkageplot.html",
    "href": "shrinkageplot.html",
    "title": "More on shrinkage plots",
    "section": "",
    "text": "I have stated that the likelihood criterion used to fit linear mixed-effects can be considered as balancing fidelity to the data (i.e.¬†fits the observed data well) versus model complexity.\nThis is similar to some of the criterion used in Machine Learning (ML), except that the criterion for LMMs has a rigorous mathematical basis.\nIn the shrinkage plot we consider the values of the random-effects coefficients for the fitted values of the model versus those from a model in which there is no penalty for model complexity.\nIf there is strong subject-to-subject variation then the model fit will tend to values of the random effects similar to those without a penalty on complexity.\nIf the random effects term is not contributing much (i.e.¬†it is ‚Äúinert‚Äù) then the random effects will be shrunk considerably towards zero in some directions.\nLoad the kb07 data set (don‚Äôt tell Reinhold that I used these data)."
  },
  {
    "objectID": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "href": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "title": "More on shrinkage plots",
    "section": "Expressing the covariance of random effects",
    "text": "Expressing the covariance of random effects\nEarlier today we mentioned that the parameters being optimized are from a ‚Äúmatrix square root‚Äù of the covariance matrix for the random effects. There is one such lower triangular matrix for each grouping factor.\n\nl1 = first(m1.Œª)   # Cholesky factor of relative covariance for subj\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.451522    ‚ãÖ          ‚ãÖ          ‚ãÖ \n  0.0502576  0.0403171   ‚ãÖ          ‚ãÖ \n -0.0552328  0.0725482  0.0177998   ‚ãÖ \n  0.0351353  0.0845401  0.0333557  0.0\n\n\nNotice the zero on the diagonal. A triangular matrix with zeros on the diagonal is singular.\n\nl2 = last(m1.Œª)    # this one is not singular\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.541769    ‚ãÖ            ‚ãÖ           ‚ãÖ \n  0.0267884  0.0544051     ‚ãÖ           ‚ãÖ \n -0.253071   0.268983     0.0          ‚ãÖ \n  0.019981   0.00612443  -0.00380246  0.05935\n\n\nTo regenerate the covariance matrix we need to know that the covariance is not the square of l1, it is l1 * l1' (so that the result is symmetric) and multiplied by œÉÃÇ¬≤\n\nŒ£‚ÇÅ = varest(m1) .* (l1 * l1')\n\n4√ó4 Matrix{Float64}:\n  91110.8   10141.3     -11145.2     7089.81\n  10141.3    1855.22        66.6142  2312.37\n -11145.2      66.6142    3857.1     2139.02\n   7089.81   2312.37      2139.02    4242.94\n\n\n\ndiag(Œ£‚ÇÅ)  # compare to the variance column in the VarCorr output\n\n4-element Vector{Float64}:\n 91110.81853641519\n  1855.222443908261\n  3857.096133983496\n  4242.938057034487\n\n\n\nsqrt.(diag(Œ£‚ÇÅ))\n\n4-element Vector{Float64}:\n 301.8456866288057\n  43.07229322787749\n  62.10552418250325\n  65.13783890362411"
  },
  {
    "objectID": "shrinkageplot.html#shrinkage-plots",
    "href": "shrinkageplot.html#shrinkage-plots",
    "title": "More on shrinkage plots",
    "section": "Shrinkage plots",
    "text": "Shrinkage plots\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\nFigure 1: Shrinkage plot of model m1\n\n\n\n\nThe upper left panel shows the perfect negative correlation for those two components of the random effects.\n\nshrinkageplot(m1, :item)\n\n\n\n\n\nX1 = Int.(m1.X')\n\n8√ó1789 Matrix{Int64}:\n  1   1   1   1   1  1   1   1   1   1  ‚Ä¶   1   1   1   1   1   1   1  1   1\n -1   1   1  -1  -1  1   1  -1  -1   1      1  -1  -1   1   1  -1  -1  1   1\n -1   1  -1   1  -1  1  -1   1  -1   1     -1   1  -1   1  -1   1  -1  1  -1\n  1  -1  -1  -1  -1  1   1   1   1  -1      1   1   1  -1  -1  -1  -1  1   1\n  1   1  -1  -1   1  1  -1  -1   1   1     -1  -1   1   1  -1  -1   1  1  -1\n -1  -1  -1   1   1  1   1  -1  -1  -1  ‚Ä¶   1  -1  -1  -1  -1   1   1  1   1\n -1  -1   1  -1   1  1  -1   1  -1  -1     -1   1  -1  -1   1  -1   1  1  -1\n  1  -1   1   1  -1  1  -1  -1   1  -1     -1  -1   1  -1   1   1  -1  1  -1\n\n\n\nX1 * X1'\n\n8√ó8 Matrix{Int64}:\n 1789    -1    -1     3    -3     1     1     3\n   -1  1789    -3     1    -1     3     3     1\n   -1    -3  1789     1    -1     3     3     1\n    3     1     1  1789     3    -1    -1    -3\n   -3    -1    -1     3  1789     1     1     3\n    1     3     3    -1     1  1789    -3    -1\n    1     3     3    -1     1    -3  1789    -1\n    3     1     1    -3     3    -1    -1  1789"
  },
  {
    "objectID": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "href": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "title": "More on shrinkage plots",
    "section": "How to interpret a shrinkage plot",
    "text": "How to interpret a shrinkage plot\n\nExtreme shrinkage (shrunk to a line or to a point) is easy to interpret - the term is not providing benefit and can be removed.\nWhen the range of the blue dots (shrunk values) is comparable to those of the red dots (unshrunk) it indicates that the term after shrinkage is about as strong as without shrinkage.\nBy itself, this doesn‚Äôt mean that the term is important. In some ways you need to get a feeling for the absolute magnitude of the random effects in addition to the relative magnitude.\nSmall magnitude and small relative magnitude indicate you can drop that term"
  },
  {
    "objectID": "shrinkageplot.html#conclusions-from-these-plots",
    "href": "shrinkageplot.html#conclusions-from-these-plots",
    "title": "More on shrinkage plots",
    "section": "Conclusions from these plots",
    "text": "Conclusions from these plots\n\nOnly the intercept for the subj appears to be contributing explanatory power\nFor the item both the intercept and the spkr appear to be contributing\n\nm2 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec * spkr * load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n<1e-99\n364.7286\n298.1109\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n<1e-11\n252.6687\n\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n<1e-06\n\n\n\n\nprec: maintain & spkr: old\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & spkr: old & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.917\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.081\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\nCode\nshrinkageplot(m2)\n\n\n\n\n\nFigure 2: Shrinkage plot of model m2\n\n\n\n\nm3 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\nVarCorr(m3)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\nrng = Random.seed!(1234321);\nm3btstrp = parametricbootstrap(rng, 2000, m3);\n\nDataFrame(shortestcovint(m3btstrp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2013.952319.532Œ≤missingprec: maintain-429.807-241.4293Œ≤missingspkr: old35.333995.72724Œ≤missingload: yes47.067111.045œÉitem(Intercept)267.788452.96œÉitemprec: maintain171.547314.7027œÅitem(Intercept), prec: maintain-0.89308-0.4570848œÉsubj(Intercept)235.921364.7179œÉresidualmissing657.736703.054\n\n\n\nridgeplot(m3btstrp)\n\n\n\n\nFigure 3: Ridge plot of the fixed-effects coefficients from the bootstrap sample\n\n\n\n\nridgeplot(m3btstrp; show_intercept=false)\nm4 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 + prec | item) + (1 | subj)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\nm4bstrp = parametricbootstrap(rng, 2000, m4);\n\nridgeplot(m4bstrp; show_intercept=false)\n\n\n\n\n\nDataFrame(shortestcovint(m4bstrp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2008.722319.82Œ≤missingprec: maintain-433.808-248.8583Œ≤missingspkr: old35.472997.95764Œ≤missingload: yes47.0078108.4375œÉitem(Intercept)261.52444.4266œÉitemprec: maintain177.437318.8467œÅitem(Intercept), prec: maintain-0.898508-0.4773468œÉsubj(Intercept)229.031356.4079œÉresidualmissing656.919701.946\n\n\nVarCorr(m4)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\nCode\nlet mods = [m1, m2, m4]\n  DataFrame(;\n    geomdof=(sum ‚àò leverage).(mods),\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsgeomdofnpardevianceAICBICAICcFloat64Int64Float64Float64Float64Float641131.552928637.128695.128854.328696.12107.5431328658.528684.528755.828684.73103.478928663.928681.928731.328682.0\n\n\n\nscatter(fitted(m4), residuals(m4))\n\n\n\n\nFigure 4: Residuals versus fitted values for model m4"
  },
  {
    "objectID": "fggk21.html",
    "href": "fggk21.html",
    "title": "Basics with Emotikon Project",
    "section": "",
    "text": "This script uses a subset of data reported in F√ºhner et al. (2021). To circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the article.\nThe script is structured in four main sections:"
  },
  {
    "objectID": "fggk21.html#packages-and-functions",
    "href": "fggk21.html#packages-and-functions",
    "title": "Basics with Emotikon Project",
    "section": "Packages and functions",
    "text": "Packages and functions\n\nCode\nusing AlgebraOfGraphics\nusing AlgebraOfGraphics: linear\nusing Arrow\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing MixedModelsMakie: simplelinreg\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\nusing Random\nusing Statistics\nusing StatsBase\ndatadir = joinpath(@__DIR__, \"data\")\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")"
  },
  {
    "objectID": "fggk21.html#readme",
    "href": "fggk21.html#readme",
    "title": "Basics with Emotikon Project",
    "section": "Readme",
    "text": "Readme\nNumber of scores: 525126 in ‚Äò./data/fggk21.arrow‚Äô\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: ‚ÄúGirls‚Äù (n=55,086), ‚ÄúBoys‚Äù (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units"
  },
  {
    "objectID": "fggk21.html#preprocessing",
    "href": "fggk21.html#preprocessing",
    "title": "Basics with Emotikon Project",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nRead data\n\ndf = @chain \"fggk21.arrow\" begin\n  joinpath(datadir, _)\n  Arrow.Table\n  DataFrame\n  transform(\n    :age => (x -> x .- 8.5) => :a1,\n    :Sex => categorical => :Sex,\n    :Test => categorical => :Test,\n  )\nend\n#levels!(df.Sex, [\"Boys\", \"Girls\"])\nlevels!(df.Test, [\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\nrecode!(\n  df.Test,\n  \"Run\" => \"Endurance\",\n  \"Star_r\" => \"Coordination\",\n  \"S20_r\" => \"Speed\",\n  \"SLJ\" => \"PowerLOW\",\n  \"BPT\" => \"PowerUP\",\n)\ndescribe(df)\n\n\n8 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1Cohort201120190String2SchoolS100043S8002000String3ChildC002352C1179660String4Sexfemalemale0CategoricalValue{String, UInt32}5age8.560737.994528.558529.106090Float646TestEndurancePowerUP0CategoricalValue{String, UInt32}7score226.1411.141524.651161530.00Float648a10.0607297-0.5054760.05852160.6060920Float64\n\n\n\nTransformations\nWe center age at 8.5 years and compute z-scores for each Test. With these variables the data frame df contains all variables used for the final model in the original publication.\n\nselect!(groupby(df, :Test), :, :score => zscore => :zScore)\ndfx = df[:, Not(:score)]\n\n\n525,126 rows √ó 8 columnsTestCohortSchoolChildSexagea1zScoreCat‚Ä¶StringStringStringCat‚Ä¶Float64Float64Float641Speed2013S100067C002352male7.99452-0.5054761.79132PowerUP2013S100067C002352male7.99452-0.505476-0.06223173PowerLOW2013S100067C002352male7.99452-0.505476-0.03365674Coordination2013S100067C002352male7.99452-0.5054761.468745Endurance2013S100067C002352male7.99452-0.5054760.3310586Speed2013S100067C002353male7.99452-0.5054761.154717PowerUP2013S100067C002353male7.99452-0.5054760.4983548PowerLOW2013S100067C002353male7.99452-0.505476-0.4988229Coordination2013S100067C002353male7.99452-0.505476-0.977310Endurance2013S100067C002353male7.99452-0.5054760.57405611Speed2013S100067C002354male7.99452-0.5054760.055148112PowerUP2013S100067C002354male7.99452-0.5054760.21806113PowerLOW2013S100067C002354male7.99452-0.505476-0.75724814Coordination2013S100067C002354male7.99452-0.505476-0.20918615Endurance2013S100067C002354male7.99452-0.505476-0.94468116Speed2013S100122C002355female7.99452-0.5054760.055148117PowerUP2013S100122C002355female7.99452-0.505476-1.0432618PowerLOW2013S100122C002355female7.99452-0.505476-0.60219319Coordination2013S100122C002355female7.99452-0.505476-0.7101320Endurance2013S100122C002355female7.99452-0.505476-1.1404321Speed2013S100146C002356male7.99452-0.505476-0.42292122PowerUP2013S100146C002356male7.99452-0.505476-0.62281723PowerLOW2013S100146C002356male7.99452-0.505476-0.39545224Coordination2013S100146C002356male7.99452-0.505476-0.49399225Endurance2013S100146C002356male7.99452-0.505476-0.9716826Speed2013S100146C002357male7.99452-0.505476-0.42292127PowerUP2013S100146C002357male7.99452-0.5054760.77864628PowerLOW2013S100146C002357male7.99452-0.5054760.22476929Coordination2013S100146C002357male7.99452-0.505476-0.18207630Endurance2013S100146C002357male7.99452-0.505476-0.296686‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\n\nExtract a stratified subsample\nFor the prupose of the tutorial, we extract a random sample of 1000 boys and 1000 girls. Child, School, and Cohort are grouping variables. Traditionally, they are called random factors because the units (levels) of the factor are assumed to be a random sample from the population of their units (levels).\nCohort has only nine ‚Äúgroups‚Äù and could have been included as a set of polynomical fixed-effect contrasts rather than a random factor. This choice warrants a short excursion: The secular trends are very different for different tests and require the inclusion of interaction terms with Test contrasts (see Figure 4 in (F√ºhner et al., 2021). The authors opted to absorb these effects in cohort-related variance components for the Test contrasts and plan to address the details of secular changes in a separate analysis.\nFor complex designs, when they are in the theoretical focus of an article, factors and covariates should be specified as part of the fixed effects. If they are not in the theoretical focus, but serve as statistical control variables, they could be put in the RES - if supported by the data.\nStratified sampling: We generate a Child table with information about children. MersenneTwister(42) specifies 42 as the seed for the random number generator to ensure reproducibility of the stratification. For a different pattern of results choose, for example, 84. We randomly sample 1000 boys and 1000 girls from this table; they are stored in samp. Then, we extract the corresponding subset of these children‚Äôs test scores from df and store them dat.\n\nChild = unique(select(df, :Cohort, :School, :Child, :Sex, :age))\nsample = let\n  rng = MersenneTwister(42)\n  combine(\n    groupby(Child, :Sex), x -> x[rand(rng, 1:nrow(x), 1000), :]\n  )\nend\ninsamp(x) = x ‚àà sample.Child\ndat = @subset(df, insamp(:Child))\n\n\n9,615 rows √ó 9 columnsTestCohortSchoolChildSexagescorea1zScoreCat‚Ä¶StringStringStringCat‚Ä¶Float64Float64Float64Float641Speed2013S101825C002497male7.994524.08163-0.505476-1.066852PowerUP2013S101825C002497male7.994523.5-0.505476-0.3425243PowerLOW2013S101825C002497male7.99452140.0-0.5054760.741624Coordination2013S101825C002497male7.994522.02032-0.505476-0.09945485Endurance2013S101825C002497male7.994521089.0-0.5054760.5740566Speed2013S102090C002514male7.994524.44444-0.505476-0.1891997PowerUP2013S102090C002514male7.994524.1-0.5054760.4983548PowerLOW2013S102090C002514male7.99452120.0-0.505476-0.2920829Coordination2013S102090C002514male7.994522.09514-0.5054760.16064810Endurance2013S102090C002514male7.99452927.0-0.505476-0.51943411Speed2013S103366C002633male7.994524.7619-0.5054760.57874812PowerUP2013S103366C002633male7.994523.9-0.5054760.21806113PowerLOW2013S103366C002633male7.99452138.0-0.5054760.63824914Coordination2013S103366C002633male7.994522.33541-0.5054760.99583915Endurance2013S103366C002633male7.994521007.0-0.5054760.020560716Speed2013S104000C002710male7.994525.12821-0.5054761.4648417PowerUP2013S104000C002710male7.994524.5-0.5054761.0589418PowerLOW2013S104000C002710male7.99452165.0-0.5054762.0337519Coordination2013S104000C002710male7.994522.62433-0.5054762.0001320Endurance2013S104000C002710male7.994521500.0-0.5054763.3482821Speed2013S104255C002715male7.994524.44444-0.505476-0.18919922PowerUP2013S104255C002715male7.994523.5-0.505476-0.34252423PowerLOW2013S104255C002715male7.99452128.0-0.5054760.12139924Coordination2013S104255C002715male7.994521.98875-0.505476-0.20918625Endurance2013S104255C002715male7.99452960.0-0.505476-0.29668626Speed2013S104632C002732male7.994525.12821-0.5054761.4648427PowerUP2013S104632C002732male7.994524.7-0.5054761.3392328PowerLOW2013S104632C002732male7.99452159.0-0.5054761.7236429Coordination2013S104632C002732male7.994522.01233-0.505476-0.12721330Endurance2013S104632C002732male7.994521071.0-0.5054760.452557‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nDue to missing scores for some tests we have about 2% less than 10,000 observtions.\n\n\nNo evidence for age x Sex x Test interaction\nThe main results are captured in the figure constructed in this section. We build it both for the full data and the stratified subset.\n\ndf2 = combine(\n  groupby(\n    select(df, :, :age => ByRow(x -> round(x; digits=1)) => :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore => mean => :zScore,\n  :zScore => length => :n,\n)\n\n\n120 rows √ó 5 columnsSexTestagezScorenCat‚Ä¶Cat‚Ä¶Float64Float64Int641maleSpeed8.0-0.026513812232malePowerUP8.00.02697312273malePowerLOW8.00.12160912274maleCoordination8.0-0.057172611865maleEndurance8.00.29269512106femaleSpeed8.0-0.3516414117femalePowerUP8.0-0.61035514178femalePowerLOW8.0-0.27987214189femaleCoordination8.0-0.268221138110femaleEndurance8.0-0.245573138711maleSpeed8.10.0608397304212malePowerUP8.10.0955413306913malePowerLOW8.10.123099306914maleCoordination8.1-0.0112098298615maleEndurance8.10.228915298916femaleSpeed8.1-0.25567339517femalePowerLOW8.1-0.283061342618femaleCoordination8.1-0.248837336219femaleEndurance8.1-0.267019336820femalePowerUP8.1-0.577338343921femaleSpeed8.2-0.250296541522femalePowerUP8.2-0.490636550723femalePowerLOW8.2-0.259983548924femaleCoordination8.2-0.207378539825femaleEndurance8.2-0.262365539826maleSpeed8.20.053516475227malePowerUP8.20.141768481128malePowerLOW8.20.109677480229maleCoordination8.2-0.00553236468030maleEndurance8.20.2301484714‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nFigure(s) of interaction\nThe core results of the article are reported in Figure 2 of F√ºhner et al. (2021). In summary:\n\nMain effects of age and Sex: There are developmental gains in the ninth year of life; boys outperform girls. There is no main effect of Test because of z-scoring.\nInteractions of Test and age: Tests differ in how much children improve during the year (i.e., the magnitude of developmental gain), that is slopes depend on Test.\nInteractions of Test and Sex: The sex difference is test dependent, that is the difference between the slopes depends on Test.\nThe most distinctive result is the absence of evidence for an age x Sex x Test interaction, that is the slopes for boys and girls are statistically parallel for each of the five tests.\n\n\n\nCode\nlet\n  design1 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines1 = design1 * linear()\n  means1 = design1 * visual(Scatter; markersize=5)\n  draw(data(df2) * means1 + data(df) * lines1;)\nend\n\n\n\n\n\nFigure 1: Age trends by sex for each Test for the full data set\n\n\n\n\nFigure¬†1 shows performance differences for the full set of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\nWhat do the results look like for the stratified subsample? Here the parallelism is much less clear. In the final LMM we test whether the two regression lines in each of the five panels are statistically parallel for this subset of data. That is, we test the interaction of Sex and age as nested within the levels of Test. Most people want to know the signficance of these five Sex x age interactions.\nThe theoretical focus of the article, however, is on comparisons between tests displayed next to each other. We ask whether the degree of parallelism is statistically the same for Endurance and Coordination (H1), Coordination and Speed (H2), Speed and PowerLOW (H3), and PowerLow and PowerUP (H4). Hypotheses H1 to H4 require Sequential Difference contrasts c1 to c4 for Test; they are tested as fixed effects for`H1 x age x Sex, H2 x age x Sex, H3 x age x Sex, and H4 x age x Sex.\n\n\nCode\ndat2 = combine(\n  groupby(\n    select(dat, :, :age => ByRow(x -> round(x; digits=1)) => :age),\n    [:Sex, :Test, :age],\n  ),\n  :zScore => mean => :zScore,\n  :zScore => length => :n,\n)\n\n\n\n120 rows √ó 5 columnsSexTestagezScorenCat‚Ä¶Cat‚Ä¶Float64Float64Int641maleSpeed8.00.150743252malePowerUP8.00.0779146243malePowerLOW8.00.412902254maleCoordination8.00.188266255maleEndurance8.00.325118256femaleSpeed8.0-0.616063207femalePowerUP8.0-0.307488208femalePowerLOW8.0-0.354104209femaleCoordination8.0-0.1798971710femaleEndurance8.00.2488671711maleSpeed8.10.3740195212malePowerUP8.1-0.04875615213malePowerLOW8.10.270495214maleCoordination8.10.01473545115maleEndurance8.10.5606915016femaleSpeed8.1-0.3012876717femalePowerUP8.1-0.5801646918femalePowerLOW8.1-0.3108096919femaleCoordination8.1-0.1176156720femaleEndurance8.1-0.3707346721femaleSpeed8.2-0.2347110622femalePowerUP8.2-0.46228611023femalePowerLOW8.2-0.2271210924femaleCoordination8.2-0.16762510625femaleEndurance8.2-0.17412510826maleSpeed8.2-0.154857927malePowerUP8.2-0.03352718328malePowerLOW8.20.05376138129maleCoordination8.2-0.2245688330maleEndurance8.20.18542880‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\nCode\nlet\n  design2 = mapping(:age, :zScore; color=:Sex, col=:Test)\n  lines2 = design2 * linear()\n  means2 = design2 * visual(Scatter; markersize=5)\n  draw(data(dat2) * means2 + data(dat) * lines2;)\nend\n\n\n\n\n\nFigure 2: Age trends by sex for each Test for the stratified sample\n\n\n\n\nFigure¬†2 Performance differences for subset of data between 8.0 and 9.2 years by sex in the five physical fitness tests presented as z-transformed data computed separately for each test.\n\nEndurance = cardiorespiratory endurance (i.e., 6-min-run test),\nCoordination = star-run test,\nSpeed = 20-m linear sprint test,\nPowerLOW = power of lower limbs (i.e., standing long jump test),\nPowerUP = power of upper limbs (i.e., ball push test),\nSD = standard deviation. Points are binned observed child means; lines are simple regression fits to the observations.\n\n\n\nRegression on age by Sex for each Test\nAnother set of relevant statistics are the slopes for the regression of performance on age for boys and girls in each of the five tests. The lines in Figures 1 and 2, however, are computed directly from the raw data with the linear() command.\n\ncombine(\n  groupby(df, [:Sex, :Test]),\n  [:age, :zScore] => simplelinreg => :coef,\n)\n\n\n10 rows √ó 3 columnsSexTestcoefCat‚Ä¶Cat‚Ä¶Tuple‚Ä¶1femaleEndurance(-0.692022, 0.0523217)2femaleCoordination(-2.50524, 0.279119)3femaleSpeed(-2.34431, 0.255687)4femalePowerLOW(-1.87241, 0.196917)5femalePowerUP(-4.82271, 0.524799)6maleEndurance(0.00256718, 0.0291899)7maleCoordination(-2.47279, 0.302819)8maleSpeed(-2.12689, 0.267153)9malePowerLOW(-1.4307, 0.189659)10malePowerUP(-4.35864, 0.549005)\n\n\n\ncombine(\n  groupby(dat, [:Sex, :Test]),\n  [:age, :zScore] => simplelinreg => :coef,\n)\n\n\n10 rows √ó 3 columnsSexTestcoefCat‚Ä¶Cat‚Ä¶Tuple‚Ä¶1femaleEndurance(0.0307573, -0.0338037)2femaleCoordination(-1.03107, 0.105189)3femaleSpeed(-2.00661, 0.212806)4femalePowerLOW(-1.04703, 0.102421)5femalePowerUP(-4.03432, 0.431796)6maleEndurance(0.922801, -0.0732062)7maleCoordination(-3.1484, 0.384406)8maleSpeed(-1.87647, 0.241661)9malePowerLOW(-0.576212, 0.0897968)10malePowerUP(-5.60153, 0.688667)\n\n\n\n\n\nSeqDiffCoding of Test\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nH1: Star_r - Run (2-1)\nH2: S20_r - Star_r (3-2)\nH3: SLJ - S20_r (4-3)\nH4: BPT - SLJ (5-4)\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe.\nNote that random factors Child, School, and Cohort are declared as Grouping variables. Technically, this specification is required for variables with a very large number of levels (e.g., 100K+ children). We recommend the explicit specification for all random factors as a general coding style.\nThe first command recodes names indicating the physical fitness components used in the above figures and tables back to the shorter actual test names. This reduces clutter in LMM outputs.\nrecode!(\n  dat.Test,\n  \"Endurance\" => \"Run\",\n  \"Coordination\" => \"Star_r\",\n  \"Speed\" => \"S20_r\",\n  \"PowerLOW\" => \"SLJ\",\n  \"PowerUP\" => \"BMT\",\n)\ncontrasts = merge(\n  Dict(nm => SeqDiffCoding() for nm in (:Test, :Sex)),\n  Dict(nm => Grouping() for nm in (:Child, :School, :Cohort)),\n);\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitations on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\nVarious options for contrast coding are the topic of the MixedModelsTutorial_contrasts_emotikon.jl and MixedModelsTutorial_contrasts_kwdyz.jl notebooks."
  },
  {
    "objectID": "fggk21.html#model-complexification",
    "href": "fggk21.html#model-complexification",
    "title": "Basics with Emotikon Project",
    "section": "Model complexification",
    "text": "Model complexification\nWe fit and compare three LMMs with the same fixed-effect structure but increasing complexity of the random-effect structure for School. We ignore the other two random factors Child and Cohort to avoid undue delays when fitting the models.\n\nLMM m_ovi: allowing only varying intercepts (‚ÄúGrand Means‚Äù);\nLMM m_zcp: adding variance components (VCs) for the four Test contrasts, Sex, and age to LMM m_ovi, yielding the zero-correlation parameters LMM;\nLMM m_cpx: adding correlation parameters (CPs) to LMM m_zcp; yielding a complex LMM.\n\nIn a final part illustrate how to check whether the complex model is supported by the data, rather than leading to a singular fit and, if supported by the data, whether there is an increase in goodness of fit associated with the model complexification.\n\nLMM m_ovi\nIn its random-effect structure (RES) we only vary intercepts (i.e., Grand Means) for School (LMM m_ovi), that is we allow that the schools differ in the average fitness of its children, average over the five tests.\nIt is well known that such a simple RES is likely to be anti-conservative with respect to fixed-effect test statistics.\n\nm_ovi = let\n  f = @formula zScore ~ 1 + Test * Sex * a1 + (1 | School)\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 17    Time: 0:00:00 (15.76 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0327\n0.0185\n-1.77\n0.0773\n0.3082\n\n\nTest: Star_r\n-0.0302\n0.0308\n-0.98\n0.3274\n\n\n\nTest: S20_r\n-0.0011\n0.0307\n-0.04\n0.9719\n\n\n\nTest: SLJ\n0.0132\n0.0306\n0.43\n0.6661\n\n\n\nTest: BMT\n-0.0590\n0.0305\n-1.94\n0.0527\n\n\n\nSex: male\n0.4210\n0.0209\n20.18\n<1e-89\n\n\n\na1\n0.2362\n0.0357\n6.61\n<1e-10\n\n\n\nTest: Star_r & Sex: male\n-0.3026\n0.0616\n-4.91\n<1e-06\n\n\n\nTest: S20_r & Sex: male\n0.1129\n0.0615\n1.84\n0.0662\n\n\n\nTest: SLJ & Sex: male\n-0.0073\n0.0612\n-0.12\n0.9049\n\n\n\nTest: BMT & Sex: male\n0.2534\n0.0609\n4.16\n<1e-04\n\n\n\nTest: Star_r & a1\n0.3003\n0.1056\n2.84\n0.0044\n\n\n\nTest: S20_r & a1\n-0.0146\n0.1050\n-0.14\n0.8893\n\n\n\nTest: SLJ & a1\n-0.1331\n0.1046\n-1.27\n0.2032\n\n\n\nTest: BMT & a1\n0.4661\n0.1043\n4.47\n<1e-05\n\n\n\nSex: male & a1\n0.1140\n0.0712\n1.60\n0.1095\n\n\n\nTest: Star_r & Sex: male & a1\n0.3349\n0.2112\n1.59\n0.1127\n\n\n\nTest: S20_r & Sex: male & a1\n-0.2487\n0.2100\n-1.18\n0.2363\n\n\n\nTest: SLJ & Sex: male & a1\n-0.0512\n0.2091\n-0.25\n0.8064\n\n\n\nTest: BMT & Sex: male & a1\n0.2678\n0.2087\n1.28\n0.1994\n\n\n\nResidual\n0.9295\n\n\n\n\n\n\n\n\n\nIs the model singular (overparameterized, degenerate)? In other words: Is the model not supported by the data?\n\nissingular(m_ovi)\n\nfalse\n\n\nModels varying only in intercepts are almost always supported by the data.\n\n\nLMM m_zcp\nIn this LMM we allow that schools differ not only in GM, but also in the size of the four contrasts defined for Test, in the difference between boys and girls (Sex) and the developmental gain children achieve within the third grade (age).\nWe assume that there is covariance associated with these CPs beyond residual noise, that is we assume that there is no detectable evidence in the data that the CPs are different from zero.\n\nm_zcp = let\n  f = @formula(\n    zScore ~\n      1 + Test * Sex * a1 + zerocorr(1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 191   Time: 0:00:00 ( 1.26 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0301\n0.0187\n-1.61\n0.1075\n0.2927\n\n\nTest: Star_r\n-0.0274\n0.0302\n-0.91\n0.3650\n0.1404\n\n\nTest: S20_r\n-0.0025\n0.0298\n-0.08\n0.9323\n0.1186\n\n\nTest: SLJ\n0.0121\n0.0296\n0.41\n0.6825\n0.1118\n\n\nTest: BMT\n-0.0538\n0.0301\n-1.79\n0.0736\n0.1539\n\n\nSex: male\n0.4241\n0.0330\n12.84\n<1e-37\n0.4668\n\n\na1\n0.2498\n0.0575\n4.35\n<1e-04\n0.7804\n\n\nTest: Star_r & Sex: male\n-0.3046\n0.0586\n-5.20\n<1e-06\n\n\n\nTest: S20_r & Sex: male\n0.1115\n0.0583\n1.91\n0.0561\n\n\n\nTest: SLJ & Sex: male\n-0.0031\n0.0580\n-0.05\n0.9573\n\n\n\nTest: BMT & Sex: male\n0.2550\n0.0580\n4.40\n<1e-04\n\n\n\nTest: Star_r & a1\n0.2902\n0.1004\n2.89\n0.0038\n\n\n\nTest: S20_r & a1\n-0.0024\n0.0997\n-0.02\n0.9809\n\n\n\nTest: SLJ & a1\n-0.1387\n0.0992\n-1.40\n0.1622\n\n\n\nTest: BMT & a1\n0.4658\n0.0992\n4.69\n<1e-05\n\n\n\nSex: male & a1\n0.0438\n0.0812\n0.54\n0.5900\n\n\n\nTest: Star_r & Sex: male & a1\n0.3487\n0.2007\n1.74\n0.0823\n\n\n\nTest: S20_r & Sex: male & a1\n-0.2556\n0.1993\n-1.28\n0.1996\n\n\n\nTest: SLJ & Sex: male & a1\n-0.0561\n0.1983\n-0.28\n0.7771\n\n\n\nTest: BMT & Sex: male & a1\n0.2776\n0.1984\n1.40\n0.1618\n\n\n\nResidual\n0.8780\n\n\n\n\n\n\n\n\n\nDepending on sampling, this model estimating variance components for School may or may not be supported by the data.\n\nissingular(m_zcp)\n\nfalse\n\n\n\n\nLMM m_cpx\nIn the complex LMM investigated in this sequence we give up the assumption of zero-correlation between VCs.\n\nm_cpx = let\n  f = @formula(\n    zScore ~ 1 + Test * Sex * a1 + (1 + Test + Sex + a1 | School)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1713      Time: 0:00:01 ( 0.59 ms/it)\n  objective:  26013.411466300993\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_School\n\n\n\n\n(Intercept)\n-0.0313\n0.0188\n-1.66\n0.0971\n0.2971\n\n\nTest: Star_r\n-0.0260\n0.0316\n-0.82\n0.4112\n0.2249\n\n\nTest: S20_r\n-0.0054\n0.0311\n-0.17\n0.8613\n0.2015\n\n\nTest: SLJ\n0.0139\n0.0301\n0.46\n0.6455\n0.1606\n\n\nTest: BMT\n-0.0477\n0.0300\n-1.59\n0.1114\n0.1640\n\n\nSex: male\n0.4245\n0.0330\n12.87\n<1e-37\n0.4670\n\n\na1\n0.2531\n0.0580\n4.36\n<1e-04\n0.7967\n\n\nTest: Star_r & Sex: male\n-0.3070\n0.0587\n-5.23\n<1e-06\n\n\n\nTest: S20_r & Sex: male\n0.1104\n0.0585\n1.89\n0.0590\n\n\n\nTest: SLJ & Sex: male\n-0.0015\n0.0579\n-0.03\n0.9795\n\n\n\nTest: BMT & Sex: male\n0.2614\n0.0576\n4.54\n<1e-05\n\n\n\nTest: Star_r & a1\n0.2885\n0.1006\n2.87\n0.0041\n\n\n\nTest: S20_r & a1\n-0.0005\n0.0999\n-0.01\n0.9959\n\n\n\nTest: SLJ & a1\n-0.1389\n0.0990\n-1.40\n0.1606\n\n\n\nTest: BMT & a1\n0.4584\n0.0986\n4.65\n<1e-05\n\n\n\nSex: male & a1\n0.0425\n0.0810\n0.52\n0.5999\n\n\n\nTest: Star_r & Sex: male & a1\n0.3488\n0.2012\n1.73\n0.0829\n\n\n\nTest: S20_r & Sex: male & a1\n-0.2506\n0.1997\n-1.26\n0.2095\n\n\n\nTest: SLJ & Sex: male & a1\n-0.0645\n0.1979\n-0.33\n0.7444\n\n\n\nTest: BMT & Sex: male & a1\n0.2822\n0.1971\n1.43\n0.1524\n\n\n\nResidual\n0.8736\n\n\n\n\n\n\n\n\n\nWe also need to see the VCs and CPs of the random-effect structure (RES).\nVarCorr(m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.088280\n0.297120\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.050580\n0.224901\n+0.14\n\n\n\n\n\n\n\n\nTest: S20_r\n0.040588\n0.201465\n+0.01\n-0.40\n\n\n\n\n\n\n\nTest: SLJ\n0.025807\n0.160645\n-0.15\n-0.47\n-0.33\n\n\n\n\n\n\nTest: BMT\n0.026909\n0.164040\n-0.56\n+0.19\n-0.21\n+0.37\n\n\n\n\n\nSex: male\n0.218091\n0.467002\n+0.13\n+0.07\n+0.02\n-0.23\n+0.36\n\n\n\n\na1\n0.634688\n0.796673\n-0.11\n-0.06\n-0.06\n-0.07\n-0.12\n-0.08\n\n\nResidual\n\n0.763258\n0.873646\n\n\n\n\n\n\n\n\n\n\nissingular(m_cpx)\n\ntrue\n\n\nThe complex model may or may not be supported by the data.\n\n\nModel comparisons\nThe checks of model singularity indicate that the three models are supported by the data. Does model complexification also increase the goodness of fit or are we only fitting noise?\n\nLRT and goodness-of-fit statistics\nAs the thee models are strictly hierarchically nested, we compare them with a likelihood-ratio tests (LRT) and AIC and BIC goodness-of-fit statistics derived from them.\nMixedModels.likelihoodratiotest(m_ovi, m_zcp, m_cpx)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 | School)\n22\n26373\n\n\n\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + MixedModels.ZeroCorr((1 + Test + Sex + a1 | School))\n28\n26045\n328\n6\n<1e-67\n\n\nzScore ~ 1 + Test + Sex + a1 + Test & Sex + Test & a1 + Sex & a1 + Test & Sex & a1 + (1 + Test + Sex + a1 | School)\n49\n26013\n31\n21\n0.0675\n\n\n\n\n\nCode\ngof_summary = let\n  nms = [:m_ovi, :m_zcp, :m_cpx]\n  mods = eval.(nms)\n  DataFrame(;\n    name=nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsnamedofdevianceAICAICcBICSymbolInt64Float64Float64Float64Float641m_ovi2226373.126417.126417.326574.92m_zcp2826044.826100.826101.026301.63m_cpx4926013.426111.426111.926462.8\n\n\nThese statistics will depend on sampling. In general, smaller deviance, AIC, and BIC indicate an improvement in goodness of fit. Usually, œá¬≤ should be larger than the associated degrees of freedom; for AIC and BIC the decrease should amount to more than 5, according to some literature. Severity of meeting these criteria increases from deviance to AIC to BIC. Therefore, it is not always the case that the criteria are unanimous in their verdict. Basicly, the more confirmatory the analysis, the more one may go with deviance and AIC; for exploratory analyses the BIC is probably a better guide. There are grey zones here.\n\n\nComparing fixed effects of m_ovi, m_zcp, and m_cpx\nWe check whether enriching the RES changed the significance of fixed effects in the final model.\n\n\nCode\nm_ovi_fe = DataFrame(coeftable(m_ovi));\nm_zcp_fe = DataFrame(coeftable(m_zcp));\nm_cpx_fe = DataFrame(coeftable(m_cpx));\nm_all = hcat(\n  m_ovi_fe[:, [1, 2, 4]],\n  leftjoin(\n    m_zcp_fe[:, [1, 2, 4]],\n    m_cpx_fe[:, [1, 2, 4]];\n    on=:Name,\n    makeunique=true,\n  );\n  makeunique=true,\n)\nrename!(\n  m_all,\n  \"Coef.\" => \"b_ovi\",\n  \"Coef._2\" => \"b_zcp\",\n  \"Coef._1\" => \"b_cpx\",\n  \"z\" => \"z_ovi\",\n  \"z_2\" => \"z_zcp\",\n  \"z_1\" => \"z_cpx\",\n)\nm_all2 =\n  round.(\n    m_all[:, [:b_ovi, :b_zcp, :b_cpx, :z_ovi, :z_zcp, :z_cpx]],\n    digits=2,\n  )\nm_all3 = hcat(m_all.Name, m_all2)\n\n\n\n20 rows √ó 7 columnsx1b_ovib_zcpb_cpxz_oviz_zcpz_cpxStringFloat64Float64Float64Float64Float64Float641(Intercept)-0.03-0.03-0.03-1.77-1.61-1.662Test: Star_r-0.03-0.03-0.03-0.98-0.91-0.823Test: S20_r-0.0-0.0-0.01-0.04-0.08-0.174Test: SLJ0.010.010.010.430.410.465Test: BMT-0.06-0.05-0.05-1.94-1.79-1.596Sex: male0.420.420.4220.1812.8412.877a10.240.250.256.614.354.368Test: Star_r & Sex: male-0.3-0.3-0.31-4.91-5.2-5.239Test: S20_r & Sex: male0.110.110.111.841.911.8910Test: SLJ & Sex: male-0.01-0.0-0.0-0.12-0.05-0.0311Test: BMT & Sex: male0.250.260.264.164.44.5412Test: Star_r & a10.30.290.292.842.892.8713Test: S20_r & a1-0.01-0.0-0.0-0.14-0.02-0.0114Test: SLJ & a1-0.13-0.14-0.14-1.27-1.4-1.415Test: BMT & a10.470.470.464.474.694.6516Sex: male & a10.110.040.041.60.540.5217Test: Star_r & Sex: male & a10.330.350.351.591.741.7318Test: S20_r & Sex: male & a1-0.25-0.26-0.25-1.18-1.28-1.2619Test: SLJ & Sex: male & a1-0.05-0.06-0.06-0.25-0.28-0.3320Test: BMT & Sex: male & a10.270.280.281.281.41.43\n\n\nThe three models usually do not differ in fixed-effect estimates. For main effects of age and Sex, z-values decrease strongly with the complexity of the model (i.e., standard errors are larger). For other coefficients, the changes are not very large and not consistent.\nIn general, dropping significant variance components and/or correlation parameters may lead to anti-conservative estimates of fixed effects (e.g., Schielzeth & Forstmeier, 2008). Basically, some of the variance allocated to age and Sex in LMM m_ovi could also be due to differences between schools. This ambiguity increased the uncertainty of the respective fixed effects in the other two LMMs.\n\n\n\nFitting an overparameterized LMM\nThe complex LMM was not overparameterized with respect to School, because there are over 400 schools in the data. When the number of units (levels) of a grouping factor is small relative to the number of parameters we are trying to estimate, we often end up with an overparameterized / degenerate random-effect structure.\nAs an illustration, we fit a full CP matrix for the Cohort. As there are only nine cohorts in the data, we may be asking too much to estimate 5*6/2 = 15 VC/CP parameters.\nm_cpxCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 + Test | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0113\n0.0141\n-0.80\n0.4230\n0.0291\n\n\nTest: Star_r\n-0.0340\n0.0358\n-0.95\n0.3419\n0.0464\n\n\nTest: S20_r\n-0.0067\n0.0401\n-0.17\n0.8670\n0.0715\n\n\nTest: SLJ\n0.0208\n0.0399\n0.52\n0.6017\n0.0709\n\n\nTest: BMT\n-0.0637\n0.0358\n-1.78\n0.0748\n0.0480\n\n\na1\n0.2095\n0.0351\n5.96\n<1e-08\n\n\n\nSex: male\n0.4341\n0.0203\n21.36\n<1e-99\n\n\n\nTest: Star_r & a1\n0.2871\n0.1108\n2.59\n0.0096\n\n\n\nTest: S20_r & a1\n-0.0408\n0.1107\n-0.37\n0.7123\n\n\n\nTest: SLJ & a1\n-0.1086\n0.1102\n-0.99\n0.3244\n\n\n\nTest: BMT & a1\n0.4528\n0.1098\n4.12\n<1e-04\n\n\n\nTest: Star_r & Sex: male\n-0.2996\n0.0646\n-4.64\n<1e-05\n\n\n\nTest: S20_r & Sex: male\n0.1203\n0.0644\n1.87\n0.0618\n\n\n\nTest: SLJ & Sex: male\n-0.0131\n0.0641\n-0.20\n0.8385\n\n\n\nTest: BMT & Sex: male\n0.2528\n0.0639\n3.96\n<1e-04\n\n\n\na1 & Sex: male\n0.1032\n0.0695\n1.48\n0.1379\n\n\n\nTest: Star_r & a1 & Sex: male\n0.3178\n0.2212\n1.44\n0.1507\n\n\n\nTest: S20_r & a1 & Sex: male\n-0.2518\n0.2200\n-1.14\n0.2525\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0400\n0.2191\n-0.18\n0.8550\n\n\n\nTest: BMT & a1 & Sex: male\n0.2671\n0.2187\n1.22\n0.2219\n\n\n\nResidual\n0.9739\n\n\n\n\n\n\n\nVarCorr(m_cpxCohort)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nCohort\n(Intercept)\n0.0008484\n0.0291275\n\n\n\n\n\n\n\nTest: Star_r\n0.0021498\n0.0463656\n+0.93\n\n\n\n\n\n\nTest: S20_r\n0.0051130\n0.0715051\n+0.70\n+0.91\n\n\n\n\n\nTest: SLJ\n0.0050332\n0.0709452\n-0.74\n-0.93\n-1.00\n\n\n\n\nTest: BMT\n0.0023066\n0.0480275\n-0.05\n+0.32\n+0.68\n-0.64\n\n\nResidual\n\n0.9484911\n0.9739051\n\n\n\n\n\n\n\n\nissingular(m_cpxCohort)\n\ntrue\n\n\nThe model is overparameterized with several CPs estimated between |.98| and |1.00|. How about the zero-correlation parameter (zcp) version of this LMM?\nm_zcpCohort = let\n  f = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0101\n0.0132\n-0.76\n0.4455\n0.0252\n\n\nTest: Star_r\n-0.0320\n0.0345\n-0.93\n0.3532\n0.0363\n\n\nTest: S20_r\n-0.0027\n0.0341\n-0.08\n0.9367\n0.0333\n\n\nTest: SLJ\n0.0154\n0.0321\n0.48\n0.6317\n0.0000\n\n\nTest: BMT\n-0.0613\n0.0319\n-1.92\n0.0548\n0.0000\n\n\na1\n0.2142\n0.0352\n6.08\n<1e-08\n\n\n\nSex: male\n0.4340\n0.0203\n21.33\n<1e-99\n\n\n\nTest: Star_r & a1\n0.2925\n0.1111\n2.63\n0.0085\n\n\n\nTest: S20_r & a1\n-0.0248\n0.1104\n-0.22\n0.8224\n\n\n\nTest: SLJ & a1\n-0.1306\n0.1096\n-1.19\n0.2335\n\n\n\nTest: BMT & a1\n0.4642\n0.1094\n4.24\n<1e-04\n\n\n\nTest: Star_r & Sex: male\n-0.3003\n0.0646\n-4.65\n<1e-05\n\n\n\nTest: S20_r & Sex: male\n0.1193\n0.0644\n1.85\n0.0641\n\n\n\nTest: SLJ & Sex: male\n-0.0119\n0.0641\n-0.19\n0.8528\n\n\n\nTest: BMT & Sex: male\n0.2528\n0.0639\n3.96\n<1e-04\n\n\n\na1 & Sex: male\n0.1020\n0.0696\n1.47\n0.1428\n\n\n\nTest: Star_r & a1 & Sex: male\n0.3192\n0.2213\n1.44\n0.1492\n\n\n\nTest: S20_r & a1 & Sex: male\n-0.2509\n0.2202\n-1.14\n0.2545\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0415\n0.2192\n-0.19\n0.8498\n\n\n\nTest: BMT & a1 & Sex: male\n0.2695\n0.2188\n1.23\n0.2180\n\n\n\nResidual\n0.9746\n\n\n\n\n\n\n\n\nissingular(m_zcpCohort)\n\ntrue\n\n\nThis zcpLMM is also singular. Three of the five VCs are estimated as zero. This raises the possibility that LMM m_oviCohort might fit as well as LMM m_zcpCohort.\nm_oviCohort = let\n  f = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Cohort)\n  fit(MixedModel, f, dat; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0101\n0.0132\n-0.76\n0.4470\n0.0253\n\n\nTest: Star_r\n-0.0309\n0.0323\n-0.96\n0.3387\n\n\n\nTest: S20_r\n-0.0012\n0.0322\n-0.04\n0.9707\n\n\n\nTest: SLJ\n0.0153\n0.0321\n0.48\n0.6327\n\n\n\nTest: BMT\n-0.0613\n0.0320\n-1.92\n0.0552\n\n\n\na1\n0.2142\n0.0352\n6.08\n<1e-08\n\n\n\nSex: male\n0.4341\n0.0204\n21.33\n<1e-99\n\n\n\nTest: Star_r & a1\n0.2981\n0.1107\n2.69\n0.0071\n\n\n\nTest: S20_r & a1\n-0.0181\n0.1101\n-0.16\n0.8693\n\n\n\nTest: SLJ & a1\n-0.1308\n0.1096\n-1.19\n0.2329\n\n\n\nTest: BMT & a1\n0.4643\n0.1094\n4.24\n<1e-04\n\n\n\nTest: Star_r & Sex: male\n-0.3008\n0.0646\n-4.66\n<1e-05\n\n\n\nTest: S20_r & Sex: male\n0.1191\n0.0644\n1.85\n0.0645\n\n\n\nTest: SLJ & Sex: male\n-0.0118\n0.0642\n-0.18\n0.8535\n\n\n\nTest: BMT & Sex: male\n0.2528\n0.0639\n3.96\n<1e-04\n\n\n\na1 & Sex: male\n0.1019\n0.0696\n1.46\n0.1431\n\n\n\nTest: Star_r & a1 & Sex: male\n0.3186\n0.2214\n1.44\n0.1500\n\n\n\nTest: S20_r & a1 & Sex: male\n-0.2504\n0.2202\n-1.14\n0.2555\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0413\n0.2193\n-0.19\n0.8505\n\n\n\nTest: BMT & a1 & Sex: male\n0.2693\n0.2188\n1.23\n0.2185\n\n\n\nResidual\n0.9749\n\n\n\n\n\n\n\n\nissingular(m_oviCohort)\n\nfalse\n\n\nThis solves the problem with singularity, but does LMM m_zcpCohort fit noise relative to the LMM m_oviCohort?\nMixedModels.likelihoodratiotest(m_oviCohort, m_zcpCohort)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 | Cohort)\n22\n26802\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Cohort))\n26\n26801\n1\n4\n0.9239\n\n\n\n\ngof_summary2 = let\n  mods = [m_oviCohort, m_zcpCohort, m_cpxCohort]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n3 rows √ó 5 columnsdofdevianceAICAICcBICInt64Float64Float64Float64Float6412226801.626845.626845.727003.422626800.726852.726852.827039.133626792.326864.326864.627122.5\n\n\nIndeed, adding VCs is fitting noise. Again, the goodness of fit statistics unanimously favor the selection of the LMM m_oviCohort.\nNot shown here, but the Cohort-related VCs for the Test contrasts could be estimated reliably for the full data. Thus, the small number of cohorts does not necessarily prevent the determination of reliable differences between tests across cohorts. What if we include VCs and CPs related to random factors Child and School?\n\n\nFitting the published LMM m1 to the reduced data\n\n\n\n\n\n\nWarning\n\n\n\nThe following LMMs m1, m2, etc. take a bit longer (e.g., close to 6 minutes in the Pluto notebook, close to 3 minutes in the REPL on a MacBook Pro).\n\n\nLMM m1 reported in F√ºhner et al. (2021) included random factors for School, Child, and Cohort. The RES for School was specified like in LMM m_cpx. The RES for Child included VCs and CPs for Test, but not for linear developmental gain in the ninth year of life a1 or Sex; they are between-Child effects.\nThe RES for Cohort included only VCs, no CPs for Test. The parsimony was due to the small number of nine levels for this grouping factor.\nHere we fit this LMM m1 for the reduced data. For a different subset of similar size on MacBook Pro [13 | 15 | 16] this took [303 | 250 | 244 ] s; for LMM m1a (i.e., dropping 1 school-relate VC for Sex), times are [212 | 165 | 160] s. The corresponding lme4 times for LMM m1 are [397 | 348 | 195].\nFinally, times for fitting the full set of data ‚Äìnot in this script‚Äì, for LMM m1are [60 | 62 | 85] minutes (!); for LMM m1a the times were [46 | 48 | 34] minutes. It was not possible to fit the full set of data with lme4; after about 13 to 18 minutes the program stopped with: Error in eval_f(x, ...) : Downdated VtV is not positive definite.\n\nm1 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (1 + Test + a1 + Sex | School) +\n      (1 + Test | Child) +\n      zerocorr(1 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1706      Time: 0:00:16 ( 9.86 ms/it)\n  objective:  24859.928545253322\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0199\n0.0182\n-1.09\n0.2738\n0.5955\n0.1890\n0.0000\n\n\nTest: Star_r\n-0.0252\n0.0364\n-0.69\n0.4878\n0.7440\n0.3460\n0.0543\n\n\nTest: S20_r\n-0.0075\n0.0339\n-0.22\n0.8251\n0.6421\n0.3562\n0.0446\n\n\nTest: SLJ\n0.0095\n0.0281\n0.34\n0.7361\n0.5581\n0.3175\n0.0000\n\n\nTest: BMT\n-0.0477\n0.0285\n-1.68\n0.0939\n0.7417\n0.2282\n0.0176\n\n\na1\n0.2161\n0.0533\n4.06\n<1e-04\n\n0.1577\n\n\n\nSex: male\n0.4289\n0.0316\n13.56\n<1e-41\n\n0.1461\n\n\n\nTest: Star_r & a1\n0.2840\n0.0899\n3.16\n0.0016\n\n\n\n\n\nTest: S20_r & a1\n-0.0099\n0.0840\n-0.12\n0.9061\n\n\n\n\n\nTest: SLJ & a1\n-0.1324\n0.0785\n-1.69\n0.0916\n\n\n\n\n\nTest: BMT & a1\n0.4621\n0.0866\n5.34\n<1e-07\n\n\n\n\n\nTest: Star_r & Sex: male\n-0.3042\n0.0520\n-5.85\n<1e-08\n\n\n\n\n\nTest: S20_r & Sex: male\n0.1156\n0.0489\n2.37\n0.0180\n\n\n\n\n\nTest: SLJ & Sex: male\n-0.0003\n0.0459\n-0.01\n0.9941\n\n\n\n\n\nTest: BMT & Sex: male\n0.2561\n0.0505\n5.07\n<1e-06\n\n\n\n\n\na1 & Sex: male\n0.1026\n0.1052\n0.98\n0.3295\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.3306\n0.1779\n1.86\n0.0631\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n-0.2498\n0.1666\n-1.50\n0.1338\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0565\n0.1567\n-0.36\n0.7185\n\n\n\n\n\nTest: BMT & a1 & Sex: male\n0.2823\n0.1727\n1.63\n0.1022\n\n\n\n\n\nResidual\n0.5454\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3545682\n0.5954563\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5535949\n0.7440396\n+0.04\n\n\n\n\n\n\n\n\nTest: S20_r\n0.4122709\n0.6420832\n+0.15\n-0.53\n\n\n\n\n\n\n\nTest: SLJ\n0.3114685\n0.5580937\n-0.00\n+0.03\n-0.37\n\n\n\n\n\n\nTest: BMT\n0.5501338\n0.7417100\n-0.30\n+0.15\n-0.24\n-0.22\n\n\n\n\nSchool\n(Intercept)\n0.0357246\n0.1890096\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1197096\n0.3459907\n+0.08\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1268505\n0.3561608\n-0.07\n-0.47\n\n\n\n\n\n\n\nTest: SLJ\n0.1008311\n0.3175392\n-0.07\n-0.23\n-0.51\n\n\n\n\n\n\nTest: BMT\n0.0520816\n0.2282138\n-0.12\n+0.01\n+0.04\n-0.27\n\n\n\n\n\na1\n0.0248607\n0.1576727\n+0.13\n-0.35\n-0.35\n-0.01\n+0.33\n\n\n\n\nSex: male\n0.0213402\n0.1460829\n+0.29\n-0.00\n+0.00\n-0.26\n+0.91\n+0.41\n\n\nCohort\n(Intercept)\n0.0000000\n0.0000000\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0029478\n0.0542937\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0019871\n0.0445772\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0000000\n0.0000000\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0003083\n0.0175598\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2975145\n0.5454489\n\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nDepending on the random number for stratified samplign, LMM m1 may or may not be supported by the data.\nWe also fit an alternative parameterization, estimating VCs and CPs for Test scores rather than Test effects by replacing the 1 + ... in the RE terms with 0 + ....\n\nm2 = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test * a1 * Sex +\n      (0 + Test + a1 + Sex | School) +\n      (0 + Test | Child) +\n      zerocorr(0 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1204      Time: 0:00:11 ( 9.72 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0215\n0.0192\n-1.12\n0.2640\n\n\n\n\n\nTest: Star_r\n-0.0240\n0.0335\n-0.72\n0.4726\n0.7397\n0.3251\n0.0000\n\n\nTest: S20_r\n-0.0125\n0.0410\n-0.31\n0.7597\n0.7926\n0.3031\n0.0823\n\n\nTest: SLJ\n0.0176\n0.0393\n0.45\n0.6537\n0.7930\n0.2362\n0.0000\n\n\nTest: BMT\n-0.0480\n0.0297\n-1.62\n0.1061\n0.7252\n0.2357\n0.0304\n\n\na1\n0.2117\n0.0544\n3.89\n0.0001\n\n0.2437\n\n\n\nSex: male\n0.4297\n0.0317\n13.58\n<1e-41\n\n0.1490\n\n\n\nTest: Star_r & a1\n0.2876\n0.0894\n3.22\n0.0013\n\n\n\n\n\nTest: S20_r & a1\n-0.0208\n0.0844\n-0.25\n0.8055\n\n\n\n\n\nTest: SLJ & a1\n-0.1078\n0.0795\n-1.36\n0.1752\n\n\n\n\n\nTest: BMT & a1\n0.4598\n0.0867\n5.30\n<1e-06\n\n\n\n\n\nTest: Star_r & Sex: male\n-0.3048\n0.0520\n-5.87\n<1e-08\n\n\n\n\n\nTest: S20_r & Sex: male\n0.1164\n0.0488\n2.38\n0.0172\n\n\n\n\n\nTest: SLJ & Sex: male\n-0.0015\n0.0459\n-0.03\n0.9743\n\n\n\n\n\nTest: BMT & Sex: male\n0.2561\n0.0505\n5.07\n<1e-06\n\n\n\n\n\na1 & Sex: male\n0.1003\n0.1056\n0.95\n0.3419\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.3313\n0.1779\n1.86\n0.0626\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n-0.2482\n0.1665\n-1.49\n0.1360\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0576\n0.1564\n-0.37\n0.7128\n\n\n\n\n\nTest: BMT & a1 & Sex: male\n0.2802\n0.1727\n1.62\n0.1046\n\n\n\n\n\nTest: Run\n\n\n\n\n0.7619\n0.2789\n0.0341\n\n\nResidual\n0.5309\n\n\n\n\n\n\n\n\n\n\n\n\nissingular(m2)\n\ntrue\n\n\nDepending on the random number generator seed, the model may or may not be supported in the alternative parameterization of scores. The fixed-effects profile is not affected (see 2.8 below).\n\n\n\n\n\n\nDanger\n\n\n\nRK: The order of RE terms is critical. In formula f2 the zerocorr() term must be placed last as shown. If it is placed first, School-related and Child-related CPs are estimated/reported (?) as zero. This was not the case for formula m1. Thus, it appears to be related to the 0-intercepts in School and Child terms. Need a reprex.\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5805028\n0.7619074\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5470959\n0.7396593\n+0.48\n\n\n\n\n\n\n\n\nTest: S20_r\n0.6282565\n0.7926263\n+0.60\n+0.63\n\n\n\n\n\n\n\nTest: SLJ\n0.6289009\n0.7930327\n+0.53\n+0.59\n+0.73\n\n\n\n\n\n\nTest: BMT\n0.5259559\n0.7252282\n+0.21\n+0.42\n+0.39\n+0.50\n\n\n\n\nSchool\nTest: Run\n0.0777884\n0.2789058\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1056798\n0.3250843\n+0.36\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0918594\n0.3030832\n+0.23\n+0.37\n\n\n\n\n\n\n\nTest: SLJ\n0.0557714\n0.2361598\n+0.51\n+0.31\n+0.34\n\n\n\n\n\n\nTest: BMT\n0.0555648\n0.2357218\n+0.35\n+0.19\n+0.25\n+0.53\n\n\n\n\n\na1\n0.0593786\n0.2436772\n+0.37\n+0.10\n-0.15\n-0.14\n+0.02\n\n\n\n\nSex: male\n0.0221873\n0.1489539\n+0.17\n+0.14\n+0.15\n-0.16\n+0.73\n+0.19\n\n\nCohort\nTest: Run\n0.0011647\n0.0341275\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0000000\n0.0000000\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0067749\n0.0823098\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0000000\n0.0000000\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0009263\n0.0304359\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2818544\n0.5308996\n\n\n\n\n\n\n\n\n\n\n\nPrinciple Component Analysis of Random Effect Structure (rePCA)\nThe √¨ssingular() command is sort of a shortcut for a quick inspection of the principle components (PCs) of the variance-covariance matrix of the RES. With the MixedModels.PCA() command, we also obtain information about the amount of cumulative variance accounted for as we add PCs.\nThe output also provides PC loadings which may facilitate interpretation of the CP matrices (if estimated). This topic will be picked uo in a separate vignette. See also F√ºhner et al. (2021) for an application.\n\n\nEffects in RES\nFor every random factor, MixedModels.PCA() extracts as many PCs as there are VCs. Therefore, the cumulation of variance across PCs within a random factor will always add up to 100% ‚Äì at the latest with the last VC, but, in the case of overparameterized LMMs, the ceiling will be reached earlier. The final PCs are usually quite small.\nPCs are extracted in the order of the amount of unique variance they account for. The first PC accounts for the largest and the final PC for the least amount of variance. The number the PCs with percent variance above a certain threshold indicates the number of weighted composites needed and reflects the dimensionality of the orthogonal space within which (almost) all the variance can be accounted for. The weights for forming composite scores are the listed loadings. For ease of interpretation it is often useful to change the sign of some composite scores.\nThe PCA for LMM m1 shows that each of the five PCs for Child accounts for a non-zero percent of unique variance.\nFor School fewer than seven PCs have unique variance. The exact number depends on sampling. The overparameterization of School might be resolved when the CPs for Sex are dropped from the LMM.\nCohort was estimated with CPs forced to zero. Therefore, the VCs were forced to be orthogonal; they already represent the PCA solution. However, depending on sampling, not all PCs may be identified for this random factor either.\nImportantly, again depending on sampling, a non-singular fit does not imply that unique variance is associated with all PCs (i.e., not for last PC for School). Embrace uncertainty!\n\nMixedModels.PCA(m1)\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .\n Test: Star_r   0.04   1.0     .      .      .\n Test: S20_r    0.15  -0.53   1.0     .      .\n Test: SLJ     -0.0    0.03  -0.37   1.0     .\n Test: BMT     -0.3    0.15  -0.24  -0.22   1.0\n\nNormalized cumulative variances:\n[0.3501, 0.6193, 0.8223, 0.9377, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n (Intercept)   -0.25  -0.47   0.65  -0.54  -0.08\n Test: Star_r   0.55  -0.11   0.53   0.42   0.48\n Test: S20_r   -0.67   0.18   0.03   0.08   0.71\n Test: SLJ      0.25  -0.6   -0.54  -0.31   0.44\n Test: BMT      0.35   0.62   0.06  -0.65   0.25, School = \nPrincipal components based on correlation matrix\n (Intercept)    1.0     .      .      .      .      .      .\n Test: Star_r   0.08   1.0     .      .      .      .      .\n Test: S20_r   -0.07  -0.47   1.0     .      .      .      .\n Test: SLJ     -0.07  -0.23  -0.51   1.0     .      .      .\n Test: BMT     -0.12   0.01   0.04  -0.27   1.0     .      .\n a1             0.13  -0.35  -0.35  -0.01   0.33   1.0     .\n Sex: male      0.29  -0.0    0.0   -0.26   0.91   0.41   1.0\n\nNormalized cumulative variances:\n[0.3247, 0.5585, 0.7561, 0.9077, 0.9997, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n (Intercept)   -0.14   0.15  -0.16  -0.9    0.21  -0.15  -0.24\n Test: Star_r   0.05   0.21  -0.8    0.13  -0.09  -0.41   0.33\n Test: S20_r   -0.03  -0.75   0.16  -0.13   0.14  -0.49   0.36\n Test: SLJ      0.27   0.49   0.37   0.11   0.6   -0.34   0.23\n Test: BMT     -0.6   -0.02  -0.05   0.37   0.25  -0.37  -0.55\n a1            -0.38   0.34   0.39  -0.11  -0.62  -0.35   0.24\n Sex: male     -0.64   0.05  -0.07  -0.02   0.33   0.44   0.53, Cohort = \nPrincipal components based on correlation matrix\n (Intercept)   0.0  .    .    .    .\n Test: Star_r  0.0  1.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  0.0  .\n Test: BMT     0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3     PC4     PC5\n (Intercept)   0.0   0.0   0.0   NaN       0.0\n Test: Star_r  1.0   0.0   0.0     0.0     0.0\n Test: S20_r   0.0   1.0   0.0     0.0     0.0\n Test: SLJ     0.0   0.0   0.0     0.0   NaN\n Test: BMT     0.0   0.0   1.0     0.0     0.0)\n\n\n\nScores in RES\nNow lets looks at the PCA results for the alternative parameterization of LMM m2. It is important to note that the reparameterization to base estimates of VCs and CPs on scores rather than effects applies only to the Test factor (i.e., the first factor in the formula); VCs for Sex and age refer to the associated effects.\nDepending on sampling, the difference between LMM m1 and LMM m2 may show that overparameterization according to PCs may depend on the specification chosen for the other the random-effect structure.\n\n\n\n\n\n\nNote\n\n\n\nFor the complete data, all PCs had unique variance associated with them.\n\n\n\nMixedModels.PCA(m2)\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run     1.0    .     .     .    .\n Test: Star_r  0.48  1.0    .     .    .\n Test: S20_r   0.6   0.63  1.0    .    .\n Test: SLJ     0.53  0.59  0.73  1.0   .\n Test: BMT     0.21  0.42  0.39  0.5  1.0\n\nNormalized cumulative variances:\n[0.6141, 0.7764, 0.8692, 0.95, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.41   0.55   0.47   0.54  -0.07\n Test: Star_r  -0.46  -0.01  -0.81   0.34  -0.13\n Test: S20_r   -0.5    0.17  -0.02  -0.44   0.73\n Test: SLJ     -0.5   -0.06   0.16  -0.55  -0.65\n Test: BMT     -0.35  -0.81   0.31   0.31   0.15, School = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .      .      .\n Test: Star_r   0.36   1.0     .      .      .      .      .\n Test: S20_r    0.23   0.37   1.0     .      .      .      .\n Test: SLJ      0.51   0.31   0.34   1.0     .      .      .\n Test: BMT      0.35   0.19   0.25   0.53   1.0     .      .\n a1             0.37   0.1   -0.15  -0.14   0.02   1.0     .\n Sex: male      0.17   0.14   0.15  -0.16   0.73   0.19   1.0\n\nNormalized cumulative variances:\n[0.3649, 0.5702, 0.7432, 0.8708, 0.9515, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5    PC6    PC7\n Test: Run     -0.45   0.01  -0.47   0.2    0.18  -0.71  -0.11\n Test: Star_r  -0.37  -0.18  -0.21  -0.59  -0.66   0.05  -0.06\n Test: S20_r   -0.35  -0.32   0.21  -0.51   0.68   0.12  -0.06\n Test: SLJ     -0.43  -0.45  -0.06   0.47  -0.07   0.34   0.52\n Test: BMT     -0.5    0.24   0.37   0.32  -0.11   0.24  -0.62\n a1            -0.1    0.5   -0.64  -0.07   0.23   0.53   0.02\n Sex: male     -0.32   0.6    0.39  -0.16  -0.04  -0.19   0.57, Cohort = \nPrincipal components based on correlation matrix\n Test: Run     1.0  .    .    .    .\n Test: Star_r  0.0  0.0  .    .    .\n Test: S20_r   0.0  0.0  1.0  .    .\n Test: SLJ     0.0  0.0  0.0  0.0  .\n Test: BMT     0.0  0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2   PC3     PC4     PC5\n Test: Run     1.0   0.0   0.0     0.0     0.0\n Test: Star_r  0.0   0.0   0.0   NaN       0.0\n Test: S20_r   0.0   1.0   0.0     0.0     0.0\n Test: SLJ     0.0   0.0   0.0     0.0   NaN\n Test: BMT     0.0   0.0   1.0     0.0     0.0)\n\n\n\n\n\nSummary of results for stratified subset of data\nReturning to the theoretical focus of the article, the significant main effects of age and Sex, the interactions between age and c1 and c4 contrasts and the interactions between Sex and three test contrasts (c1, c2, c4) are replicated. Obviously, the subset of data is much noisier than the full set."
  },
  {
    "objectID": "fggk21.html#age-x-sex-nested-in-levels-of-test",
    "href": "fggk21.html#age-x-sex-nested-in-levels-of-test",
    "title": "Basics with Emotikon Project",
    "section": "Age x Sex nested in levels of Test",
    "text": "Age x Sex nested in levels of Test\nIn this final LMM, we test post-hoc five age x Sex interactions by nesting the interaction in the levels of Test. As this LMM m2_nested is a reparameterization of LMM m2.\n\nm2_nested = let\n  f = @formula(\n    zScore ~\n      1 +\n      Test +\n      Test & (a1 * Sex) +\n      (0 + Test + a1 + Sex | School) +\n      (0 + Test | Child) +\n      zerocorr(0 + Test | Cohort)\n  )\n  fit(MixedModel, f, dat; contrasts)\nend\n\nMinimizing 1261      Time: 0:00:12 (10.31 ms/it)\n  objective:  24853.932242878553\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\nœÉ_School\nœÉ_Cohort\n\n\n\n\n(Intercept)\n-0.0215\n0.0192\n-1.12\n0.2640\n\n\n\n\n\nTest: Star_r\n-0.0240\n0.0335\n-0.72\n0.4726\n0.7409\n0.3250\n0.0000\n\n\nTest: S20_r\n-0.0125\n0.0410\n-0.31\n0.7596\n0.7938\n0.3032\n0.0823\n\n\nTest: SLJ\n0.0176\n0.0393\n0.45\n0.6536\n0.7942\n0.2361\n0.0000\n\n\nTest: BMT\n-0.0480\n0.0297\n-1.62\n0.1061\n0.7265\n0.2357\n0.0305\n\n\nTest: Run & a1\n-0.0548\n0.0786\n-0.70\n0.4859\n\n\n\n\n\nTest: Star_r & a1\n0.2328\n0.0768\n3.03\n0.0024\n\n\n\n\n\nTest: S20_r & a1\n0.2120\n0.0807\n2.63\n0.0086\n\n\n\n\n\nTest: SLJ & a1\n0.1043\n0.0786\n1.33\n0.1848\n\n\n\n\n\nTest: BMT & a1\n0.5641\n0.0749\n7.53\n<1e-13\n\n\n\n\n\nTest: Run & Sex: male\n0.5531\n0.0454\n12.18\n<1e-33\n\n\n\n\n\nTest: Star_r & Sex: male\n0.2483\n0.0448\n5.54\n<1e-07\n\n\n\n\n\nTest: S20_r & Sex: male\n0.3647\n0.0466\n7.82\n<1e-14\n\n\n\n\n\nTest: SLJ & Sex: male\n0.3632\n0.0459\n7.91\n<1e-14\n\n\n\n\n\nTest: BMT & Sex: male\n0.6193\n0.0434\n14.26\n<1e-45\n\n\n\n\n\nTest: Run & a1 & Sex: male\n-0.0488\n0.1541\n-0.32\n0.7515\n\n\n\n\n\nTest: Star_r & a1 & Sex: male\n0.2825\n0.1514\n1.87\n0.0621\n\n\n\n\n\nTest: S20_r & a1 & Sex: male\n0.0343\n0.1568\n0.22\n0.8270\n\n\n\n\n\nTest: SLJ & a1 & Sex: male\n-0.0233\n0.1551\n-0.15\n0.8805\n\n\n\n\n\nTest: BMT & a1 & Sex: male\n0.2569\n0.1468\n1.75\n0.0801\n\n\n\n\n\na1\n\n\n\n\n\n0.2440\n\n\n\nSex: male\n\n\n\n\n\n0.1490\n\n\n\nTest: Run\n\n\n\n\n0.7632\n0.2789\n0.0341\n\n\nResidual\n0.5291\n\n\n\n\n\n\n\n\n\n\n\nThe results show that none of the interactions in the panels of Figure¬†2 is significant. The size and direction of interaction effects correspond with what is shown in Figure¬†2.\n\nCONSTRUCTION SITE: More model comparisons\n\n\nCode\ngof_summary3 = let\n  nms = [:m1, :m2, :m2_nested]\n  mods = eval.(nms)\n  DataFrame(;\n    name=nms,\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsnamedofdevianceAICAICcBICSymbolInt64Float64Float64Float64Float641m16924859.924997.924998.925492.72m26924853.924991.924992.925486.73m2_nested6924853.924991.924992.925486.7\n\n\n\nn, p, q, k = size(m1)  # nobs, fe params, VCs+CPs, re terms\n\n(9615, 20, 13095, 3)\n\n\nIn prinicple, the models should yield the save deviance. When models are not supported by the data, that is for singular models, there may be small differences between deviances for these reparameterizations. During optimization such models search for the absolute minimum in a very shallow surface and may end up in a local minimum instead.\n\n\nGeometric degrees of freedom\nFrom MixedModels documentation: ‚ÄúThe sum of the leverage values is the rank of the model matrix and n - sum(leverage(m)) is the degrees of freedom for residuals. The sum of the leverage values is also the trace of the so-called‚Äùhat‚Äù matrixH.‚Äù\nNew term: geometric degrees of freedom.\n\nm1_geomdf = sum(leverage(m1))  # geom_dof\n\n5401.172562881657\n\n\n\nsum(leverage(m2))\n\n5617.992522037874\n\n\n\nsum(leverage(m2_nested))\n\n5645.266087841611\n\n\n\nn - m1_geomdf\n\n4213.827437118343\n\n\n\nm1.feterm.rank\n\n20\n\n\n\ndof(m1)\n\n69"
  },
  {
    "objectID": "fggk21.html#glossary-of-mixedmodels.jl-commands",
    "href": "fggk21.html#glossary-of-mixedmodels.jl-commands",
    "title": "Basics with Emotikon Project",
    "section": "Glossary of MixedModels.jl commands",
    "text": "Glossary of MixedModels.jl commands\nHere we introduce most of the commands available in the MixedModels.jl package that allow the immediated inspection and analysis of results returned in a fitted linear mixed-effect model.\nPostprocessing related to conditional modes will be dealt with in a different tutorial.\n\nOverall summary statistics\n+ julia> m1.optsum         # MixedModels.OptSummary:  gets all info \n+ julia> loglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood\n                             of the model\n+ julia> deviance(m1)      # StatsBase.deviance: negative twice the log-likelihood\n                             relative to saturated model\n+ julia> objective(m1)     # MixedModels.objective: saturated model not clear:\n                             negative twice the log-likelihood\n+ julia> nobs(m1)          # n of observations; they are not independent\n+ julia> dof(m1)           # n of degrees of freedom is number of model parameters\n+ julia> aic(m1)           # objective(m1) + 2*dof(m1)\n+ julia> bic(m1)           # objective(m1) + dof(m1)*log(nobs(m1))\nm1.optsum            # MixedModels.OptSummary:  gets all info\n\n\n\n\n\n\n\nInitialization\n\n\n\nInitial parameter vector\n[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\n\nInitial objective value\n26037.141446130794\n\n\nOptimizer settings\n\n\n\nOptimizer (from NLopt)\nLN_BOBYQA\n\n\nLower bounds\n[0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, -Inf, 0.0, -Inf, -Inf, 0.0, -Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nftol_rel\n1.0e-12\n\n\nftol_abs\n1.0e-8\n\n\nxtol_rel\n0.0\n\n\nxtol_abs\n[1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10, 1.0e-10]\n\n\ninitial_step\n[0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.75, 1.0, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]\n\n\nmaxfeval\n-1\n\n\nmaxtime\n-1.0\n\n\nResult\n\n\n\nFunction evaluations\n1704\n\n\nFinal parameter vector\n[1.0917, 0.0612, 0.1801, -0.0021, -0.4076, 1.3627, -0.6347, 0.0332, 0.2279, 0.9749, -0.4348, -0.1724, 0.9256, -0.4273, 1.1911, 0.3465, 0.0476, -0.044, -0.0421, -0.0511, 0.0385, 0.0784, 0.6325, -0.3051, -0.1291, 0.0093, -0.1029, -0.0062, 0.5756, -0.4054, 0.0178, -0.1667, 0.0038, 0.3951, -0.1482, -0.2037, -0.0918, 0.3874, 0.0413, 0.2388, 0.0225, -0.008, 0.0059, 0.0, 0.0995, 0.0817, 0.0, 0.0322]\n\n\nFinal objective value\n24859.9285\n\n\nReturn code\nFTOL_REACHED\n\n\n\n\nloglikelihood(m1) # StatsBase.loglikelihood: return loglikelihood of the model\n\n-12429.964272626661\n\n\n\ndeviance(m1)      # StatsBase.deviance: negative twice the log-likelihood relative to saturated mode`\n\n24859.928545253322\n\n\n\nobjective(m1)    # MixedModels.objective: saturated model not clear: negative twice the log-likelihood\n\n24859.928545253322\n\n\n\nnobs(m1) # n of observations; they are not independent\n\n9615\n\n\n\nn_, p_, q_, k_ = size(m1)\n\n(9615, 20, 13095, 3)\n\n\n\ndof(m1)  # n of degrees of freedom is number of model parameters\n\n69\n\n\n\ngeom_df = sum(leverage(m1)) # trace of hat / rank of model matrix / geom dof\n\n5401.172562881657\n\n\n\nresid_df = nobs(m1) - geom_df  # eff. residual degrees of freedom\n\n4213.827437118343\n\n\n\naic(m1)  # objective(m1) + 2*dof(m1)\n\n24997.928545253322\n\n\n\nbic(m1)  # objective(m1) + dof(m1)*log(nobs(m1))\n\n25492.733041656902\n\n\n\n\nFixed-effect statistics\n+ julia> coeftable(m1)     # StatsBase.coeftable: fixed-effects statiscs; \n                             default level=0.95\n+ julia> Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n+ julia> coef(m1)          # StatsBase.coef - parts of the table\n+ julia> fixef(m1)         # MixedModels.fixef: not the same as coef() \n                             for rank-deficient case\n+ julia> m1.beta           # alternative extractor\n+ julia> fixefnames(m1)    # works also for coefnames(m1)\n+ julia> vcov(m1)          # StatsBase.vcov: var-cov matrix of fixed-effects coef.\n+ julia> stderror(m1)      # StatsBase.stderror: SE for fixed-effects coefficients\n+ julia> propertynames(m1) # names of available extractors\ncoeftable(m1) # StatsBase.coeftable: fixed-effects statiscs; default level=0.95\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(>\n\n\n\n\n(Intercept)\n-0.0198957\n0.0181797\n-1.09\n0.2738\n\n\nTest: Star_r\n-0.0252286\n0.0363626\n-0.69\n0.4878\n\n\nTest: S20_r\n-0.00750331\n0.033948\n-0.22\n0.8251\n\n\nTest: SLJ\n0.00948514\n0.028148\n0.34\n0.7361\n\n\nTest: BMT\n-0.0477326\n0.0284959\n-1.68\n0.0939\n\n\na1\n0.216067\n0.0532644\n4.06\n<1e-04\n\n\nSex: male\n0.428918\n0.0316259\n13.56\n<1e-41\n\n\nTest: Star_r & a1\n0.284022\n0.0898508\n3.16\n0.0016\n\n\nTest: S20_r & a1\n-0.00990745\n0.083981\n-0.12\n0.9061\n\n\nTest: SLJ & a1\n-0.132446\n0.0785153\n-1.69\n0.0916\n\n\nTest: BMT & a1\n0.462133\n0.0865555\n5.34\n<1e-07\n\n\nTest: Star_r & Sex: male\n-0.304156\n0.0519637\n-5.85\n<1e-08\n\n\nTest: S20_r & Sex: male\n0.115606\n0.0488647\n2.37\n0.0180\n\n\nTest: SLJ & Sex: male\n-0.000338472\n0.0459467\n-0.01\n0.9941\n\n\nTest: BMT & Sex: male\n0.256071\n0.0504849\n5.07\n<1e-06\n\n\na1 & Sex: male\n0.102575\n0.105193\n0.98\n0.3295\n\n\nTest: Star_r & a1 & Sex: male\n0.330641\n0.177878\n1.86\n0.0631\n\n\nTest: S20_r & a1 & Sex: male\n-0.249753\n0.166599\n-1.50\n0.1338\n\n\nTest: SLJ & a1 & Sex: male\n-0.0564921\n0.156745\n-0.36\n0.7185\n\n\nTest: BMT & a1 & Sex: male\n0.282304\n0.172721\n1.63\n0.1022\n\n\n\n#Arrow.write(\"./data/m_cpx_fe.arrow\", DataFrame(coeftable(m1)));\n\ncoef(m1)              # StatsBase.coef; parts of the table\n\n20-element Vector{Float64}:\n -0.019895661169173753\n -0.0252286086466606\n -0.007503307499783369\n  0.009485141256008411\n -0.04773260617184128\n  0.21606682359478394\n  0.4289181827805576\n  0.28402239792208384\n -0.009907452850893879\n -0.13244568126871425\n  0.46213334334532774\n -0.3041558420858966\n  0.11560595912596357\n -0.0003384723054319844\n  0.25607093229692646\n  0.10257483102997447\n  0.33064077723505425\n -0.24975262721592556\n -0.05649207024203066\n  0.2823038814941723\n\n\n\nfixef(m1)    # MixedModels.fixef: not the same as coef() for rank-deficient case\n\n20-element Vector{Float64}:\n -0.019895661169173753\n -0.0252286086466606\n -0.007503307499783369\n  0.009485141256008411\n -0.04773260617184128\n  0.21606682359478394\n  0.4289181827805576\n  0.28402239792208384\n -0.009907452850893879\n -0.13244568126871425\n  0.46213334334532774\n -0.3041558420858966\n  0.11560595912596357\n -0.0003384723054319844\n  0.25607093229692646\n  0.10257483102997447\n  0.33064077723505425\n -0.24975262721592556\n -0.05649207024203066\n  0.2823038814941723\n\n\n\nm1.Œ≤                  # alternative extractor\n\n20-element Vector{Float64}:\n -0.019895661169173753\n -0.0252286086466606\n -0.007503307499783369\n  0.009485141256008411\n -0.04773260617184128\n  0.21606682359478394\n  0.4289181827805576\n  0.28402239792208384\n -0.009907452850893879\n -0.13244568126871425\n  0.46213334334532774\n -0.3041558420858966\n  0.11560595912596357\n -0.0003384723054319844\n  0.25607093229692646\n  0.10257483102997447\n  0.33064077723505425\n -0.24975262721592556\n -0.05649207024203066\n  0.2823038814941723\n\n\n\nfixefnames(m1)        # works also for coefnames(m1)\n\n20-element Vector{String}:\n \"(Intercept)\"\n \"Test: Star_r\"\n \"Test: S20_r\"\n \"Test: SLJ\"\n \"Test: BMT\"\n \"a1\"\n \"Sex: male\"\n \"Test: Star_r & a1\"\n \"Test: S20_r & a1\"\n \"Test: SLJ & a1\"\n \"Test: BMT & a1\"\n \"Test: Star_r & Sex: male\"\n \"Test: S20_r & Sex: male\"\n \"Test: SLJ & Sex: male\"\n \"Test: BMT & Sex: male\"\n \"a1 & Sex: male\"\n \"Test: Star_r & a1 & Sex: male\"\n \"Test: S20_r & a1 & Sex: male\"\n \"Test: SLJ & a1 & Sex: male\"\n \"Test: BMT & a1 & Sex: male\"\n\n\n\nvcov(m1)   # StatsBase.vcov: var-cov matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  0.000330501   2.43591e-5    1.96056e-5   ‚Ä¶   3.93329e-6    2.31201e-5\n  2.43591e-5    0.00132224   -0.000477655      3.76311e-6   -1.77831e-5\n  1.96056e-5   -0.000477655   0.00115246       0.000111285   2.09003e-5\n -1.32449e-5   -6.17289e-5   -0.000405898     -0.00022474    9.06503e-5\n -8.81187e-5    4.90941e-5   -5.39657e-5       9.03322e-5   -0.000248286\n -0.00015691   -6.47616e-5   -7.85807e-5   ‚Ä¶  -3.68946e-5    0.000138017\n  3.32926e-5    9.72333e-7    1.73469e-6       1.382e-5      0.000209752\n -1.48292e-5   -0.000460011   0.000222094     -1.34859e-5    1.73724e-6\n -2.82151e-5    0.000222059  -0.000415524      3.90503e-5   -2.51012e-6\n  2.9336e-6     1.03153e-6    0.00018521      -2.30757e-5   -1.25704e-5\n  5.5421e-5    -3.20883e-5    4.23244e-5   ‚Ä¶  -1.17946e-5    0.000107244\n  5.78972e-7    2.12578e-5   -1.02265e-5      -2.92897e-6   -0.000128039\n  9.42827e-7   -1.00092e-5    1.41961e-5       0.000718311   0.000168555\n -3.80592e-6    1.25308e-6   -3.91593e-6      -0.00144658    0.000587444\n  1.00848e-5    2.22377e-6   -2.9881e-6        0.000587241  -0.00171884\n -9.22279e-5   -4.20131e-6   -7.74037e-6   ‚Ä¶  -8.99195e-5   -0.00334634\n -6.14052e-6   -0.00027966    0.000137583      0.000100617   0.0021312\n -9.46166e-6    0.000136945  -0.000256438     -0.0118558    -0.00282681\n  3.93329e-6    3.76311e-6    0.000111285      0.0245691    -0.0102727\n  2.31201e-5   -1.77831e-5    2.09003e-5      -0.0102727     0.0298327\n\n\n\nvcov(m1; corr=true) # StatsBase.vcov: correlation matrix of fixed-effects coefficients\n\n20√ó20 Matrix{Float64}:\n  1.0           0.0368485     0.0317672   ‚Ä¶   0.0013803     0.00736303\n  0.0368485     1.0          -0.386941        0.000660232  -0.00283144\n  0.0317672    -0.386941      1.0             0.0209136     0.00356445\n -0.0258831    -0.0603095    -0.424772       -0.0509377     0.0186456\n -0.170098      0.0473797    -0.0557856       0.020224     -0.0504456\n -0.162042     -0.0334369    -0.0434576   ‚Ä¶  -0.00441908    0.015002\n  0.0579053     0.000845505   0.00161571      0.00278785    0.0383987\n -0.00907838   -0.140796      0.0728117      -0.000957554   0.000111942\n -0.0184805     0.0727163    -0.145748        0.00296653   -0.000173049\n  0.00205523    0.000361302   0.0694859      -0.00187502   -0.000926931\n  0.0352203    -0.0101952     0.014404    ‚Ä¶  -0.000869348   0.00717354\n  0.000612874   0.0112503    -0.00579714     -0.0003596    -0.0142658\n  0.00106133   -0.00563312    0.00855775      0.0937828     0.019971\n -0.00455637    0.000750012  -0.00251054     -0.20086       0.074023\n  0.010988      0.00121136   -0.00174349      0.0742095    -0.197118\n -0.048227     -0.00109836   -0.00216752  ‚Ä¶  -0.00545349   -0.184178\n -0.00189888   -0.0432368     0.022784        0.00360873    0.0693674\n -0.00312399    0.0226058    -0.0453415      -0.454011     -0.0982379\n  0.0013803     0.000660232   0.0209136       1.0          -0.379441\n  0.00736303   -0.00283144    0.00356445     -0.379441      1.0\n\n\n\nstderror(m1)       # StatsBase.stderror: SE for fixed-effects coefficients\n\n20-element Vector{Float64}:\n 0.018179695754188763\n 0.036362647652246415\n 0.033947966931311996\n 0.028147974876918565\n 0.0284958511780161\n 0.05326436046683719\n 0.03162592280498092\n 0.0898507907685589\n 0.08398095381138845\n 0.07851534867595167\n 0.08655549082455408\n 0.051963682688124575\n 0.048864694440272756\n 0.04594665491480662\n 0.05048494535642958\n 0.10519260031429802\n 0.1778777471389226\n 0.1665987201208234\n 0.15674526525976237\n 0.17272135026035296\n\n\n\npropertynames(m1)  # names of available extractors\n\n(:formula, :reterms, :Xymat, :feterm, :sqrtwts, :parmap, :dims, :A, :L, :optsum, :Œ∏, :theta, :Œ≤, :beta, :Œ≤s, :betas, :Œª, :lambda, :stderror, :œÉ, :sigma, :œÉs, :sigmas, :œÉœÅs, :sigmarhos, :b, :u, :lowerbd, :X, :y, :corr, :vcov, :PCA, :rePCA, :objective, :pvalues)\n\n\n\n\nCovariance parameter estimates\nThese commands inform us about the model parameters associated with the RES.\n+ julia> issingular(m1)        # Test singularity for param. vector m1.theta\n+ julia> VarCorr(m1)           # MixedModels.VarCorr: est. of RES\n+ julia> propertynames(m1)\n+ julia> m1.œÉ                  # residual; or: m1.sigma\n+ julia> m1.œÉs                 # VCs; m1.sigmas\n+ julia> m1.Œ∏                  # Parameter vector for RES (w/o residual); m1.theta\n+ julia> MixedModels.sdest(m1) #  prsqrt(MixedModels.varest(m1))\n+ julia> BlockDescription(m1)  #  Description of blocks of A and L in an LMM\n\nissingular(m1) # Test if model is singular for paramter vector m1.theta (default)\n\ntrue\n\n\n\nissingular(m2)\n\ntrue\n\n\nVarCorr(m1) # MixedModels.VarCorr: estimates of random-effect structure (RES)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3545682\n0.5954563\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5535949\n0.7440396\n+0.04\n\n\n\n\n\n\n\n\nTest: S20_r\n0.4122709\n0.6420832\n+0.15\n-0.53\n\n\n\n\n\n\n\nTest: SLJ\n0.3114685\n0.5580937\n-0.00\n+0.03\n-0.37\n\n\n\n\n\n\nTest: BMT\n0.5501338\n0.7417100\n-0.30\n+0.15\n-0.24\n-0.22\n\n\n\n\nSchool\n(Intercept)\n0.0357246\n0.1890096\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1197096\n0.3459907\n+0.08\n\n\n\n\n\n\n\n\nTest: S20_r\n0.1268505\n0.3561608\n-0.07\n-0.47\n\n\n\n\n\n\n\nTest: SLJ\n0.1008311\n0.3175392\n-0.07\n-0.23\n-0.51\n\n\n\n\n\n\nTest: BMT\n0.0520816\n0.2282138\n-0.12\n+0.01\n+0.04\n-0.27\n\n\n\n\n\na1\n0.0248607\n0.1576727\n+0.13\n-0.35\n-0.35\n-0.01\n+0.33\n\n\n\n\nSex: male\n0.0213402\n0.1460829\n+0.29\n-0.00\n+0.00\n-0.26\n+0.91\n+0.41\n\n\nCohort\n(Intercept)\n0.0000000\n0.0000000\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0029478\n0.0542937\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0019871\n0.0445772\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0000000\n0.0000000\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0003083\n0.0175598\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2975145\n0.5454489\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\n\nChild\nTest: Run\n0.5805028\n0.7619074\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.5470959\n0.7396593\n+0.48\n\n\n\n\n\n\n\n\nTest: S20_r\n0.6282565\n0.7926263\n+0.60\n+0.63\n\n\n\n\n\n\n\nTest: SLJ\n0.6289009\n0.7930327\n+0.53\n+0.59\n+0.73\n\n\n\n\n\n\nTest: BMT\n0.5259559\n0.7252282\n+0.21\n+0.42\n+0.39\n+0.50\n\n\n\n\nSchool\nTest: Run\n0.0777884\n0.2789058\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.1056798\n0.3250843\n+0.36\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0918594\n0.3030832\n+0.23\n+0.37\n\n\n\n\n\n\n\nTest: SLJ\n0.0557714\n0.2361598\n+0.51\n+0.31\n+0.34\n\n\n\n\n\n\nTest: BMT\n0.0555648\n0.2357218\n+0.35\n+0.19\n+0.25\n+0.53\n\n\n\n\n\na1\n0.0593786\n0.2436772\n+0.37\n+0.10\n-0.15\n-0.14\n+0.02\n\n\n\n\nSex: male\n0.0221873\n0.1489539\n+0.17\n+0.14\n+0.15\n-0.16\n+0.73\n+0.19\n\n\nCohort\nTest: Run\n0.0011647\n0.0341275\n\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0000000\n0.0000000\n.\n\n\n\n\n\n\n\n\nTest: S20_r\n0.0067749\n0.0823098\n.\n.\n\n\n\n\n\n\n\nTest: SLJ\n0.0000000\n0.0000000\n.\n.\n.\n\n\n\n\n\n\nTest: BMT\n0.0009263\n0.0304359\n.\n.\n.\n.\n\n\n\n\nResidual\n\n0.2818544\n0.5308996\n\n\n\n\n\n\n\n\n\n\nm1.œÉs      # VCs; m1.sigmas\n\n(Child = (var\"(Intercept)\" = 0.5954563012956545, var\"Test: Star_r\" = 0.7440395656928731, var\"Test: S20_r\" = 0.642083242243342, var\"Test: SLJ\" = 0.5580936633835759, var\"Test: BMT\" = 0.7417100325651534), School = (var\"(Intercept)\" = 0.18900961537081512, var\"Test: Star_r\" = 0.3459907312274253, var\"Test: S20_r\" = 0.3561607937297112, var\"Test: SLJ\" = 0.31753918142583637, var\"Test: BMT\" = 0.22821383079614746, a1 = 0.1576726836362368, var\"Sex: male\" = 0.1460829099567715), Cohort = (var\"(Intercept)\" = 0.0, var\"Test: Star_r\" = 0.05429365509364568, var\"Test: S20_r\" = 0.04457721914401475, var\"Test: SLJ\" = 0.0, var\"Test: BMT\" = 0.017559801357771326))\n\n\n\nm1.Œ∏       # Parameter vector for RES (w/o residual); m1.theta\n\n48-element Vector{Float64}:\n  1.091681149690514\n  0.06122032237794931\n  0.18010538600424852\n -0.002115798921991684\n -0.407639295612378\n  1.3627121326733238\n -0.6346633507602674\n  0.033161340725922556\n  0.2279159755264362\n  0.9749263283676822\n -0.43480940048954275\n -0.17240321202738373\n  0.9256017495380907\n  ‚ãÆ\n -0.09175954127636438\n  0.38741095386246566\n  0.041317113041006485\n  0.23876664699545527\n  0.02245130461806735\n -0.008008990788460643\n  0.005893532357977424\n  0.0\n  0.09953939472059105\n  0.08172574501144028\n  0.0\n  0.03219330132686052\n\n\nBlockDescription(m1) #  Description of blocks of A and L in a LinearMixedModel\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9935\nBlkDiag\n\n\n\n\n\n3115\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense\n\n\n\n\nm2.Œ∏\n\n48-element Vector{Float64}:\n  1.4351251540146468\n  0.6689585063336703\n  0.8945881052776556\n  0.7976264781871568\n  0.29092328451397836\n  1.2221102042751668\n  0.5741878852107406\n  0.5615985877923488\n  0.4901041933613002\n  1.0483469868670556\n  0.5664550767111587\n  0.24119200862511742\n  0.9791960565175124\n  ‚ãÆ\n -0.09381253971965217\n  0.3727101256397806\n  0.031693097804373876\n  0.2574912228403121\n  0.3808427084858804\n -0.0068038419400346314\n  0.0\n  0.06428248565092799\n  0.0\n  0.15503827514152782\n  0.0\n  0.057328830741360075\n\n\nBlockDescription(m2)\n\n\n\nrows\nChild\nSchool\nCohort\nfixed\n\n\n\n\n9935\nBlkDiag\n\n\n\n\n\n3115\nSparse\nBlkDiag\n\n\n\n\n45\nDense\nDense\nBlkDiag/Dense\n\n\n\n21\nDense\nDense\nDense\nDense\n\n\n\n\n\nModel ‚Äúpredictions‚Äù\nThese commands inform us about extracion of conditional modes/means and (co-)variances, that using the model parameters to improve the predictions for units (levels) of the grouping (random) factors. We need this information, e.g., for partial-effect response profiles (e.g., facet plot) or effect profiles (e.g., caterpillar plot), or visualizing the borrowing-strength effect for correlation parameters (e.g., shrinkage plots). We are using the fit of LMM m2.\njulia> condVar(m2)\nSome plotting functions are currently available from the MixedModelsMakie package or via custom functions.\n+ julia> caterpillar!(m2)\n+ julia> shrinkage!(m2)\n\nConditional covariances\n\ncondVar(m1)\n\n3-element Vector{Array{Float64, 3}}:\n [0.07542670442620816 0.007778060896008802 ‚Ä¶ -0.0008706712326870283 -0.0148895913274024; 0.007778060896008802 0.30117578717575644 ‚Ä¶ -0.0010542665368989857 0.023203095240326578; ‚Ä¶ ; -0.0008706712326870283 -0.0010542665368989857 ‚Ä¶ 0.2095320148216277 -0.08228817264960842; -0.0148895913274024 0.023203095240326578 ‚Ä¶ -0.08228817264960842 0.272432823067156;;; 0.0721174039203284 0.006797076840335152 ‚Ä¶ -0.00040608979210250863 -0.013930254491114484; 0.006797076840335152 0.2987487764780418 ‚Ä¶ -0.0004258551961350167 0.02330774158457693; ‚Ä¶ ; -0.00040608979210250863 -0.0004258551961350167 ‚Ä¶ 0.20776156615572544 -0.08249534861043777; -0.013930254491114484 0.02330774158457693 ‚Ä¶ -0.08249534861043777 0.27122556017183214;;; 0.07672417474133605 0.007782199151053684 ‚Ä¶ -0.0009903300931224596 -0.014470902655515195; 0.007782199151053684 0.30253944721607445 ‚Ä¶ -0.0012740066922604602 0.02328506911334749; ‚Ä¶ ; -0.0009903300931224596 -0.0012740066922604602 ‚Ä¶ 0.20960044231588618 -0.0821445064665663; -0.014470902655515195 0.02328506911334749 ‚Ä¶ -0.0821445064665663 0.2725298797859062;;; ‚Ä¶ ;;; 0.07290752691140319 -0.00020548247912515009 ‚Ä¶ -0.0007106239486054456 -0.020032735165400727; -0.00020548247912515009 0.30181764044174186 ‚Ä¶ -0.0013544667993968932 0.024575531433736315; ‚Ä¶ ; -0.0007106239486054456 -0.0013544667993968932 ‚Ä¶ 0.20960182180943945 -0.08225466998918285; -0.020032735165400727 0.024575531433736315 ‚Ä¶ -0.08225466998918285 0.2748404903181522;;; 0.08240289822252186 0.0019449952698139283 ‚Ä¶ -0.002236364993212201 -0.012991402043288578; 0.0019449952698139283 0.3004945131454864 ‚Ä¶ -0.0010981954517667641 0.024648284330633582; ‚Ä¶ ; -0.002236364993212201 -0.0010981954517667641 ‚Ä¶ 0.20883268496069335 -0.08191603413043402; -0.012991402043288578 0.024648284330633582 ‚Ä¶ -0.08191603413043402 0.2711274191641896;;; 0.08240289822252186 0.0019449952698139283 ‚Ä¶ -0.002236364993212201 -0.012991402043288578; 0.0019449952698139283 0.3004945131454864 ‚Ä¶ -0.0010981954517667641 0.024648284330633582; ‚Ä¶ ; -0.002236364993212201 -0.0010981954517667641 ‚Ä¶ 0.20883268496069335 -0.08191603413043402; -0.012991402043288578 0.024648284330633582 ‚Ä¶ -0.08191603413043402 0.2711274191641896]\n [0.03201332228613893 0.004397440532923171 ‚Ä¶ 0.002177489999747446 0.006066923551017444; 0.004397440532923171 0.10703161922532489 ‚Ä¶ -0.017480095144768195 0.0003349202723669057; ‚Ä¶ ; 0.002177489999747446 -0.017480095144768195 ‚Ä¶ 0.02240812678040294 0.008050133654325005; 0.006066923551017444 0.0003349202723669057 ‚Ä¶ 0.008050133654325005 0.019246407169991035;;; 0.02570577601307552 0.0015649248917573796 ‚Ä¶ 0.0017000657043176027 0.0042519591047390495; 0.0015649248917573796 0.08483584842804327 ‚Ä¶ -0.01646786127585138 0.0002528482543600472; ‚Ä¶ ; 0.0017000657043176027 -0.01646786127585138 ‚Ä¶ 0.01962560504996929 0.007179689503420948; 0.0042519591047390495 0.0002528482543600472 ‚Ä¶ 0.007179689503420948 0.016301740984804185;;; 0.019101324742681627 0.0014488256928758974 ‚Ä¶ 0.00018203957041563077 0.003179131143334205; 0.0014488256928758974 0.06281298293390207 ‚Ä¶ -0.013214744786112016 0.0008931220821422633; ‚Ä¶ ; 0.00018203957041563077 -0.013214744786112016 ‚Ä¶ 0.014752128628845288 0.005065223994771072; 0.003179131143334205 0.0008931220821422633 ‚Ä¶ 0.005065223994771072 0.012432045065121957;;; ‚Ä¶ ;;; 0.03356147275376861 0.006442859822173949 ‚Ä¶ 0.0031736533908885392 0.007843405387714168; 0.006442859822173949 0.11537391081463803 ‚Ä¶ -0.016263518223212356 0.0001405000070967628; ‚Ä¶ ; 0.0031736533908885392 -0.016263518223212356 ‚Ä¶ 0.022994739819057364 0.008909015216855979; 0.007843405387714168 0.0001405000070967628 ‚Ä¶ 0.008909015216855979 0.020504136181176198;;; 0.028506269044162803 0.004687332362245225 ‚Ä¶ 0.0016503960632583746 0.006184181342629733; 0.004687332362245225 0.09400942705021262 ‚Ä¶ -0.01479230157426482 0.0002646454164114237; ‚Ä¶ ; 0.0016503960632583746 -0.01479230157426482 ‚Ä¶ 0.0199013375778105 0.007916391641568146; 0.006184181342629733 0.0002646454164114237 ‚Ä¶ 0.007916391641568146 0.01820454506381111;;; 0.03148352024438303 0.0028068250514362717 ‚Ä¶ 0.004004709554842062 0.008403258564544713; 0.0028068250514362717 0.09690407251236807 ‚Ä¶ -0.01656876972955971 0.000249842496594262; ‚Ä¶ ; 0.004004709554842062 -0.01656876972955971 ‚Ä¶ 0.021698557532286847 0.008767061938059711; 0.008403258564544713 0.000249842496594262 ‚Ä¶ 0.008767061938059711 0.019843222981041513]\n [0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0019830268765478167 ‚Ä¶ 0.0 -6.185122504223227e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.185122504223227e-6 ‚Ä¶ 0.0 0.00029256150423556036;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.001878276953541943 ‚Ä¶ 0.0 -6.172292436984793e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.172292436984793e-6 ‚Ä¶ 0.0 0.0002898606808662169;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.001798923132456498 ‚Ä¶ 0.0 -6.099508526007712e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.099508526007712e-6 ‚Ä¶ 0.0 0.000287906548935756;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0019286511521350704 ‚Ä¶ 0.0 -6.452253208503372e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.452253208503372e-6 ‚Ä¶ 0.0 0.00029142898918976557;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.00185019938632921 ‚Ä¶ 0.0 -6.072114731339023e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.072114731339023e-6 ‚Ä¶ 0.0 0.0002892097573836892;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0017700723809946587 ‚Ä¶ 0.0 -6.053407573416679e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.053407573416679e-6 ‚Ä¶ 0.0 0.000286287620121245;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0017791570051395884 ‚Ä¶ 0.0 -6.2755420561367225e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -6.2755420561367225e-6 ‚Ä¶ 0.0 0.00028648952929911226;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0016464785382481787 ‚Ä¶ 0.0 -5.469313386159436e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -5.469313386159436e-6 ‚Ä¶ 0.0 0.0002828144160516554;;; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0017066657260981736 ‚Ä¶ 0.0 -5.6534209717495444e-6; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 -5.6534209717495444e-6 ‚Ä¶ 0.0 0.0002840054056602571]\n\n\n\ncondVar(m2)\n\n3-element Vector{Array{Float64, 3}}:\n [0.19523335411609347 0.04319330246086449 ‚Ä¶ 0.049885075958329084 0.017778601952765823; 0.04319330246086449 0.19515564354125176 ‚Ä¶ 0.05543489552165503 0.04403888336783887; ‚Ä¶ ; 0.049885075958329084 0.05543489552165503 ‚Ä¶ 0.19084227715810548 0.05934276454532541; 0.017778601952765823 0.04403888336783887 ‚Ä¶ 0.05934276454532541 0.20369951908972664;;; 0.19136937077461416 0.03947053014342581 ‚Ä¶ 0.04562829330293411 0.014667359024812447; 0.03947053014342581 0.18889904544991962 ‚Ä¶ 0.05045964475106784 0.040331696740157116; ‚Ä¶ ; 0.04562829330293411 0.05045964475106784 ‚Ä¶ 0.18444924034419075 0.05455379195694386; 0.014667359024812447 0.040331696740157116 ‚Ä¶ 0.05455379195694386 0.19916105536801487;;; 0.1962907560774691 0.043610865170077215 ‚Ä¶ 0.04971194395540189 0.01812022659013002; 0.043610865170077215 0.19647597191341037 ‚Ä¶ 0.05508358082356382 0.0442704675542718; ‚Ä¶ ; 0.04971194395540189 0.05508358082356382 ‚Ä¶ 0.1898452898726911 0.059360191805678565; 0.01812022659013002 0.0442704675542718 ‚Ä¶ 0.059360191805678565 0.204757330250997;;; ‚Ä¶ ;;; 0.21074629817491167 0.051576124767729124 ‚Ä¶ 0.05699519419042287 0.016969174061697324; 0.051576124767729124 0.19732752672527198 ‚Ä¶ 0.054420630882119625 0.03621428952157521; ‚Ä¶ ; 0.05699519419042287 0.054420630882119625 ‚Ä¶ 0.1873006386358677 0.049000845082648556; 0.016969174061697324 0.03621428952157521 ‚Ä¶ 0.049000845082648556 0.18932839226525974;;; 0.2112281371468095 0.05441098477823823 ‚Ä¶ 0.057925756114080205 0.02629467160468497; 0.05441098477823823 0.20102296569537695 ‚Ä¶ 0.05738560563934567 0.04774272215772214; ‚Ä¶ ; 0.057925756114080205 0.05738560563934567 ‚Ä¶ 0.1875536434201301 0.05916529900927038; 0.02629467160468497 0.04774272215772214 ‚Ä¶ 0.05916529900927038 0.20541992843116355;;; 0.2112281371468095 0.05441098477823823 ‚Ä¶ 0.057925756114080205 0.02629467160468497; 0.05441098477823823 0.20102296569537695 ‚Ä¶ 0.05738560563934567 0.04774272215772214; ‚Ä¶ ; 0.057925756114080205 0.05738560563934567 ‚Ä¶ 0.1875536434201301 0.05916529900927038; 0.02629467160468497 0.04774272215772214 ‚Ä¶ 0.05916529900927038 0.20541992843116355]\n [0.06893750234665875 0.027624843701506176 ‚Ä¶ 0.020302289888270923 0.004614210842135138; 0.027624843701506176 0.0924009611401355 ‚Ä¶ 0.004243563057891995 0.0047653922360549345; ‚Ä¶ ; 0.020302289888270923 0.004243563057891995 ‚Ä¶ 0.056483541270777524 0.0052401981016719145; 0.004614210842135138 0.0047653922360549345 ‚Ä¶ 0.0052401981016719145 0.020068840989176467;;; 0.05873590764679962 0.021643219662326466 ‚Ä¶ 0.018958212615170604 0.00313027417462382; 0.021643219662326466 0.06855718444346845 ‚Ä¶ 0.004003091394056976 0.0032122779476219743; ‚Ä¶ ; 0.018958212615170604 0.004003091394056976 ‚Ä¶ 0.055263213441089074 0.0053752292069264955; 0.00313027417462382 0.0032122779476219743 ‚Ä¶ 0.0053752292069264955 0.016872324738517797;;; 0.0429295356212511 0.014265509867178736 ‚Ä¶ 0.009801252841880525 0.002287815751325804; 0.014265509867178736 0.047891860202778705 ‚Ä¶ -0.0011787352523036148 0.002852400181463205; ‚Ä¶ ; 0.009801252841880525 -0.0011787352523036148 ‚Ä¶ 0.04569393574217092 0.0016969508797054183; 0.002287815751325804 0.002852400181463205 ‚Ä¶ 0.0016969508797054183 0.01315728667976761;;; ‚Ä¶ ;;; 0.06979285126417656 0.029687894735718732 ‚Ä¶ 0.020607724261614047 0.006889068902012754; 0.029687894735718732 0.10396154107226319 ‚Ä¶ 0.006225014382702326 0.0068216840794921435; ‚Ä¶ ; 0.020607724261614047 0.006225014382702326 ‚Ä¶ 0.056672296855706274 0.006414956396936069; 0.006889068902012754 0.0068216840794921435 ‚Ä¶ 0.006414956396936069 0.021339280400213247;;; 0.05847510220399124 0.023687591593518693 ‚Ä¶ 0.01558298710295896 0.005262656941467362; 0.023687591593518693 0.08210142187817218 ‚Ä¶ 0.0027228733146932387 0.005302277412493336; ‚Ä¶ ; 0.01558298710295896 0.0027228733146932387 ‚Ä¶ 0.05290003692979896 0.005749951317875997; 0.005262656941467362 0.005302277412493336 ‚Ä¶ 0.005749951317875997 0.018910525143995712;;; 0.06747767618598617 0.028261130003842193 ‚Ä¶ 0.022946631292108315 0.007505671475164516; 0.028261130003842193 0.085066281230219 ‚Ä¶ 0.007956254407873264 0.007592771129540978; ‚Ä¶ ; 0.022946631292108315 0.007956254407873264 ‚Ä¶ 0.056225736503113986 0.006405731142508409; 0.007505671475164516 0.007592771129540978 ‚Ä¶ 0.006405731142508409 0.020621393474069404]\n [0.0009399607795008945 0.0 ‚Ä¶ 0.0 -1.196174650276707e-6; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -1.196174650276707e-6 0.0 ‚Ä¶ 0.0 0.0007724957051659319;;; 0.0009075033293564738 0.0 ‚Ä¶ 0.0 -6.027979776855023e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -6.027979776855023e-7 0.0 ‚Ä¶ 0.0 0.0007495249423962104;;; 0.0008799350333624272 0.0 ‚Ä¶ 0.0 -5.390467017285931e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -5.390467017285931e-7 0.0 ‚Ä¶ 0.0 0.0007329735018575524;;; 0.0009220319038320892 0.0 ‚Ä¶ 0.0 -5.049802359259136e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -5.049802359259136e-7 0.0 ‚Ä¶ 0.0 0.0007629610132067539;;; 0.0008991392294401429 0.0 ‚Ä¶ 0.0 -6.080649630933205e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -6.080649630933205e-7 0.0 ‚Ä¶ 0.0 0.0007434694428274968;;; 0.000872558144176728 0.0 ‚Ä¶ 0.0 -3.8466256884483373e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -3.8466256884483373e-7 0.0 ‚Ä¶ 0.0 0.0007194056628668594;;; 0.000874691811882813 0.0 ‚Ä¶ 0.0 3.282221351959827e-8; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 3.282221351959827e-8 0.0 ‚Ä¶ 0.0 0.0007211595117856373;;; 0.0008295917162955104 0.0 ‚Ä¶ 0.0 5.378294602604871e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; 5.378294602604871e-7 0.0 ‚Ä¶ 0.0 0.0006921839036702976;;; 0.0008496787069048915 0.0 ‚Ä¶ 0.0 -4.6325843293391996e-7; 0.0 0.0 ‚Ä¶ 0.0 0.0; ‚Ä¶ ; 0.0 0.0 ‚Ä¶ 0.0 0.0; -4.6325843293391996e-7 0.0 ‚Ä¶ 0.0 0.0007017694680640312]\n\n\nThey are hard to look at. Let‚Äôs take pictures.\n\n\nCaterpillar plots\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 400)), ranefinfo(m1, :Cohort)\n)\n\n\n\n\n\nFigure 3: Prediction intervals of the random effects for Cohort in model m1\n\n\n\n\n\n\nShrinkage plots\nThese are just teasers. We will pick this up in a separate tutorial. Enjoy!\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m1, :Cohort)\n\n\n\n\n\nFigure 4: Shrinkage plot of the random effects for Cohort in model m1\n\n\n\n\n\n\nCode\nshrinkageplot!(Figure(; resolution=(800, 800)), m2, :Cohort)\n\n\n\n\n\nFigure 5: Shrinkage plot of the random effects for Cohort in model m2"
  }
]