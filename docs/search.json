[
  {
    "objectID": "Arrow.html",
    "href": "Arrow.html",
    "title": "Notes on saved data files",
    "section": "",
    "text": "The Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means “something that looks like a data frame in R”. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather."
  },
  {
    "objectID": "Arrow.html#the-emotikon-data",
    "href": "Arrow.html#the-emotikon-data",
    "title": "Notes on saved data files",
    "section": "The Emotikon data",
    "text": "The Emotikon data\nThe SMLP2021 repository contains a version of the data from Fühner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing PyCall  # show reading Arrow in Python\nusing RCall   # show reading Arrow in R\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density"
  },
  {
    "objectID": "Arrow.html#downloading-and-importing-the-rds-file",
    "href": "Arrow.html#downloading-and-importing-the-rds-file",
    "title": "Notes on saved data files",
    "section": "Downloading and importing the RDS file",
    "text": "Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\ndfrm = rcopy(R\"readRDS($fn)\")\n\n\n525,126 rows × 7 columnsCohortSchoolChildSexageTestscoreCat…Cat…Cat…Cat…Float64Cat…Float6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0⋮⋮⋮⋮⋮⋮⋮⋮\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3077850\n\n\n\ndf = DataFrame(tbl)\n\n\n525,126 rows × 7 columnsCohortSchoolChildSexageTestscoreStringStringStringStringFloat64StringFloat6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0⋮⋮⋮⋮⋮⋮⋮⋮"
  },
  {
    "objectID": "Arrow.html#avoiding-needless-repetition",
    "href": "Arrow.html#avoiding-needless-repetition",
    "title": "Notes on saved data files",
    "section": "Avoiding needless repetition",
    "text": "Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design."
  },
  {
    "objectID": "Arrow.html#creating-the-smaller-table",
    "href": "Arrow.html#creating-the-smaller-table",
    "title": "Notes on saved data files",
    "section": "Creating the smaller table",
    "text": "Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n\n108,295 rows × 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452⋮⋮⋮⋮⋮⋮\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1774946\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794058\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n\n525,126 rows × 3 columnsChildTestscoreStringStringFloat641C002352S20_r5.263162C002352BPT3.73C002352SLJ125.04C002352Star_r2.471465C002352Run1053.06C002353S20_r5.07C002353BPT4.18C002353SLJ116.09C002353Star_r1.7677810C002353Run1089.011C002354S20_r4.5454512C002354BPT3.913C002354SLJ111.014C002354Star_r1.9887515C002354Run864.016C002355S20_r4.5454517C002355BPT3.018C002355SLJ114.019C002355Star_r1.8446420C002355Run835.021C002356S20_r4.3478322C002356BPT3.323C002356SLJ118.024C002356Star_r1.9068225C002356Run860.026C002357S20_r4.3478327C002357BPT4.328C002357SLJ130.029C002357Star_r1.9965530C002357Run960.0⋮⋮⋮⋮\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius’s presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @c zscore(:score))\n\n\n525,126 rows × 4 columnsChildTestscorezScoreStringStringFloat64Float641C002352S20_r5.263161.79132C002352BPT3.7-0.06223173C002352SLJ125.0-0.03365674C002352Star_r2.471461.468745C002352Run1053.00.3310586C002353S20_r5.01.154717C002353BPT4.10.4983548C002353SLJ116.0-0.4988229C002353Star_r1.76778-0.977310C002353Run1089.00.57405611C002354S20_r4.545450.055148112C002354BPT3.90.21806113C002354SLJ111.0-0.75724814C002354Star_r1.98875-0.20918615C002354Run864.0-0.94468116C002355S20_r4.545450.055148117C002355BPT3.0-1.0432618C002355SLJ114.0-0.60219319C002355Star_r1.84464-0.7101320C002355Run835.0-1.1404321C002356S20_r4.34783-0.42292122C002356BPT3.3-0.62281723C002356SLJ118.0-0.39545224C002356Star_r1.90682-0.49399225C002356Run860.0-0.9716826C002357S20_r4.34783-0.42292127C002357BPT4.30.77864628C002357SLJ130.00.22476929C002357Star_r1.99655-0.18207630C002357Run960.0-0.296686⋮⋮⋮⋮⋮\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n\n108,295 rows × 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452⋮⋮⋮⋮⋮⋮\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n\n525,126 rows × 8 columnsChildTestscorezScoreSchoolCohortSexageStringStringFloat64Float64StringStringStringFloat641C002352S20_r5.263161.7913S1000672013male7.994522C002352BPT3.7-0.0622317S1000672013male7.994523C002352SLJ125.0-0.0336567S1000672013male7.994524C002352Star_r2.471461.46874S1000672013male7.994525C002352Run1053.00.331058S1000672013male7.994526C002353S20_r5.01.15471S1000672013male7.994527C002353BPT4.10.498354S1000672013male7.994528C002353SLJ116.0-0.498822S1000672013male7.994529C002353Star_r1.76778-0.9773S1000672013male7.9945210C002353Run1089.00.574056S1000672013male7.9945211C002354S20_r4.545450.0551481S1000672013male7.9945212C002354BPT3.90.218061S1000672013male7.9945213C002354SLJ111.0-0.757248S1000672013male7.9945214C002354Star_r1.98875-0.209186S1000672013male7.9945215C002354Run864.0-0.944681S1000672013male7.9945216C002355S20_r4.545450.0551481S1001222013female7.9945217C002355BPT3.0-1.04326S1001222013female7.9945218C002355SLJ114.0-0.602193S1001222013female7.9945219C002355Star_r1.84464-0.71013S1001222013female7.9945220C002355Run835.0-1.14043S1001222013female7.9945221C002356S20_r4.34783-0.422921S1001462013male7.9945222C002356BPT3.3-0.622817S1001462013male7.9945223C002356SLJ118.0-0.395452S1001462013male7.9945224C002356Star_r1.90682-0.493992S1001462013male7.9945225C002356Run860.0-0.97168S1001462013male7.9945226C002357S20_r4.34783-0.422921S1001462013male7.9945227C002357BPT4.30.778646S1001462013male7.9945228C002357SLJ130.00.224769S1001462013male7.9945229C002357Star_r1.99655-0.182076S1001462013male7.9945230C002357Run960.0-0.296686S1001462013male7.99452⋮⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created."
  },
  {
    "objectID": "Arrow.html#discovering-patterns-in-the-data",
    "href": "Arrow.html#discovering-patterns-in-the-data",
    "title": "Notes on saved data files",
    "section": "Discovering patterns in the data",
    "text": "Discovering patterns in the data\nOne of the motivations for creating the Child table was be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow => :ntest)\n\n\n108,295 rows × 2 columnsChildntestStringInt641C00235252C00235353C00235454C00235555C00235656C00235757C00235858C00235949C002360510C002361411C002362512C002363513C002364514C002365515C002366516C002367517C002368518C002369519C002370520C002371421C002372522C002373523C002374524C002375525C002376526C002377527C002378528C002379529C002380530C0023815⋮⋮⋮\n\n\nNow create a table of the number of children with 1, 2, …, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n\n5 rows × 2 columnsntestnrowInt64Int6411462227293317394488365596529\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002452S1011752013male7.9945212C002625S1033292013male7.9945213C002754S1048142013female7.9945214C003269S1022582012female7.9972615C003599S1058432012female7.9972616C003807S1007542011male8.017C003985S1029452011male8.018C004086S1042552011male8.019C004657S1014002014male8.03833110C005036S1059092014male8.03833111C005440S1010232019male8.05202112C005523S1018252019female8.05202113C005697S1036152019male8.05202114C005759S1046322019female8.05202115C005810S1049542019female8.05202116C005835S1050532019male8.05202117C005854S1054052019male8.05202118C006550S1033292013male8.0794119C006760S1051812013female8.0794120C007031S1132442013male8.0794121C007050S1001952012female8.08214122C007305S1023502012male8.08214123C007828S1114052012female8.08214124C008698S1049172016female8.09309125C008707S1022712016male8.09582126C009596S1034212014female8.1232127C009651S1037062014female8.1232128C009879S1059092014female8.1232129C010203S1026602016male8.12594130C010204S1026602016male8.125941⋮⋮⋮⋮⋮⋮⋮⋮Last Group (96529 rows): ntest = 5ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002360S1001952013female7.9945259C002362S1002372013female7.99452510C002363S1002372013female7.99452511C002364S1002502013female7.99452512C002365S1003042013male7.99452513C002366S1003042013male7.99452514C002367S1003162013female7.99452515C002368S1003652013male7.99452516C002369S1003652013male7.99452517C002370S1003652013female7.99452518C002372S1004322013male7.99452519C002373S1004812013male7.99452520C002374S1004812013male7.99452521C002375S1004812013female7.99452522C002376S1004932013female7.99452523C002377S1004932013female7.99452524C002378S1005472013male7.99452525C002379S1005472013male7.99452526C002380S1005472013male7.99452527C002381S1005472013female7.99452528C002382S1005472013female7.99452529C002383S1005842013female7.99452530C002384S1005962013male7.994525⋮⋮⋮⋮⋮⋮⋮\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow => :nchild)\n\n\n2 rows × 2 columnsSexnchildStringInt641male2302female232\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow => :nchild)\n\n\n2 rows × 2 columnsSexnchildStringInt641male475522female48977\n\n\nWhat about the distribution of ages?\n\"\"\"\n    ridgeplot!(ax::Axis, df::AbstractDataFrame, densvar::Symbol, group::Symbol; normalize=false)\n    ridgeplot!(f::Figure, args...; pos=(1,1) kwargs...)\n    ridgeplot(args...; kwargs...)\nCreate a \"ridge plot\".\nA ridge plot is stacked plot of densities for a given variable (`densvar`) grouped by a different variable (`group`). Because densities can very widely in scale, it is sometimes useful to `normalize` the densities so that each density has a maximum of 1.\nThe non-mutating method creates a Figure before calling the method for Figure.\nThe method for Figure places the ridge plot in the grid position specified by `pos`, default is (1,1).\n\"\"\"\nfunction ridgeplot!(\n  ax::Axis,\n  df::AbstractDataFrame,\n  densvar::Symbol,\n  group::Symbol;\n  normalize=false,\n)\n  # `normalize` makes it so that the max density is always 1\n  # `normalize` works on the density not the area/mass\n  gdf = groupby(df, group)\n  dens = combine(gdf, densvar => kde => :kde)\n  sort!(dens, group)\n  spacing = normalize ? 1.0 : 0.9 * maximum(dens[!, :kde]) do val\n    return maximum(val.density)\n  end\n\n  nticks = length(gdf)\n\n  for (idx, row) in enumerate(eachrow(dens))\n    dd = if normalize\n      row.kde.density ./ maximum(row.kde.density)\n    else\n      row.kde.density\n    end\n\n    offset = idx * spacing\n\n    lower = Node(Point2f.(row.kde.x, offset))\n    upper = Node(Point2f.(row.kde.x, dd .+ offset))\n    band!(ax, lower, upper; color=(:black, 0.3))\n    lines!(ax, upper; color=(:black, 1.0))\n  end\n\n  ax.yticks[] = (\n    1:spacing:(nticks * spacing), string.(dens[!, group])\n  )\n  ylims!(ax, 0, (nticks + 2) * spacing)\n  ax.xlabel[] = string(densvar)\n  ax.ylabel[] = string(group)\n\n  return ax\nend\nfunction ridgeplot!(f::Figure, args...; pos=(1, 1), kwargs...)\n  ridgeplot!(Axis(f[pos...]), args...; kwargs...)\n  return f\nend\n\"\"\"\n    ridgeplot(args...; kwargs...)\nSee [ridgeplot!](@ref).\n\"\"\"\nfunction ridgeplot(args...; kwargs...)\n  return ridgeplot!(Figure(), args...; kwargs...)\nend\nridgeplot(parent(gdf), :age, :ntest)\n\nparent(gdf)\n\n\n108,295 rows × 6 columnsChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002359S1001832013female7.9945249C002360S1001952013female7.99452510C002361S1002132013male7.99452411C002362S1002372013female7.99452512C002363S1002372013female7.99452513C002364S1002502013female7.99452514C002365S1003042013male7.99452515C002366S1003042013male7.99452516C002367S1003162013female7.99452517C002368S1003652013male7.99452518C002369S1003652013male7.99452519C002370S1003652013female7.99452520C002371S1004322013female7.99452421C002372S1004322013male7.99452522C002373S1004812013male7.99452523C002374S1004812013male7.99452524C002375S1004812013female7.99452525C002376S1004932013female7.99452526C002377S1004932013female7.99452527C002378S1005472013male7.99452528C002379S1005472013male7.99452529C002380S1005472013male7.99452530C002381S1005472013female7.994525⋮⋮⋮⋮⋮⋮⋮"
  },
  {
    "objectID": "Arrow.html#reading-arrow-files-in-other-languages",
    "href": "Arrow.html#reading-arrow-files-in-other-languages",
    "title": "Notes on saved data files",
    "section": "Reading Arrow files in other languages",
    "text": "Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow). They can be accessed within Julia using the RCall and PyCall packages.\nfeather = pyimport(\"pyarrow.feather\");\n\nfeather.read_table(\"./data/fggk21.arrow\")\n\nPyObject pyarrow.Table\nCohort: dictionary<values=string, indices=int8, ordered=0> not null\nSchool: dictionary<values=string, indices=int16, ordered=0> not null\nChild: dictionary<values=string, indices=int32, ordered=0> not null\nSex: dictionary<values=string, indices=int8, ordered=0> not null\nage: double not null\nTest: dictionary<values=string, indices=int8, ordered=0> not null\nscore: double not null\n----\nCohort: [  -- dictionary:\n[\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\"]  -- indices:\n[2,2,2,2,2,...,7,7,7,7,7]]\nSchool: [  -- dictionary:\n[\"S100572\",\"S100833\",\"S100912\",\"S100948\",\"S100997\",...,\"S400713\",\"S400865\",\"S700010\",\"S401031\",\"S401055\"]  -- indices:\n[419,419,419,419,419,...,275,275,275,275,275]]\nChild: [  -- dictionary:\n[\"C002352\",\"C002353\",\"C002354\",\"C002355\",\"C002356\",...,\"C117959\",\"C117962\",\"C117964\",\"C117965\",\"C117966\"]  -- indices:\n[0,0,0,0,0,...,108294,108294,108294,108294,108294]]\nSex: [  -- dictionary:\n[\"female\",\"male\"]  -- indices:\n[1,1,1,1,1,...,1,1,1,1,1]]\nage: [[7.994524298425736,7.994524298425736,7.994524298425736,7.994524298425736,7.994524298425736,...,9.10609171800137,9.10609171800137,9.10609171800137,9.10609171800137,9.10609171800137]]\nTest: [  -- dictionary:\n[\"Run\",\"Star_r\",\"S20_r\",\"SLJ\",\"BPT\"]  -- indices:\n[2,4,3,1,0,...,2,4,3,1,0]]\nscore: [[5.2631578947368425,3.7,125,2.4714563106796112,1053,...,4.545454545454545,3.8,100,2.1850643776824032,990]]\n\n\n\nR\"\"\"\nfggk21 <- arrow::read_feather(\"./data/fggk21.arrow\")\nnrow(fggk21)\n\"\"\"\n\nRObject{IntSxp}\n[1] 525126\n\n\n\nR\"tibble::glimpse(fggk21)\";\n\nRows: 525,126\nColumns: 7\n\n\n$ Cohort <fct> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ School <fct> S100067, S100067, S100067, S100067, S100067, S100067, S100067, …\n$ Child  <fct> C002352, C002352, C002352, C002352, C002352, C002353, C002353, …\n$ Sex    <fct> male, male, male, male, male, male, male, male, male, male, mal…\n$ age    <dbl> 7.994524, 7.994524, 7.994524, 7.994524, 7.994524, 7.994524, 7.9…\n$ Test   <fct> S20_r, BPT, SLJ, Star_r, Run, S20_r, BPT, SLJ, Star_r, Run, S20…\n$ score  <dbl> 5.263158, 3.700000, 125.000000, 2.471456, 1053.000000, 5.000000…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMLP2022 - Advanced frequentist methods",
    "section": "",
    "text": "Useful packages\nSleepstudy\nSleepstudy2\nArrow\nModel-building\nGLMM\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used"
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "Matrix notation for the sleepstudy model",
    "text": "Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n\n180 rows × 3 columnssubjdaysreactionStringInt8Float641S3080249.562S3081258.7053S3082250.8014S3083321.445S3084356.8526S3085414.697S3086382.2048S3087290.1499S3088430.58510S3089466.35311S3090222.73412S3091205.26613S3092202.97814S3093204.70715S3094207.71616S3095215.96217S3096213.6318S3097217.72719S3098224.29620S3099237.31421S3100199.05422S3101194.33223S3102234.3224S3103232.84225S3104229.30726S3105220.45827S3106235.42128S3107255.75129S3108261.01230S3109247.515⋮⋮⋮⋮\n\n\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51068 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n──────────────────────────────────────────────────\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, β, has 2 elements and the fixed-effects model matrix, X, is of size 180 × 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ⋮\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.β\n\n2-element Vector{Float64}:\n 251.40510484848417\n  10.467285959595715\n\n\n\nm1.X\n\n180×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ⋮    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1’s.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The “estimates” (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158188821572576 -40.04844169634357 … 0.7232620708173191 12.118907835512482; 9.075511758123813 -8.644079444480187 … -0.9710526399373302 1.310698060178722]\n\n\n\nfirst(m1.b)   # only one grouping factor\n\n2×18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   …  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180×36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.β + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725613\n 215.00307618237164\n   ⋮\n 328.09823347656857\n 337.5944667962269\n 263.52401268399666\n 275.3019967037711\n 287.07998072354553\n 298.85796474331994\n 310.6359487630944\n 322.41393278286887\n 334.1919168026433\n 345.9699008224177\n 357.74788484219215\n 369.5258688619666\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725616\n 215.00307618237167\n   ⋮\n 328.09823347656857\n 337.594466796227\n 263.52401268399666\n 275.30199670377107\n 287.0799807235455\n 298.85796474331994\n 310.6359487630944\n 322.4139327828688\n 334.1919168026432\n 345.9699008224177\n 357.74788484219215\n 369.52586886196656\n\n\nIn symbols we would write the linear predictor expression as\n\\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\]\nwhere \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 × 2 and \\(\\bf Z\\) is of size 180 × 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 × 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 × 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.θ\n\n3-element Vector{Float64}:\n 0.9292213219958606\n 0.018168376087695465\n 0.22264487473562505\n\n\n\nλ = only(m1.λ)  # with multiple grouping factors there will be multiple λ's\n\n2×2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\n\nΣ = varest(m1) * (λ * λ')\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "Linear predictors in LMMs and GLMMs",
    "text": "Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as\n\\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\]\nmay seem like over-mathematization (or “overkill”, if you prefer) relative to expressions like\n\\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\]\nbut this more abstract form is necessary for generalizations.\nThe way that I read the first form is :::{.callout} The conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\nSo the only things that differ in the distributions of the \\(y_i\\)’s are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "Generalized Linear Mixed Models",
    "text": "Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e. yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i’th response is again determined by the i’th element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of “success” for the i’th response and must be between 0 and 1. We can’t guarantee that the i’th component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren’t as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function,\n\\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\]\n(it’s called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic\n\\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\]\nThis is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(η) = inv(increment(exp(-η)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don’t have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet “Beauty is truth, truth beauty - that is all ye know on earth and all ye need to know.”)\n\nAn example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n\n1,934 rows × 5 columnsdisturbanlivchageuseStringStringStringFloat64String1D01Y3+18.44N2D01Y0-5.56N3D01Y21.44N4D01Y3+8.44N5D01Y0-13.56N6D01Y0-11.56N7D01Y3+18.44N8D01Y3+-3.56N9D01Y1-5.56N10D01Y3+1.44N11D01Y0-11.56Y12D01Y0-2.56N13D01Y1-4.56N14D01Y3+5.44N15D01Y3+-0.56N16D01Y3+4.44Y17D01Y0-5.56N18D01Y3+-0.56Y19D01Y1-6.56Y20D01Y2-3.56N21D01Y0-4.56N22D01Y0-9.56N23D01Y3+2.44N24D01Y22.44Y25D01Y1-4.56Y26D01Y3+14.44N27D01Y0-6.56Y28D01Y1-3.56Y29D01Y1-5.56Y30D01Y1-1.56Y⋮⋮⋮⋮⋮⋮\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n\n60 rows × 2 columnsdistnrowStringInt641D011172D02203D0324D04305D05396D06657D07188D08379D092310D101311D112112D122913D132414D1411815D152216D162017D172418D184719D192620D201521D211822D222023D231524D241425D256726D261327D274428D284929D293230D3061⋮⋮⋮\n\n\nThe information recorded included woman’s age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, drawn using lattice graphics in R, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20’s to early 30’s) and low again for older women (late 30’s to early 40’s in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn’t mean that there is no “age effect”, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist => Grouping(),\n  :urban => HelmertCoding(),\n  :children => HelmertCoding(),\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 186   Time: 0:00:00 ( 1.21 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n<1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7021\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n<1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n<1e-08\n\n\n\nlivch: 1\n0.8151\n0.1622\n5.02\n<1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n<1e-06\n\n\n\nlivch: 3+\n0.9153\n0.1858\n4.93\n<1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\nVarCorr(gm1)\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229100\n0.478644\n\n\n\nNotice that there is no “residual” variance being estimated. This is because the Bernoulli distribution doesn’t have a scale parameter.\n\n\nConvert livch to a binary factor\n\n@transform!(contra, :children = :livch ≠ \"0\")\n\n\n1,934 rows × 6 columnsdisturbanlivchageusechildrenStringStringStringFloat64StringBool1D01Y3+18.44N12D01Y0-5.56N03D01Y21.44N14D01Y3+8.44N15D01Y0-13.56N06D01Y0-11.56N07D01Y3+18.44N18D01Y3+-3.56N19D01Y1-5.56N110D01Y3+1.44N111D01Y0-11.56Y012D01Y0-2.56N013D01Y1-4.56N114D01Y3+5.44N115D01Y3+-0.56N116D01Y3+4.44Y117D01Y0-5.56N018D01Y3+-0.56Y119D01Y1-6.56Y120D01Y2-3.56N121D01Y0-4.56N022D01Y0-9.56N023D01Y3+2.44N124D01Y22.44Y125D01Y1-4.56Y126D01Y3+14.44N127D01Y0-6.56Y028D01Y1-3.56Y129D01Y1-5.56Y130D01Y1-1.56Y1⋮⋮⋮⋮⋮⋮⋮\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 138   Time: 0:00:00 ( 0.88 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows × 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm272364.922379.182418.152379.242gm182372.462388.732433.272388.81\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\nUsing urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 143   Time: 0:00:00 ( 0.89 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows × 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm372353.822368.482407.462368.542gm272364.922379.182418.152379.243gm182372.462388.732433.272388.81\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\ngm2\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age² (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\nPredictions for some subgroups\nFor a “typical” district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e. centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children => [true, false], :urban => [\"Y\", \"N\"], :age => [0.0]\n)\npreds = effects(design, gm3)\n\n\n4 rows × 7 columnschildrenageurbanuse: YerrlowerupperBoolFloat64StringFloat64Float64Float64Float64110.0Y0.6585760.150530.5080460.809106200.0Y-0.5543250.230477-0.784802-0.323848310.0N-0.1286160.113017-0.241633-0.0155985400.0N-1.341520.221575-1.56309-1.11994\n\n\nConverting these η values to probabilities yields\n\nlogistic.(preds[!, \"use: Y\"])\n\n4-element Vector{Float64}:\n 0.6589404445793394\n 0.3648614994485519\n 0.46789030652047003\n 0.2072606855141492"
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "Summarizing the results",
    "text": "Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule."
  },
  {
    "objectID": "sleepstudy.html#loading-the-data",
    "href": "sleepstudy.html#loading-the-data",
    "title": "Analysis of the sleepstudy data",
    "section": "0.1 Loading the data",
    "text": "0.1 Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch and Krumbiegel 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrameMacros  # simplified dplyr-like data wrangling \nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"svg\")\n\nThe sleepstudy data are one of the datasets available with the MixedModels package.\n\nsleepstudy = MixedModels.dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nThe following plot displays the data in a multi-panel plot created with the lattice package in R (Sarkar 2008), using the RCall package for Julia.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nxyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n)\n\"\"\"\n\n\n\n\n\nAverage response time versus days of sleep deprivation by subject\n\n\n\n\nRObject{VecSxp}\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject’s data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic."
  },
  {
    "objectID": "sleepstudy.html#fitting-an-initial-model",
    "href": "sleepstudy.html#fitting-an-initial-model",
    "title": "Analysis of the sleepstudy data",
    "section": "0.2 Fitting an initial model",
    "text": "0.2 Fitting an initial model\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\nMinimizing 58    Time: 0:00:00 ( 5.82 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not “parameters” in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1)\n\n\n\n\n\nFigure 1: Prediction intervals on random effects for model m1\n\n\n\n\nFigure 1 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope."
  },
  {
    "objectID": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "href": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "title": "Analysis of the sleepstudy data",
    "section": "0.3 A model with uncorrelated random effects",
    "text": "0.3 A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\nm2 = let\n  form = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\nAgain, the default display doesn’t show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\nVarCorr(m2)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nreaction ~ 1 + days + MixedModels.ZeroCorr((1 + days | subj))\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  DataFrame(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ∘ leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows × 6 columnsmodelparsgeomdofAICBICAICcSymbolInt64Float64Float64Float64Float641m2529.0451762.01777.971762.352m1628.61151763.941783.11764.42\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a “smaller is better” scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of “degrees of freedom”, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We’re not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor."
  },
  {
    "objectID": "sleepstudy.html#some-diagnostic-plots",
    "href": "sleepstudy.html#some-diagnostic-plots",
    "title": "Analysis of the sleepstudy data",
    "section": "0.4 Some diagnostic plots",
    "text": "0.4 Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure 2, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\nFigure 2: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)"
  },
  {
    "objectID": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "href": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "title": "Analysis of the sleepstudy data",
    "section": "0.5 Assessing variability by bootstrapping",
    "text": "0.5 Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1; hide_progress=true)\nallpars = DataFrame(m1bstp.allpars)\n\n\n30,000 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411βmissing(Intercept)260.71221βmissingdays9.8497531σsubj(Intercept)15.331741σsubjdays6.4029651ρsubj(Intercept), days-0.025958261σresidualmissing23.409272βmissing(Intercept)262.25382βmissingdays12.300892σsubj(Intercept)16.3183102σsubjdays5.54688112ρsubj(Intercept), days0.552607122σresidualmissing25.7047133βmissing(Intercept)253.149143βmissingdays12.879153σsubj(Intercept)25.4787163σsubjdays6.1444173ρsubj(Intercept), days0.0691545183σresidualmissing22.2753194βmissing(Intercept)263.376204βmissingdays11.5798214σsubj(Intercept)18.8039224σsubjdays4.6557234ρsubj(Intercept), days0.103361244σresidualmissing23.3128255βmissing(Intercept)248.429265βmissingdays9.39444275σsubj(Intercept)20.1411285σsubjdays5.27358295ρsubj(Intercept), days-0.163603305σresidualmissing25.4355⋮⋮⋮⋮⋮⋮\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure 3, shows the normal distribution, “bell-curve”, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"),\n    @subset(allpars, :type == \"β\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    @subset(allpars, :type == \"β\" && :names == \"days\").value,\n  )\n  f1\nend\n\n\n\n\n\nFigure 3: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-α shortestcovint to be the shortest interval that contains a proportion 1-α (defaults to 95%) of the bootstrap estimates of the parameter.\n\nDataFrame(shortestcovint(m1bstp))\n\n\n6 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)239.64265.2282βmissingdays7.4234713.16073σsubj(Intercept)10.172233.08764σsubjdays2.994767.661215ρsubj(Intercept), days-0.4013531.06σresidualmissing22.70128.5016\n\n\nThe intervals look reasonable except that the upper bound on the interval for ρ, the correlation coefficient, is 1.0 . It turns out that the estimates of ρ have a great deal of variability.\nEven more alarming, some of these ρ values are undefined (denoted NaN) because the way ρ is calculated can create a division by zero.\n\ndescribe(@select(@subset(allpars, :type == \"ρ\"), :value))\n\n\n1 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolFloat64Float64NothingFloat64Int64DataType1valueNaNNaNNaN0Float64\n\n\nBecause there are several values on the boundary (ρ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure 4.\n\n\nCode\nhist(\n  @subset(allpars, :type == \"ρ\", isfinite(:value)).value;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\nFigure 4: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure 5, show reasonable symmetry.\n\n\nCode\nbegin\n  σs = @subset(allpars, :type == \"σ\")\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual σ\"),\n    @subset(σs, :group == \"residual\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept σ\"),\n    @subset(σs, :group == \"subj\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope σ\"),\n    @subset(σs, :group == \"subj\" && :names == \"days\").value,\n  )\n  f2\nend\n\n\n\n\n\nFigure 5: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, β₁ and β₂, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure 6 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2×2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  vals = disallowmissing(\n    Array(\n      select(\n        unstack(DataFrame(m1bstp.β), :iter, :coefname, :β),\n        Not(:iter),\n      ),\n    ),\n  )\n  scatter(\n    vals;\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde(vals))\n  current_figure()\nend\n\n\n\n\n\nFigure 6: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours"
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al. 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ⁄ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges and Powell 1985).\nDesign\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1–E7, R1–R3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\nTest schedule within days\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al. 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\nStatistical analyses\nThe authors analyzed response speed, that is (1/RT)*1000 – completely warranted according to a Box-Cox check of the current data – with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\nCurrent data\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al. 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#setup",
    "href": "sleepstudy_speed.html#setup",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.2 Setup",
    "text": "0.2 Setup\nFirst we attach the various packages needed, define a few helper functions, read the data, and get everything in the desired shape.\n\nCode\nusing CairoMakie         # device driver for static (SVG, PDF, PNG) plots\nusing Chain              # like pipes but cleaner\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie   # plots specific to mixed-effects models using Makie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "sleepstudy_speed.html#preprocessing",
    "href": "sleepstudy_speed.html#preprocessing",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.3 Preprocessing",
    "text": "0.3 Preprocessing\nThe sleepstudy data are one of the datasets available with recent versions of the MixedModels package. We carry out some preprocessing to have the dataframe in the desired shape:\n\nCapitalize random factor Subj\nCompute speed as an alternative dependent variable from reaction, warranted by a ‘boxcox’ check of residuals.\nCreate a GroupedDataFrame by levels of Subj (the original dataframe is available as gdf.parent, which we name df)\n\n\ngdf = @chain MixedModels.dataset(:sleepstudy) begin\n  DataFrame\n  rename!(:subj => :Subj, :days => :day)\n  @transform!(:speed = 1000 / :reaction)\n  groupby(:Subj)\nend\n\n\nGroupedDataFrame with 18 groups based on key: SubjFirst Group (10 rows): Subj = \"S308\"SubjdayreactionspeedStringInt8Float64Float641S3080249.564.007052S3081258.7053.865413S3082250.8013.987234S3083321.443.1115S3084356.8522.802286S3085414.692.411447S3086382.2042.616418S3087290.1493.446519S3088430.5852.3224210S3089466.3532.1443⋮Last Group (10 rows): Subj = \"S372\"SubjdayreactionspeedStringInt8Float64Float641S3720269.4123.711792S3721273.4743.656653S3722297.5973.360254S3723310.6323.219255S3724287.1733.482236S3725329.6083.033917S3726334.4822.98978S3727343.222.913589S3728369.1422.7089910S3729364.1242.74632\n\n\n\ndf = gdf.parent\ndescribe(df)\n\n\n4 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS308S3720String2day4.504.590Int83reaction298.508194.332288.651466.3530Float644speed3.466342.14433.464435.145830Float64"
  },
  {
    "objectID": "sleepstudy_speed.html#estimates-for-pooled-data",
    "href": "sleepstudy_speed.html#estimates-for-pooled-data",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.4 Estimates for pooled data",
    "text": "0.4 Estimates for pooled data\nIn the first analysis we ignore the dependency of observations due to repeated measures from the same subjects. We pool all the data and estimate the regression of 180 speed scores on the nine days of the experiment.\n\npooledcoef = simplelinreg(df.day, df.speed)  # produces a Tuple\n\n(3.96581197478315, -0.11099359232199725)"
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-effects",
    "href": "sleepstudy_speed.html#within-subject-effects",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.5 Within-subject effects",
    "text": "0.5 Within-subject effects\nIn the second analysis we estimate coefficients for each Subj without regard of the information available from the complete set of data. We do not “borrow strength” to adjust for differences due to between-Subj variability and due to being far from the population mean.\n\n0.5.1 Within-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] => simplelinreg => :coef)\n\n\n18 rows × 2 columnsSubjcoefStringTuple…1S308(3.94806, -0.194812)2S309(4.87022, -0.0475185)3S310(4.90606, -0.120054)4S330(3.4449, -0.0291309)5S331(3.47647, -0.0498047)6S332(3.84436, -0.105511)7S333(3.60159, -0.0917378)8S334(4.04528, -0.133527)9S335(3.80451, 0.0455771)10S337(3.34374, -0.137744)11S349(4.46855, -0.170885)12S350(4.21414, -0.20151)13S351(3.80469, -0.0728582)14S352(3.68634, -0.144957)15S369(3.85384, -0.120531)16S370(4.52679, -0.215965)17S371(3.853, -0.0936243)18S372(3.69208, -0.113292)\n\n\nFigure 1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n┌ Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n│   caller = top-level scope at In[7]:8\n└ @ Core ./In[7]:8\n\n\n\n\n\nFigure 1: Reaction speed (s⁻¹) versus days of sleep deprivation by subject"
  },
  {
    "objectID": "sleepstudy_speed.html#basic-lmm",
    "href": "sleepstudy_speed.html#basic-lmm",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.6 Basic LMM",
    "text": "0.6 Basic LMM\n\ncontrasts = Dict(:Subj => Grouping())\nm1 = let\n  form = @formula speed ~ 1 + day + (1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\nMinimizing 70    Time: 0:00:00 ( 4.78 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1056\n37.55\n<1e-99\n0.4190\n\n\nday\n-0.1110\n0.0151\n-7.37\n<1e-12\n0.0566\n\n\nResidual\n0.2698\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept which estimates the average speed on the baseline day of the experiment prior to sleep deprivation, and the slowing per day of sleep deprivation. In this case about -0.11/second.\nThe random effects represent shifts from the typical behavior for each subject.The shift in the intercept has a standard deviation of about 0.42/s.\nThe within-subject correlation of the random effects for intercept and slope is small, -0.18, indicating that a simpler model with a correlation parameter (CP) forced to/ assumed to be zero may be sufficient."
  },
  {
    "objectID": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "href": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.7 No correlation parameter: zcp LMM",
    "text": "0.7 No correlation parameter: zcp LMM\nThe zerocorr function applied to a random-effects term estimates one parameter less than LMM m1– the CP is now fixed to zero.\nm2 = let\n  form = @formula speed ~ 1 + day + zerocorr(1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1033\n38.38\n<1e-99\n0.4085\n\n\nday\n-0.1110\n0.0147\n-7.53\n<1e-13\n0.0550\n\n\nResidual\n0.2706\n\n\n\n\n\n\n\nLMM m2 has a slghtly lower log-likelihood than LMM m1 but also one fewer parameters. A likelihood-ratio test is used to compare these nested models.\n\nCode\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nspeed ~ 1 + day + MixedModels.ZeroCorr((1 + day | Subj))\n5\n125\n\n\n\n\n\nspeed ~ 1 + day + (1 + day | Subj)\n6\n125\n0\n1\n0.5192\n\n\n\nAlternatively, the AIC, AICc, and BIC values can be compared. They are on a scale where “smaller is better”. All three model-fit statistics prefer the zcpLMM m2.\n\n\nCode\nlet\n  mods = [m2, m1]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n\n2 rows × 5 columnsdofdevianceAICAICcBICInt64Float64Float64Float64Float6415125.379135.379135.724151.34426124.964136.964137.45156.122"
  },
  {
    "objectID": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "href": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.8 Conditional modes of the random effects",
    "text": "0.8 Conditional modes of the random effects\nThe third set of estimates are their conditional modes. They represent a compromise between their own data and the model parameters. When distributional assumptions hold, predictions based on these estimates are more accurate than either the pooled or the within-subject estimates. Here we “borrow strength” to improve the accuracy of prediction."
  },
  {
    "objectID": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "href": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.9 Caterpillar plots (effect profiles)",
    "text": "0.9 Caterpillar plots (effect profiles)\n\n\nCode\ncaterpillar(m2)\n\n\n\n\n\nFigure 2: Prediction intervals on the random effects in model m2"
  },
  {
    "objectID": "sleepstudy_speed.html#shrinkage-plot",
    "href": "sleepstudy_speed.html#shrinkage-plot",
    "title": "The sleepstudy: Speed - for a change …",
    "section": "0.10 Shrinkage plot",
    "text": "0.10 Shrinkage plot\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m2)\n\n\n\n\n\nFigure 3: Shrinkage plot of the means of the random effects in model m2"
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\nProvide a short alias for AlgebraOfGraphics."
  },
  {
    "objectID": "kb07.html#data-set-and-model",
    "href": "kb07.html#data-set-and-model",
    "title": "Bootstrapping a fitted model",
    "section": "Data set and model",
    "text": "Data set and model\nThe kb07 data (Kronmüller and Barr 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n\n7 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1subjS030S1030String2itemI01I320String3spkrnewold0String4precbreakmaintain0String5loadnoyes0String6rt_trunc2182.25791940.051710Int167rt_raw2226.245791940.0159230Int16\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\ncontrasts = merge(\n  Dict(nm => EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm => Grouping() for nm in (:subj, :item)),\n);\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 723   Time: 0:00:00 ( 0.29 ms/it)\n  objective:  28637.1393507629\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.6750\n77.3042\n28.22\n<1e-99\n301.8457\n362.1769\n\n\nspkr: old\n67.7490\n18.2846\n3.71\n0.0002\n43.0723\n40.5401\n\n\nprec: maintain\n-333.9206\n47.1549\n-7.08\n<1e-11\n62.1055\n246.8926\n\n\nload: yes\n78.7681\n19.5218\n4.03\n<1e-04\n65.1378\n42.1405\n\n\nspkr: old & prec: maintain\n-21.9634\n15.8063\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3838\n15.8063\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5334\n15.8063\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6051\n15.8063\n1.49\n0.1353\n\n\n\n\nResidual\n668.5074\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91110.8185\n301.8457\n\n\n\n\n\n\nspkr: old\n1855.2224\n43.0723\n+0.78\n\n\n\n\n\nprec: maintain\n3857.0961\n62.1055\n-0.59\n+0.02\n\n\n\n\nload: yes\n4242.9381\n65.1378\n+0.36\n+0.82\n+0.53\n\n\nitem\n(Intercept)\n131172.1058\n362.1769\n\n\n\n\n\n\nspkr: old\n1643.4957\n40.5401\n+0.44\n\n\n\n\n\nprec: maintain\n60955.9375\n246.8926\n-0.69\n+0.35\n\n\n\n\nload: yes\n1775.8252\n42.1405\n+0.32\n+0.23\n-0.15\n\n\nResidual\n\n446902.1445\n668.5074\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_item\nσ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.240\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n27\n20\n0.1436\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, kbm02, to the more complex, kbm01."
  },
  {
    "objectID": "kb07.html#a-bootstrap-sample",
    "href": "kb07.html#a-bootstrap-sample",
    "title": "Bootstrapping a fitted model",
    "section": "A bootstrap sample",
    "text": "A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\nRandom.seed!(1234321)\nhide_progress = true\nkbm02samp = parametricbootstrap(2000, kbm02; hide_progress);\nOne of the uses of such a sample is to form “confidence intervals” on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nDataFrame(shortestcovint(kbm02samp))\n\n\n9 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)2028.012337.922βmissingspkr: old38.43199.59443βmissingprec: maintain-439.321-245.8644βmissingload: yes46.0262107.5115σitem(Intercept)261.196448.516σitemprec: maintain175.489312.0387ρitem(Intercept), prec: maintain-0.89799-0.4455958σsubj(Intercept)228.099357.7899σresidualmissing655.249701.497\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure 1.\n\n\nCode\ndraw(\n  data(kbm02samp.β) * mapping(:β; color=:coefname) * AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\ndraw(\n  data(\n    filter(\n      :column => ==(Symbol(\"(Intercept)\")), DataFrame(kbm02samp.σs)\n    ),\n  ) *\n  mapping(:σ; color=:group) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(filter(:type => ==(\"ρ\"), DataFrame(kbm02samp.allpars))) *\n  mapping(:value => \"Correlation\"; color=:names) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 3: Density plot of correlation parameters in bootstrap sample from model kbm02"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, but one further motivation is that Julia is at a broader “technical computing” audience (like MATLAB or perhaps Python) and less at a “statistical analysis” audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. Pluto helpfully installs and manages for you, but the package-manager REPL mode (activated by typing ] at the julia> prompt) is very useful."
  },
  {
    "objectID": "useful_packages.html#data-wrangling",
    "href": "useful_packages.html#data-wrangling",
    "title": "Useful packages",
    "section": "1 Data wrangling",
    "text": "1 Data wrangling\n\n1.1 Reading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather “column tables” – named tuples of column vectors.\n\n\n1.2 DataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R’s dplyr and much of the tidyverse. The DataFrames.jl documentation is the place to for looking at how to e.g. read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations.\nDataFrame.jl’s mini language can be a bit daunting, if you’re used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we’e had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or “pipe” together successive operations. It’s your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl."
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "2 Regression",
    "text": "2 Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don’t have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e. the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia > lm(@formula(y ~ 1 + x), data)\njulia > glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia > fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia > fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don’t recommend this as it’s easy to mess up in really subtle but very probelmatic ways.)\n\n2.1 @formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that’s because formulas are essentially their own domain-specific language (a variant of Wilkinson-Roger notation) and macros are used for manipulating the language itself – or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.\n\n\n2.2 Extensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the “extras” available in R, e.g. RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.\n\n\n2.3 Standardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e. on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g. centering new data around the mean computed during fitting the original data) correctly and automatically.\n\n\n2.4 Effects\nJohn Fox’s effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model’s overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we’re working on it!\n\n\n2.5 Estimated Marginal / Least Square Means\nTo our knowledge, There is currently nothing like the R package emmeans package in Julia. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. 😃"
  },
  {
    "objectID": "useful_packages.html#hypothesis-testing",
    "href": "useful_packages.html#hypothesis-testing",
    "title": "Useful packages",
    "section": "3 Hypothesis Testing",
    "text": "3 Hypothesis Testing\nClassical statistical tests such as the t-test can be found in the package HypothesisTests.jl."
  },
  {
    "objectID": "useful_packages.html#plotting-ecosystem",
    "href": "useful_packages.html#plotting-ecosystem",
    "title": "Useful packages",
    "section": "4 Plotting ecosystem",
    "text": "4 Plotting ecosystem\nThroughtout this course, we have used the Makie ecosystem for plotting, but there are several alternatives in Julia.\n\n4.1 Makie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g. MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Pluto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It’s a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly.\n\n\n4.2 Plots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX’s tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.\n\n\n4.3 Gadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the “gg” in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).\n\n\n4.4 Others\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite."
  },
  {
    "objectID": "useful_packages.html#connecting-to-other-languages",
    "href": "useful_packages.html#connecting-to-other-languages",
    "title": "Useful packages",
    "section": "5 Connecting to Other Languages",
    "text": "5 Connecting to Other Languages\nUsing Julia doesn’t mean you have to leave all the packages you knew in other languages behind. In Julia, it’s often possible to even easily and quickly invoke code from other languages from within Julia.\nRCall.jl provides a very convenient interface for interacting with R. JellyMe4.jl add support for moving MixedModels.jl and lme4 models back and forth between the languages (which means that you can use emmeans, sjtools, DHARMa, car, etc. to examine MixedModels.jl models!). RData.jl provides support for reading .rds and .rda files from Julia, while RDatasets.jl provides convenient access to many of the standard datasets provided by R and various R packages.\nPyCall.jl provides a very convenient way for interacting with Python code and packages. PyPlot.jl builds upon this foundation to provide support for Python’s matplotlib. Similarly, PyMNE.jl and PyFOOOF.jl provide some additional functionality to make interacting with MNE-Python and FOOOF from within Julia even easier than with vanilla PyCall.\nFor MATLAB users, there is also MATLAB.jl\nCxx.jl provides interoperability with C++. It also provides a C++ REPL mode, making it possible to treating C++ much more like a dynamic language than the traditional compiler toolchain would allow.\nSupport for calling C and Fortran is part of the Julia standard library."
  },
  {
    "objectID": "useful_packages.html#notebooks-and-webpages",
    "href": "useful_packages.html#notebooks-and-webpages",
    "title": "Useful packages",
    "section": "6 Notebooks and Webpages",
    "text": "6 Notebooks and Webpages\nWe haved Pluto.jl throughout this course because it provides a nice, easy way to interact with Julia, including automatic package management. PlutoUI.jl provides some nice tools for interacting with Pluto notebooks and making them more dynamic (including the TableOfContents() in this one!). We also like that Pluto notebooks are essentially normal Julia code stored in plain text under the hood, which means that they play quite nice with version control systems such as Git. Sometimes, however, the reactive nature of Pluto (update any one cell and the changes propogate to all impacted cells) is not desireable.\nAn alternative notebook interface is provide by Jupyter, formerly IPython. Indeed, the “Ju” actually stands for Julia. Jupyter support is provided by the package IJulia.jl.\nThe Weave.jl package provides “Julia markdown”, which is similar to knitr/RMarkdown or the the Sweave formats in R. Notably, Weave also provides support for converting between jmd files and Jupyter notebooks.\nSimilarly, Literate.jl is a simple package for literate programming (i.e. programming where documentation and code are “woven” together) and can generate Markdown, plain code and Jupyter notebook output. It is designed for simplicity and speed and is less extensive than Weave.jl and Documenter.jl.\nDocumenter.jl is the standard tool for building webpages from Julia documentation\nBooks.jl is a package designed to offer somewhat similar functionality to the bookdown package in R."
  }
]