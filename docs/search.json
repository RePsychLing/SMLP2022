[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sixth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "",
    "text": "This site provides materials for the Advanced frequentist methods stream of the Summer School on Statistical Methods to be held at the University of Potsdam, 12-16 September, 2022"
  },
  {
    "objectID": "index.html#git",
    "href": "index.html#git",
    "title": "Sixth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.1 git",
    "text": "1.1 git\nWe will assume that you have git installed and are able to clone a repository from github. If not, Happy Git with R is a good place to learn about git for data science.\nThis website is built using quarto, described below, from the repository. Clone this repository with, e.g.\ngit clone https://github.com/RePsychLing/SMLP2022"
  },
  {
    "objectID": "index.html#julia-programming-language",
    "href": "index.html#julia-programming-language",
    "title": "Sixth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.2 Julia Programming Language",
    "text": "1.2 Julia Programming Language\nWe will use Julia v1.8.0 in the summer school. You can download the version appropriate for your setup from here: Julia Programming Language"
  },
  {
    "objectID": "index.html#visual-studio-code-vs-code",
    "href": "index.html#visual-studio-code-vs-code",
    "title": "Sixth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.3 Visual Studio Code (VS Code)",
    "text": "1.3 Visual Studio Code (VS Code)\nWe will use VS Code IDE, that is Julia : VS Code ~ R : RStudio. You can download the version appropriate for your setup from here: VS Code"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Sixth Summer School on Statistical Methods for Linguistics and Psychology",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nThe web site and other documents for this course are rendered using a knitr-like system called Quarto. You can download the version appropriate for your setup from here: quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "useful_packages.html",
    "href": "useful_packages.html",
    "title": "Useful packages",
    "section": "",
    "text": "Unlike R, Julia does not immediately expose a huge number of functions, but instead requires loading packages (whether from the standard library or from the broader package ecosystem) for a lot of relevant functionality for statistical analysis. There are technical reasons for this, but one further motivation is that Julia is at a broader ‚Äútechnical computing‚Äù audience (like MATLAB or perhaps Python) and less at a ‚Äústatistical analysis‚Äù audience.\nThis has two important implications:\nThis notebook is not intended to be an exhaustive list of packages, but rather to highlight a few packages that I suspect will be particularly useful. Before getting onto the packages, I have one final hint: take advantage of how easy and first-class package management in Julia is. Having good package management makes reproducible analyses much easier and avoids breaking old analyses when you start a new one. Pluto helpfully installs and manages for you, but the package-manager REPL mode (activated by typing ] at the julia> prompt) is very useful."
  },
  {
    "objectID": "useful_packages.html#data-wrangling",
    "href": "useful_packages.html#data-wrangling",
    "title": "Useful packages",
    "section": "Data wrangling",
    "text": "Data wrangling\n\nReading data\n\nArrow.jl a high performance format for data storage, accessible in R via the arrow package and in Python via pyarrow. (Confusingly, the function for reading and writing Arrow format files in R is called read_feather and write_feather, but the modern Arrow format is distinct from the older Feather format provided by the feather package.) This is the format that we store the example and test datasets in for MixedModels.jl.\nCSV.jl useful for reading comma-separated values, tab-separated values and basically everything handled by the read.csv and read.table family of functions in R.\n\nNote that by default both Arrow.jl and CSV.jl do not return a DataFrame, but rather ‚Äúcolumn tables‚Äù ‚Äì named tuples of column vectors.\n\n\nDataFrames\nUnlike in R, DataFrames are not part of the base language, nor the standard library.\nDataFrames.jl provides the basic infrastructure around DataFrames, as well as its own mini language for doing the split-apply-combine approach that underlies R‚Äôs dplyr and much of the tidyverse. The DataFrames.jl documentation is the place to for looking at how to e.g.¬†read in a CSV or Arrow file as a DataFrame. Note that DataFrames.jl by default depends on CategoricalArrays.jl to handle the equivalent of factor in the R world, but there is an alternative package for factor-like array type in Julia, PooledArrays.jl. PooledArrays are simpler, but more limited than CategoricalArrays and we (Phillip and Doug) sometimes use them in our examples and simulations.\nDataFrame.jl‚Äôs mini language can be a bit daunting, if you‚Äôre used to manipulations in the style of base R or the tidyverse. For that, there are several options; recently, we‚Äôe had particularly nice experiences with DataFrameMacros.jl and Chain.jl for a convenient syntax to connect or ‚Äúpipe‚Äù together successive operations. It‚Äôs your choice whether and which of these add-ons you want to use! Phillip tends to write his code using raw DataFrames.jl, but Doug really enjoys DataFrameMacros.jl."
  },
  {
    "objectID": "useful_packages.html#regression",
    "href": "useful_packages.html#regression",
    "title": "Useful packages",
    "section": "Regression",
    "text": "Regression\nUnlike in R, neither formula processing nor basic regression are part of the base language or the standard library.\nThe formula syntax and basic contrast-coding schemes in Julia is provided by StatsModels.jl. By default, MixedModels.jl re-exports the @formula macro and most commonly used contrast schemes from StatsModels.jl, so you often don‚Äôt have to worry about loading StatsModels.jl directly. The same is true for GLM.jl, which provides basic linear and generalized linear models, such as ordinary least squares (OLS) regression and logistic regression, i.e.¬†the classical, non mixed regression models.\nThe basic functionality looks quite similar to R, e.g.\njulia > lm(@formula(y ~ 1 + x), data)\njulia > glm(@formula(y ~ 1 + x), data, Binomial(), LogitLink())\nbut the more general modelling API (also used by MixedModels.jl) is also supported:\njulia > fit(LinearModel, @formula(y ~ 1 + x), mydata)\njulia > fit(\n  GeneralizedLinearModel,\n  @formula(y ~ 1 + x),\n  data,\n  Binomial(),\n  LogitLink(),\n)\n(You can also specify your model matrices directly and skip the formula interface, but we don‚Äôt recommend this as it‚Äôs easy to mess up in really subtle but very probelmatic ways.)\n\n@formula, macros and domain-specific languages\nAs a sidebar: why is @formula a macro and not a normal function? Well, that‚Äôs because formulas are essentially their own domain-specific language (a variant of Wilkinson-Roger notation) and macros are used for manipulating the language itself ‚Äì or in this case, handling an entirely new, embedded language! This is also why macros are used by packages like Turing.jl and Soss.jl that define a language for Bayesian probabilistic programming like PyMC3 or Stan.\n\n\nExtensions to the formula syntax\nThere are several ongoing efforts to extend the formula syntax to include some of the ‚Äúextras‚Äù available in R, e.g.¬†RegressionFormulae.jl to use the caret (^) notation to limit interactions to a certain order ((a+b+c)^2 generates a + b + c + a&b + a&c + b&c, but not a&b&c). Note also that Julia uses & to express interactions, not : like in R.\n\n\nStandardizing Predictors\nAlthough function calls such as log can be used within Julia formulae, they must act on a rowwise basis, i.e.¬†on observations. Transformations such as z-scoring or centering (often done with scale in R) require knowledge of the entire column. StandardizedPredictors.jl provides functions for centering, scaling, and z-scoring within the formula. These are treated as pseudo-contrasts and computed on demand, meaning that predict and effects (see next) computations will handle these transformations on new data (e.g.¬†centering new data around the mean computed during fitting the original data) correctly and automatically.\n\n\nEffects\nJohn Fox‚Äôs effects package in R (and the related ggeffects package for plotting these using ggplot2) provides a nice way to visualize a model‚Äôs overall view of the data. This functionality is provided by Effects.jl and works out-of-the-box with most regression model packages in Julia (including MixedModels.jl). Support for formulae with embedded functions (such as log) is not yet complete, but we‚Äôre working on it!\n\n\nEstimated Marginal / Least Square Means\nEffects.jl provides a subset of the functionality (basic estimated-marginal means and exhaustive pairwise comparisons) of the R package emmeans package. However, it is often better to use sensible, hypothesis-driven contrast coding than to compute all pairwise comparisons after the fact. üòÉ"
  },
  {
    "objectID": "useful_packages.html#hypothesis-testing",
    "href": "useful_packages.html#hypothesis-testing",
    "title": "Useful packages",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nClassical statistical tests such as the t-test can be found in the package HypothesisTests.jl."
  },
  {
    "objectID": "useful_packages.html#plotting-ecosystem",
    "href": "useful_packages.html#plotting-ecosystem",
    "title": "Useful packages",
    "section": "Plotting ecosystem",
    "text": "Plotting ecosystem\nThroughtout this course, we have used the Makie ecosystem for plotting, but there are several alternatives in Julia.\n\nMakie\nThe Makie ecosystem is a relatively new take on graphics that aims to be both powerful and easy to use. Makie.jl itself only provides abstract definitions for many components (and is used in e.g.¬†MixedModelsMakie.jl to define plot types for MixedModels.jl). The actual plotting and rendering is handled by a backend package such as CairoMakie.jl (good for Quarto notebooks or rending static 2D images) and GLMakie.jl (good for dynamic, interactive visuals and 3D images). AlgebraOfGraphics.jl builds a grammar of graphics upon the Makie framework. It‚Äôs a great way to get good plots very quickly, but extensive customization is still best achieved by using Makie directly.\n\n\nPlots.jl\nPlots.jl is the original plotting package in Julia, but we often find it difficult to work with compared to some of the other alternatives. StatsPlots.jl builds on this, adding common statistical plots, while UnicodePlots.jl renders plots as Unicode characters directly in the REPL.\nPGFPlotsX.jl is a very new package that writes directly to PGF (the format used by LaTeX‚Äôs tikz framework) and can stand alone or be used as a rendering backend for the Plots.jl ecosystem.\n\n\nGadfly\nGadfly.jl was the original attempt to create a plotting system in Julia based on the grammar of graphics (the ‚Äúgg‚Äù in ggplot2). Development has largely stalled, but some functionality still exceeds AlgebraOfGraphics.jl, which has taken up the grammar of graphics mantle. Notably, the MixedModels.jl documentation still uses Gadfly as of this writing (early September 2021).\n\n\nOthers\nThere are many other graphics packages available in Julia, often wrapping well-established frameworks such as VegaLite."
  },
  {
    "objectID": "useful_packages.html#connecting-to-other-languages",
    "href": "useful_packages.html#connecting-to-other-languages",
    "title": "Useful packages",
    "section": "Connecting to Other Languages",
    "text": "Connecting to Other Languages\nUsing Julia doesn‚Äôt mean you have to leave all the packages you knew in other languages behind. In Julia, it‚Äôs often possible to even easily and quickly invoke code from other languages from within Julia.\nRCall.jl provides a very convenient interface for interacting with R. JellyMe4.jl add support for moving MixedModels.jl and lme4 models back and forth between the languages (which means that you can use emmeans, sjtools, DHARMa, car, etc. to examine MixedModels.jl models!). RData.jl provides support for reading .rds and .rda files from Julia, while RDatasets.jl provides convenient access to many of the standard datasets provided by R and various R packages.\nPyCall.jl provides a very convenient way for interacting with Python code and packages. PyPlot.jl builds upon this foundation to provide support for Python‚Äôs matplotlib. Similarly, PyMNE.jl and PyFOOOF.jl provide some additional functionality to make interacting with MNE-Python and FOOOF from within Julia even easier than with vanilla PyCall. More recently, PythonCall.jl has proven to be a populat alternative to PyCall.jl.\nFor MATLAB users, there is also MATLAB.jl\nCxx.jl provides interoperability with C++. It also provides a C++ REPL mode, making it possible to treating C++ much more like a dynamic language than the traditional compiler toolchain would allow.\nSupport for calling C and Fortran is part of the Julia standard library."
  },
  {
    "objectID": "pkg.html",
    "href": "pkg.html",
    "title": "Package management and reproducible environments",
    "section": "",
    "text": "Julia packages can be configured (in a file called Project.toml) on a per-project basis. The packaged sources and compiled versions are stored in a central location, e.g.¬†~/.julia/packages and ~/.julia/compiled on Linux systems, but the configuration of packages to be used can be local to a project. The Pkg package is used to modify the local project‚Äôs configuration. (An alternative is ‚Äúpackage mode‚Äù in the read-eval-print-loop or REPL, which we will show at the summer school.) Start julia in the directory of the cloned SMLP2022 repository\n\nusing Pkg        # there's a package called 'Pkg' to manipulate package configs\nPkg.activate(\".\")# activate the current directory as the project\n\nIf you‚Äôve recieved an environment from someone/somwhere else ‚Äì such as this course repository ‚Äì then you‚Äôll need to first ‚Äúinstantiate‚Äù it (i.e., install all the dependencies).\n\nPkg.instantiate()# only needed the first time you work in a project\nPkg.update()     # get the latest package versions compatible with the project\n\n\nPkg.status()\n\nOccasionally the Pkg.status function call will give info about new versions being available but blocked by requirements of other packages. This is to be expected - the package system is large and the web of dependencies are complex. Generally the Julia package system is very good at resolving dependencies."
  },
  {
    "objectID": "sleepstudy.html",
    "href": "sleepstudy.html",
    "title": "Analysis of the sleepstudy data",
    "section": "",
    "text": "The sleepstudy data are from a study of the effects of sleep deprivation on response time reported in Balkin et al. (2000) and in Belenky et al. (2003). Eighteen subjects were allowed only 3 hours of time to sleep each night for 9 successive nights. Their reaction time was measured each day, starting the day before the first night of sleep deprivation, when the subjects were on their regular sleep schedule."
  },
  {
    "objectID": "sleepstudy.html#loading-the-data",
    "href": "sleepstudy.html#loading-the-data",
    "title": "Analysis of the sleepstudy data",
    "section": "Loading the data",
    "text": "Loading the data\nFirst attach the MixedModels package and other packages for plotting. The CairoMakie package allows the Makie graphics system (Danisch & Krumbiegel, 2021) to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.\n\n\nCode\nusing CairoMakie       # graphics back-end\nusing DataFrameMacros  # simplified dplyr-like data wrangling\nusing DataFrames\nusing KernelDensity    # density estimation\nusing MixedModels\nusing MixedModelsMakie # diagnostic plots\nusing ProgressMeter\nusing Random           # random number generators\nusing RCall            # call R from Julia\n\nProgressMeter.ijulia_behavior(:clear);\nCairoMakie.activate!(; type=\"svg\");\n\n\nThe sleepstudy data are one of the datasets available with the MixedModels package.\n\nsleepstudy = MixedModels.dataset(\"sleepstudy\")\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nFigure¬†1 displays the data in a multi-panel plot created with the lattice package in R (Sarkar, 2008), using RCall.jl.\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"); width=10, height=4.5)\nR\"\"\"\nrequire(\"lattice\", quietly=TRUE)\nprint(xyplot(reaction ~ days | subj,\n  $(DataFrame(sleepstudy)),\n  aspect=\"xy\",\n  layout=c(9,2),\n  type=c(\"g\", \"p\", \"r\"),\n  index.cond=function(x,y) coef(lm(y ~ x))[1],\n  xlab = \"Days of sleep deprivation\",\n  ylab = \"Average reaction time (ms)\"\n))\n\"\"\";\n\n\n\n\n\nFigure¬†1: Average response time versus days of sleep deprivation by subject\n\n\n\n\nEach panel shows the data from one subject and a line fit by least squares to that subject‚Äôs data. Starting at the lower left panel and proceeding across rows, the panels are ordered by increasing intercept of the least squares line.\nThere are some deviations from linearity within the panels but the deviations are neither substantial nor systematic."
  },
  {
    "objectID": "sleepstudy.html#fitting-an-initial-model",
    "href": "sleepstudy.html#fitting-an-initial-model",
    "title": "Analysis of the sleepstudy data",
    "section": "Fitting an initial model",
    "text": "Fitting an initial model\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\nMinimizing 57    Time: 0:00:00 ( 7.50 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept, representing the typical reaction time at the beginning of the experiment with zero days of sleep deprivation, and the slope w.r.t. days of sleep deprivation. The parameter estimates are about 250 ms. typical reaction time without deprivation and a typical increase of 10.5 ms. per day of sleep deprivation.\nThe random effects represent shifts from the typical behavior for each subject. The shift in the intercept has a standard deviation of about 24 ms. which would suggest a range of about 200 ms. to 300 ms. in the intercepts. Similarly within-subject slopes would be expected to have a range of about 0 ms./day up to 20 ms./day.\nThe random effects for the slope and for the intercept are allowed to be correlated within subject. The estimated correlation, 0.08, is small. This estimate is not shown in the default display above but is shown in the output from VarCorr (variance components and correlations).\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\n\n\nTechnically, the random effects for each subject are unobserved random variables and are not ‚Äúparameters‚Äù in the model per se. Hence we do not report standard errors or confidence intervals for these deviations. However, we can produce prediction intervals on the random effects for each subject. Because the experimental design is balanced, these intervals will have the same width for all subjects.\nA plot of the prediction intervals versus the level of the grouping factor (subj, in this case) is sometimes called a caterpillar plot because it can look like a fuzzy caterpillar if there are many levels of the grouping factor. By default, the levels of the grouping factor are sorted by increasing value of the first random effect.\n\n\nCode\ncaterpillar(m1)\n\n\n\n\n\nFigure¬†2: Prediction intervals on random effects for model m1\n\n\n\n\nFigure¬†2 reinforces the conclusion that there is little correlation between the random effect for intercept and the random effect for slope."
  },
  {
    "objectID": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "href": "sleepstudy.html#a-model-with-uncorrelated-random-effects",
    "title": "Analysis of the sleepstudy data",
    "section": "A model with uncorrelated random effects",
    "text": "A model with uncorrelated random effects\nThe zerocorr function applied to a random-effects term creates uncorrelated vector-valued per-subject random effects.\n\nm2 = let\n  form = @formula reaction ~ 1 + days + zerocorr(1 + days | subj)\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\nAgain, the default display doesn‚Äôt show that there is no correlation parameter to be estimated in this model, but the VarCorr display does.\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n584.25897\n24.17145\n\n\n\n\ndays\n33.63281\n5.79938\n.\n\n\nResidual\n\n653.11578\n25.55613\n\n\n\n\n\n\nThis model has a slightly lower log-likelihood than does m1 and one fewer parameter than m1. A likelihood-ratio test can be used to compare these nested models.\n\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nreaction ~ 1 + days + MixedModels.ZeroCorr((1 + days | subj))\n5\n1752\n\n\n\n\n\nreaction ~ 1 + days + (1 + days | subj)\n6\n1752\n0\n1\n0.8004\n\n\n\n\n\nAlternatively, the AIC or BIC values can be compared.\n\n\nCode\nlet mods = [m2, m1]\n  DataFrame(;\n    model=[:m2, :m1],\n    pars=dof.(mods),\n    geomdof=(sum ‚àò leverage).(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows √ó 6 columnsmodelparsgeomdofAICBICAICcSymbolInt64Float64Float64Float64Float641m2529.0451762.01777.971762.352m1628.61151763.941783.11764.42\n\n\nThe goodness of fit measures: AIC, BIC, and AICc, are all on a ‚Äúsmaller is better‚Äù scale and, hence, they all prefer m2.\nThe pars column, which is the same as the model-dof column in the likelihood ratio test output, is simply a count of the number of parameters to be estimated when fitting the model. For example, in m2 there are two fixed-effects parameters and three variance components (including the residual variance).\nAn alternative, more geometrically inspired definition of ‚Äúdegrees of freedom‚Äù, is the sum of the leverage values, called geomdof in this table.\nInterestingly, the model with fewer parameters, m2, has a greater sum of the leverage values than the model with more parameters, m1. We‚Äôre not sure what to make of that.\nIn both cases the sum of the leverage values is toward the upper end of the range of possible values, which is the rank of the fixed-effects model matrix (2) up to the rank of the fixed-effects plus the random effects model matrix (2 + 36 = 38).\n\n\n\n\n\n\nNote\n\n\n\nI think that the upper bound may be 36, not 38, because the two columns of X lie in the column span of Z\n\n\nThis comparison does show, however, that a simple count of the parameters in a mixed-effects model can underestimate, sometimes drastically, the model complexity. This is because a single variance component or multiple components can add many dimensions to the linear predictor."
  },
  {
    "objectID": "sleepstudy.html#some-diagnostic-plots",
    "href": "sleepstudy.html#some-diagnostic-plots",
    "title": "Analysis of the sleepstudy data",
    "section": "Some diagnostic plots",
    "text": "Some diagnostic plots\nIn mixed-effects models the linear predictor expression incorporates fixed-effects parameters, which summarize trends for the population or certain well-defined subpopulations, and random effects which represent deviations associated with the experimental units or observational units - individual subjects, in this case. The random effects are modeled as unobserved random variables.\nThe conditional means of these random variables, sometimes called the BLUPs or Best Linear Unbiased Predictors, are not simply the least squares estimates. They are attenuated or shrunk towards zero to reflect the fact that the individuals are assumed to come from a population. A shrinkage plot, Figure¬†3, shows the BLUPs from the model fit compared to the values without any shrinkage. If the BLUPs are similar to the unshrunk values then the more complicated model accounting for individual differences is supported. If the BLUPs are strongly shrunk towards zero then the additional complexity in the model to account for individual differences is not providing sufficient increase in fidelity to the data to warrant inclusion.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m1)\n\n\n\n\n\nFigure¬†3: Shrinkage plot of means of the random effects in model m1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot could be drawn as shrinkageplot(m1). The reason for explicitly creating a Figure to be modified by shrinkageplot! is to control the resolution.\n\n\nThis plot shows an intermediate pattern. The random effects are somewhat shrunk toward the origin, a model simplification trend, but not completely shrunk - indicating that fidelity to the data is enhanced with these additional coefficients in the linear predictor.\nIf the shrinkage were primarily in one direction - for example, if the arrows from the unshrunk values to the shrunk values were mostly in the vertical direction - then we would get an indication that we could drop the random effect for slope and revert to a simpler model. This is not the case here.\nAs would be expected, the unshrunk values that are further from the origin tend to be shrunk more toward the origin. That is, the arrows that originate furthest from the origin are longer. However, that is not always the case. The arrow in the upper right corner, from S337, is relatively short. Examination of the panel for S337 in the data plot shows a strong linear trend, even though both the intercept and the slope are unusually large. The neighboring panels in the data plot, S330 and S331, have more variability around the least squares line and are subject to a greater amount of shrinkage in the model. (They correspond to the two arrows on the right hand side of the figure around -5 on the vertical scale.)"
  },
  {
    "objectID": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "href": "sleepstudy.html#assessing-variability-by-bootstrapping",
    "title": "Analysis of the sleepstudy data",
    "section": "Assessing variability by bootstrapping",
    "text": "Assessing variability by bootstrapping\nThe speed of fitting linear mixed-effects models using MixedModels.jl allows for using simulation-based approaches to inference instead of relying on approximate standard errors. A parametric bootstrap sample for model m is a collection of models of the same form as m fit to data values simulated from m. That is, we pretend that m and its parameter values are the true parameter values, simulate data from these values, and estimate parameters from the simulated data.\nSimulating and fitting a substantial number of model fits, 5000 in this case, takes only a few seconds, following which we extract a data frame of the parameter estimates and plot densities of some of these estimates.\n\nrng = Random.seed!(42)    # initialize a random number generator\nm1bstp = parametricbootstrap(rng, 5000, m1; hide_progress=true)\nallpars = DataFrame(m1bstp.allpars)\n\n\n30,000 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)260.71221Œ≤missingdays9.8497531œÉsubj(Intercept)15.331741œÉsubjdays6.4029651œÅsubj(Intercept), days-0.025958261œÉresidualmissing23.409272Œ≤missing(Intercept)262.25382Œ≤missingdays12.300892œÉsubj(Intercept)16.3183102œÉsubjdays5.54688112œÅsubj(Intercept), days0.552607122œÉresidualmissing25.7047133Œ≤missing(Intercept)253.149143Œ≤missingdays12.879153œÉsubj(Intercept)25.4787163œÉsubjdays6.1444173œÅsubj(Intercept), days0.0691545183œÉresidualmissing22.2753194Œ≤missing(Intercept)263.376204Œ≤missingdays11.5798214œÉsubj(Intercept)18.8039224œÉsubjdays4.6557234œÅsubj(Intercept), days0.103361244œÉresidualmissing23.3128255Œ≤missing(Intercept)248.429265Œ≤missingdays9.39444275œÉsubj(Intercept)20.1411285œÉsubjdays5.27358295œÅsubj(Intercept), days-0.163603305œÉresidualmissing25.4355‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAn empirical density plot of the estimates for the fixed-effects coefficients, Figure¬†4, shows the normal distribution, ‚Äúbell-curve‚Äù, shape as we might expect.\n\n\nCode\nbegin\n  f1 = Figure(; resolution=(1000, 400))\n  CairoMakie.density!(\n    Axis(f1[1, 1]; xlabel=\"Intercept [ms]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f1[1, 2]; xlabel=\"Coefficient of days [ms/day]\"),\n    @subset(allpars, :type == \"Œ≤\" && :names == \"days\").value,\n  )\n  f1\nend\n\n\n\n\n\nFigure¬†4: Empirical density plots of bootstrap replications of fixed-effects parameter estimates\n\n\n\n\nIt is also possible to create interval estimates of the parameters from the bootstrap replicates. We define the 1-Œ± shortestcovint to be the shortest interval that contains a proportion 1-Œ± (defaults to 95%) of the bootstrap estimates of the parameter.\n\nDataFrame(shortestcovint(m1bstp))\n\n\n6 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)239.64265.2282Œ≤missingdays7.4234713.16073œÉsubj(Intercept)10.172233.08764œÉsubjdays2.994767.661215œÅsubj(Intercept), days-0.4013531.06œÉresidualmissing22.70128.5016\n\n\nThe intervals look reasonable except that the upper bound on the interval for œÅ, the correlation coefficient, is 1.0 . It turns out that the estimates of œÅ have a great deal of variability.\nEven more alarming, some of these œÅ values are undefined (denoted NaN) because the way œÅ is calculated can create a division by zero.\n\ndescribe(@select(@subset(allpars, :type == \"œÅ\"), :value))\n\n\n1 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolFloat64Float64NothingFloat64Int64DataType1valueNaNNaNNaN0Float64\n\n\nBecause there are several values on the boundary (œÅ = 1.0) and a pulse like this is not handled well by a density plot, we plot this sample as a histogram, Figure¬†5.\n\n\nCode\nhist(\n  @subset(allpars, :type == \"œÅ\", isfinite(:value)).value;\n  bins=40,\n  axis=(; xlabel=\"Estimated correlation of the random effects\"),\n  figure=(; resolution=(500, 500)),\n)\n\n\n\n\n\nFigure¬†5: Histogram of bootstrap replications of the within-subject correlation parameter\n\n\n\n\nFinally, density plots for the variance components (but on the scale of the standard deviation), Figure¬†6, show reasonable symmetry.\n\n\nCode\nbegin\n  œÉs = @subset(allpars, :type == \"œÉ\")\n  f2 = Figure(; resolution=(1000, 300))\n  CairoMakie.density!(\n    Axis(f2[1, 1]; xlabel=\"Residual œÉ\"),\n    @subset(œÉs, :group == \"residual\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 2]; xlabel=\"subj-Intercept œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"(Intercept)\").value,\n  )\n  CairoMakie.density!(\n    Axis(f2[1, 3]; xlabel=\"subj-slope œÉ\"),\n    @subset(œÉs, :group == \"subj\" && :names == \"days\").value,\n  )\n  f2\nend\n\n\n\n\n\nFigure¬†6: Empirical density plots of bootstrap replicates of standard deviation estimates\n\n\n\n\nThe estimates of the coefficients, Œ≤‚ÇÅ and Œ≤‚ÇÇ, are not highly correlated as shown in a scatterplot of the bootstrap estimates, Figure¬†7 .\n\nvcov(m1; corr=true)  # correlation estimate from the model\n\n2√ó2 Matrix{Float64}:\n  1.0       -0.137545\n -0.137545   1.0\n\n\n\n\nCode\nlet\n  vals = disallowmissing(\n    Array(\n      select(\n        unstack(DataFrame(m1bstp.Œ≤), :iter, :coefname, :Œ≤),\n        Not(:iter),\n      ),\n    ),\n  )\n  scatter(\n    vals;\n    color=(:blue, 0.20),\n    axis=(; xlabel=\"Intercept\", ylabel=\"Coefficient of days\"),\n    figure=(; resolution=(500, 500)),\n  )\n  contour!(kde(vals))\n  current_figure()\nend\n\n\n\n\n\nFigure¬†7: Scatter-plot of bootstrap replicates of fixed-effects estimates with contours"
  },
  {
    "objectID": "sleepstudy_speed.html",
    "href": "sleepstudy_speed.html",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "",
    "text": "Belenky et al. (2003) reported effects of sleep deprivation across a 14-day study of 30-to-40-year old men and women holding commercial vehicle driving licenses. Their analyses are based on a subset of tasks and ratings from very large and comprehensive test and questionnaire battery (Balkin et al., 2000).\nInitially 66 subjects were assigned to one of four time-in-bed (TIB) groups with 9 hours (22:00-07:00) of sleep augmentation or 7 hours (24:00-07:00), 5 hours (02:00-07:00), and 3 hours (04:00-0:00) of sleep restrictions per night, respectively. The final sample comprised 56 subjects. The Psychomotor Vigilance Test (PVT) measures simple reaction time to a visual stimulus, presented approximately 10 times ‚ÅÑ minute (interstimulus interval varied from 2 to 10 s in 2-s increments) for 10 min and implemented in a thumb-operated, hand-held device (Dinges & Powell, 1985).\nDesign\nThe study comprised 2 training days (T1, T2), one day with baseline measures (B), seven days with sleep deprivation (E1 to E7), and four recovery days (R1 to R4). T1 and T2 were devoted to training on the performance tests and familiarization with study procedures. PVT baseline testing commenced on the morning of the third day (B) and testing continued for the duration of the study (E1‚ÄìE7, R1‚ÄìR3; no measures were taken on R4). Bed times during T, B, and R days were 8 hours (23:00-07:00).\nTest schedule within days\nThe PVT (along with the Stanford Sleepiness Scale) was administered as a battery four times per day (09:00, 12:00, 15:00, and 21:00 h); the battery included other tests not reported here (see Balkin et al., 2000). The sleep latency test was administered at 09:40 and 15:30 h for all groups. Subjects in the 3- and 5-h TIB groups performed an additional battery at 00:00 h and 02:00 h to occupy their additional time awake. The PVT and SSS were administered in this battery; however, as data from the 00:00 and 02:00 h sessions were not common to all TIB groups, these data were not included in the statistical analyses reported in the paper.\nStatistical analyses\nThe authors analyzed response speed, that is (1/RT)*1000 ‚Äì completely warranted according to a Box-Cox check of the current data ‚Äì with mixed-model ANOVAs using group as between- and day as within-subject factors. The ANOVA was followed up with simple tests of the design effects implemented over days for each of the four groups.\nCurrent data\nThe current data distributed with the RData collection is attributed to the 3-hour TIB group, but the means do not agree at all with those reported for this group in (Belenky et al., 2003, fig. 3) where the 3-hour TIB group is also based on only 13 (not 18) subjects. Specifically, the current data show a much smaller slow-down of response speed across E1 to E7 and do not reflect the recovery during R1 to R3. The currrent data also cover only 10 not 11 days, but it looks like only R3 is missing. The closest match of the current means was with the average of the 3-hour and 7-hour TIB groups; if only males were included, this would amount to 18 subjects. (This conjecture is based only on visual inspection of graphs.)"
  },
  {
    "objectID": "sleepstudy_speed.html#setup",
    "href": "sleepstudy_speed.html#setup",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Setup",
    "text": "Setup\nFirst we attach the various packages needed, define a few helper functions, read the data, and get everything in the desired shape.\n\n\nCode\nusing CairoMakie         # device driver for static (SVG, PDF, PNG) plots\nusing Chain              # like pipes but cleaner\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie   # plots specific to mixed-effects models using Makie\n\nusing ProgressMeter\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "sleepstudy_speed.html#preprocessing",
    "href": "sleepstudy_speed.html#preprocessing",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe sleepstudy data are one of the datasets available with recent versions of the MixedModels package. We carry out some preprocessing to have the dataframe in the desired shape:\n\nCapitalize random factor Subj\nCompute speed as an alternative dependent variable from reaction, warranted by a ‚Äòboxcox‚Äô check of residuals.\nCreate a GroupedDataFrame by levels of Subj (the original dataframe is available as gdf.parent, which we name df)\n\n\ngdf = @chain MixedModels.dataset(:sleepstudy) begin\n  DataFrame\n  rename!(:subj => :Subj, :days => :day)\n  @transform!(:speed = 1000 / :reaction)\n  groupby(:Subj)\nend\n\n\nGroupedDataFrame with 18 groups based on key: SubjFirst Group (10 rows): Subj = \"S308\"SubjdayreactionspeedStringInt8Float64Float641S3080249.564.007052S3081258.7053.865413S3082250.8013.987234S3083321.443.1115S3084356.8522.802286S3085414.692.411447S3086382.2042.616418S3087290.1493.446519S3088430.5852.3224210S3089466.3532.1443‚ãÆLast Group (10 rows): Subj = \"S372\"SubjdayreactionspeedStringInt8Float64Float641S3720269.4123.711792S3721273.4743.656653S3722297.5973.360254S3723310.6323.219255S3724287.1733.482236S3725329.6083.033917S3726334.4822.98978S3727343.222.913589S3728369.1422.7089910S3729364.1242.74632\n\n\n\ndf = gdf.parent\ndescribe(df)\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS308S3720String2day4.504.590Int83reaction298.508194.332288.651466.3530Float644speed3.466342.14433.464435.145830Float64"
  },
  {
    "objectID": "sleepstudy_speed.html#estimates-for-pooled-data",
    "href": "sleepstudy_speed.html#estimates-for-pooled-data",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Estimates for pooled data",
    "text": "Estimates for pooled data\nIn the first analysis we ignore the dependency of observations due to repeated measures from the same subjects. We pool all the data and estimate the regression of 180 speed scores on the nine days of the experiment.\n\npooledcoef = simplelinreg(df.day, df.speed)  # produces a Tuple\n\n(3.96581197478315, -0.11099359232199725)"
  },
  {
    "objectID": "sleepstudy_speed.html#within-subject-effects",
    "href": "sleepstudy_speed.html#within-subject-effects",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Within-subject effects",
    "text": "Within-subject effects\nIn the second analysis we estimate coefficients for each Subj without regard of the information available from the complete set of data. We do not ‚Äúborrow strength‚Äù to adjust for differences due to between-Subj variability and due to being far from the population mean.\n\nWithin-subject simple regressions\nApplying combine to a grouped data frame like gdf produces a DataFrame with a row for each group. The permutation ord provides an ordering for the groups by increasing intercept (predicted response at day 0).\n\nwithin = combine(gdf, [:day, :speed] => simplelinreg => :coef)\n\n\n18 rows √ó 2 columnsSubjcoefStringTuple‚Ä¶1S308(3.94806, -0.194812)2S309(4.87022, -0.0475185)3S310(4.90606, -0.120054)4S330(3.4449, -0.0291309)5S331(3.47647, -0.0498047)6S332(3.84436, -0.105511)7S333(3.60159, -0.0917378)8S334(4.04528, -0.133527)9S335(3.80451, 0.0455771)10S337(3.34374, -0.137744)11S349(4.46855, -0.170885)12S350(4.21414, -0.20151)13S351(3.80469, -0.0728582)14S352(3.68634, -0.144957)15S369(3.85384, -0.120531)16S370(4.52679, -0.215965)17S371(3.853, -0.0936243)18S372(3.69208, -0.113292)\n\n\nFigure¬†1 shows the reaction speed versus days of sleep deprivation by subject. The panels are arranged by increasing initial reaction speed starting at the lower left and proceeding across rows.\n\n\nCode\nlet\n  ord = sortperm(first.(within.coef))\n  labs = values(only.(keys(gdf)))[ord]       # labels for panels\n  f = clevelandaxes!(Figure(; resolution=(1000, 750)), labs, (2, 9))\n  for (axs, sdf) in zip(f.content, gdf[ord]) # iterate over the panels and groups\n    scatter!(axs, sdf.day, sdf.speed)      # add the points\n    coef = simplelinreg(sdf.day, sdf.speed)\n    abline!(axs, first(coef), last(coef))  # add the regression line\n  end\n  f\nend\n\n\n‚îå Warning: abline! is deprecated and will be removed in the future. Use ablines / ablines! instead.\n‚îÇ   caller = top-level scope at In[7]:8\n‚îî @ Core ./In[7]:8\n\n\n\n\n\nFigure¬†1: Reaction speed (s‚Åª¬π) versus days of sleep deprivation by subject"
  },
  {
    "objectID": "sleepstudy_speed.html#basic-lmm",
    "href": "sleepstudy_speed.html#basic-lmm",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Basic LMM",
    "text": "Basic LMM\n\ncontrasts = Dict(:Subj => Grouping())\nm1 = let\n  form = @formula speed ~ 1 + day + (1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\nMinimizing 69    Time: 0:00:00 ( 5.76 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1056\n37.55\n<1e-99\n0.4190\n\n\nday\n-0.1110\n0.0151\n-7.37\n<1e-12\n0.0566\n\n\nResidual\n0.2698\n\n\n\n\n\n\n\n\n\nThis model includes fixed effects for the intercept which estimates the average speed on the baseline day of the experiment prior to sleep deprivation, and the slowing per day of sleep deprivation. In this case about -0.11/second.\nThe random effects represent shifts from the typical behavior for each subject.The shift in the intercept has a standard deviation of about 0.42/s.\nThe within-subject correlation of the random effects for intercept and slope is small, -0.18, indicating that a simpler model with a correlation parameter (CP) forced to/ assumed to be zero may be sufficient."
  },
  {
    "objectID": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "href": "sleepstudy_speed.html#no-correlation-parameter-zcp-lmm",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "No correlation parameter: zcp LMM",
    "text": "No correlation parameter: zcp LMM\nThe zerocorr function applied to a random-effects term estimates one parameter less than LMM m1‚Äì the CP is now fixed to zero.\n\nm2 = let\n  form = @formula speed ~ 1 + day + zerocorr(1 + day | Subj)\n  fit(MixedModel, form, df; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n3.9658\n0.1033\n38.38\n<1e-99\n0.4085\n\n\nday\n-0.1110\n0.0147\n-7.53\n<1e-13\n0.0550\n\n\nResidual\n0.2706\n\n\n\n\n\n\n\n\n\nLMM m2 has a slghtly lower log-likelihood than LMM m1 but also one fewer parameters. A likelihood-ratio test is used to compare these nested models.\n\n\nCode\nMixedModels.likelihoodratiotest(m2, m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nspeed ~ 1 + day + MixedModels.ZeroCorr((1 + day | Subj))\n5\n125\n\n\n\n\n\nspeed ~ 1 + day + (1 + day | Subj)\n6\n125\n0\n1\n0.5192\n\n\n\n\n\nAlternatively, the AIC, AICc, and BIC values can be compared. They are on a scale where ‚Äúsmaller is better‚Äù. All three model-fit statistics prefer the zcpLMM m2.\n\n\nCode\nlet\n  mods = [m2, m1]\n  DataFrame(;\n    dof=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    AICc=aicc.(mods),\n    BIC=bic.(mods),\n  )\nend\n\n\n\n2 rows √ó 5 columnsdofdevianceAICAICcBICInt64Float64Float64Float64Float6415125.379135.379135.724151.34426124.964136.964137.45156.122"
  },
  {
    "objectID": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "href": "sleepstudy_speed.html#conditional-modes-of-the-random-effects",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Conditional modes of the random effects",
    "text": "Conditional modes of the random effects\nThe third set of estimates are their conditional modes. They represent a compromise between their own data and the model parameters. When distributional assumptions hold, predictions based on these estimates are more accurate than either the pooled or the within-subject estimates. Here we ‚Äúborrow strength‚Äù to improve the accuracy of prediction."
  },
  {
    "objectID": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "href": "sleepstudy_speed.html#caterpillar-plots-effect-profiles",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Caterpillar plots (effect profiles)",
    "text": "Caterpillar plots (effect profiles)\n\n\nCode\ncaterpillar(m2)\n\n\n\n\n\nFigure¬†2: Prediction intervals on the random effects in model m2"
  },
  {
    "objectID": "sleepstudy_speed.html#shrinkage-plot",
    "href": "sleepstudy_speed.html#shrinkage-plot",
    "title": "The sleepstudy: Speed - for a change ‚Ä¶",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot\n\n\nCode\nshrinkageplot!(Figure(; resolution=(500, 500)), m2)\n\n\n\n\n\nFigure¬†3: Shrinkage plot of the means of the random effects in model m2"
  },
  {
    "objectID": "arrow.html",
    "href": "arrow.html",
    "title": "Notes on saved data files",
    "section": "",
    "text": "The Arrow storage format provides a language-agnostic storage and memory specification for columnar data tables, which just means ‚Äúsomething that looks like a data frame in R‚Äù. That is, an arrow table is an ordered, named collection of columns, all of the same length.\nThe columns can be of different types including numeric values, character strings, and factor-like representations - called DictEncoded.\nAn Arrow file can be read or written from R, Python, Julia and many other languages. Somewhat confusingly in R and Python the name feather, which refers to an earlier version of the storage format, is used in some function names like read_feather."
  },
  {
    "objectID": "arrow.html#the-emotikon-data",
    "href": "arrow.html#the-emotikon-data",
    "title": "Notes on saved data files",
    "section": "The Emotikon data",
    "text": "The Emotikon data\nThe SMLP2021 repository contains a version of the data from F√ºhner et al. (2021) in notebooks/data/fggk21.arrow. After that file was created there were changes in the master RDS file on the osf.io site for the project. We will recreate the Arrow file here then split it into two separate tables, one with a row for each child in the study and one with a row for each test result.\nThe Arrow package for Julia does not export any function names, which means that the function to read an Arrow file must be called as Arrow.Table. It returns a column table, as described in the Tables package. This is like a read-only data frame, which can be easily converted to a full-fledged DataFrame if desired.\nThis arrangement allows for the Arrow package not to depend on the DataFrames package, which is a heavy-weight dependency, but still easily produce a DataFrame if warranted.\nLoad the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Downloads\nusing KernelDensity\nusing RCall   # access R from within Julia\nusing StatsBase\n\nCairoMakie.activate!(; type=\"svg\")\nusing AlgebraOfGraphics: density\n\n\n‚îå Info: Precompiling AlgebraOfGraphics [cbdf2221-f076-402e-a563-3d30da359d67]\n‚îî @ Base loading.jl:1662"
  },
  {
    "objectID": "arrow.html#downloading-and-importing-the-rds-file",
    "href": "arrow.html#downloading-and-importing-the-rds-file",
    "title": "Notes on saved data files",
    "section": "Downloading and importing the RDS file",
    "text": "Downloading and importing the RDS file\nThis is similar to some of the code shown by Julius Krumbiegel on Monday. In the data directory of the emotikon project on osf.io under Data, the url for the rds data file is found to be [https://osf.io/xawdb/]. Note that we want version 2 of this file.\n\nfn = Downloads.download(\"https://osf.io/xawdb/download?version=2\");\n\n\ndfrm = rcopy(R\"readRDS($fn)\")\n\n\n525,126 rows √ó 7 columnsCohortSchoolChildSexageTestscoreCat‚Ä¶Cat‚Ä¶Cat‚Ä¶Cat‚Ä¶Float64Cat‚Ä¶Float6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nNow write this file as a Arrow file and read it back in.\n\narrowfn = joinpath(\"data\", \"fggk21.arrow\")\nArrow.write(arrowfn, dfrm; compress=:lz4)\ntbl = Arrow.Table(arrowfn)\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\nfilesize(arrowfn)\n\n3077850\n\n\n\ndf = DataFrame(tbl)\n\n\n525,126 rows √ó 7 columnsCohortSchoolChildSexageTestscoreStringStringStringStringFloat64StringFloat6412013S100067C002352male7.99452S20_r5.2631622013S100067C002352male7.99452BPT3.732013S100067C002352male7.99452SLJ125.042013S100067C002352male7.99452Star_r2.4714652013S100067C002352male7.99452Run1053.062013S100067C002353male7.99452S20_r5.072013S100067C002353male7.99452BPT4.182013S100067C002353male7.99452SLJ116.092013S100067C002353male7.99452Star_r1.76778102013S100067C002353male7.99452Run1089.0112013S100067C002354male7.99452S20_r4.54545122013S100067C002354male7.99452BPT3.9132013S100067C002354male7.99452SLJ111.0142013S100067C002354male7.99452Star_r1.98875152013S100067C002354male7.99452Run864.0162013S100122C002355female7.99452S20_r4.54545172013S100122C002355female7.99452BPT3.0182013S100122C002355female7.99452SLJ114.0192013S100122C002355female7.99452Star_r1.84464202013S100122C002355female7.99452Run835.0212013S100146C002356male7.99452S20_r4.34783222013S100146C002356male7.99452BPT3.3232013S100146C002356male7.99452SLJ118.0242013S100146C002356male7.99452Star_r1.90682252013S100146C002356male7.99452Run860.0262013S100146C002357male7.99452S20_r4.34783272013S100146C002357male7.99452BPT4.3282013S100146C002357male7.99452SLJ130.0292013S100146C002357male7.99452Star_r1.99655302013S100146C002357male7.99452Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ"
  },
  {
    "objectID": "arrow.html#avoiding-needless-repetition",
    "href": "arrow.html#avoiding-needless-repetition",
    "title": "Notes on saved data files",
    "section": "Avoiding needless repetition",
    "text": "Avoiding needless repetition\nOne of the principles of relational database design is that information should not be repeated needlessly. Each row of df is determined by a combination of Child and Test, together producing a score, which can be converted to a zScore.\nThe other columns in the table, Cohort, School, age, and Sex, are properties of the Child.\nStoring these values redundantly in the full table takes up space but, more importantly, allows for inconsistency. As it stands, a given Child could be recorded as being in one Cohort for the Run test and in another Cohort for the S20_r test and nothing about the table would detect this as being an error.\nThe approach used in relational databases is to store the information for score in one table that contains only Child, Test and score, store the information for the Child in another table including Cohort, School, age and Sex. These tables can then be combined to create the table to be used for analysis by joining the different tables together.\nThe maintainers of the DataFrames package have put in a lot of work over the past few years to make joins quite efficient in Julia. Thus the processing penalty of reassembling the big table from three smaller tables is minimal.\nIt is important to note that the main advantage of using smaller tables that are joined together to produce the analysis table is the fact that the information in the analysis table is consistent by design."
  },
  {
    "objectID": "arrow.html#creating-the-smaller-table",
    "href": "arrow.html#creating-the-smaller-table",
    "title": "Notes on saved data files",
    "section": "Creating the smaller table",
    "text": "Creating the smaller table\n\nChild = unique(select(df, :Child, :School, :Cohort, :Sex, :age))\n\n\n108,295 rows √ó 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nlength(unique(Child.Child))  # should be 108295\n\n108295\n\n\n\nfilesize(\n  Arrow.write(\"./data/fggk21_Child.arrow\", Child; compress=:lz4)\n)\n\n1774946\n\n\n\nfilesize(\n  Arrow.write(\n    \"./data/fggk21_Score.arrow\",\n    select(df, :Child, :Test, :score);\n    compress=:lz4,\n  ),\n)\n\n2794058\n\n\n\n\n\n\n\n\nNote\n\n\n\nA careful examination of the file sizes versus that of ./data/fggk21.arrow will show that the separate tables combined take up more space than the original because of the compression. Compression algorithms are often more successful when applied to larger files.\n\n\nNow read the Arrow tables in and reassemble the original table.\n\nScore = DataFrame(Arrow.Table(\"./data/fggk21_Score.arrow\"))\n\n\n525,126 rows √ó 3 columnsChildTestscoreStringStringFloat641C002352S20_r5.263162C002352BPT3.73C002352SLJ125.04C002352Star_r2.471465C002352Run1053.06C002353S20_r5.07C002353BPT4.18C002353SLJ116.09C002353Star_r1.7677810C002353Run1089.011C002354S20_r4.5454512C002354BPT3.913C002354SLJ111.014C002354Star_r1.9887515C002354Run864.016C002355S20_r4.5454517C002355BPT3.018C002355SLJ114.019C002355Star_r1.8446420C002355Run835.021C002356S20_r4.3478322C002356BPT3.323C002356SLJ118.024C002356Star_r1.9068225C002356Run860.026C002357S20_r4.3478327C002357BPT4.328C002357SLJ130.029C002357Star_r1.9965530C002357Run960.0‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAt this point we can create the z-score column by standardizing the scores for each Test. The code to do this follows Julius‚Äôs presentation on Monday.\n\n@transform!(groupby(Score, :Test), :zScore = @c zscore(:score))\n\n\n525,126 rows √ó 4 columnsChildTestscorezScoreStringStringFloat64Float641C002352S20_r5.263161.79132C002352BPT3.7-0.06223173C002352SLJ125.0-0.03365674C002352Star_r2.471461.468745C002352Run1053.00.3310586C002353S20_r5.01.154717C002353BPT4.10.4983548C002353SLJ116.0-0.4988229C002353Star_r1.76778-0.977310C002353Run1089.00.57405611C002354S20_r4.545450.055148112C002354BPT3.90.21806113C002354SLJ111.0-0.75724814C002354Star_r1.98875-0.20918615C002354Run864.0-0.94468116C002355S20_r4.545450.055148117C002355BPT3.0-1.0432618C002355SLJ114.0-0.60219319C002355Star_r1.84464-0.7101320C002355Run835.0-1.1404321C002356S20_r4.34783-0.42292122C002356BPT3.3-0.62281723C002356SLJ118.0-0.39545224C002356Star_r1.90682-0.49399225C002356Run860.0-0.9716826C002357S20_r4.34783-0.42292127C002357BPT4.30.77864628C002357SLJ130.00.22476929C002357Star_r1.99655-0.18207630C002357Run960.0-0.296686‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nChild = DataFrame(Arrow.Table(\"./data/fggk21_Child.arrow\"))\n\n\n108,295 rows √ó 5 columnsChildSchoolCohortSexageStringStringStringStringFloat641C002352S1000672013male7.994522C002353S1000672013male7.994523C002354S1000672013male7.994524C002355S1001222013female7.994525C002356S1001462013male7.994526C002357S1001462013male7.994527C002358S1001462013male7.994528C002359S1001832013female7.994529C002360S1001952013female7.9945210C002361S1002132013male7.9945211C002362S1002372013female7.9945212C002363S1002372013female7.9945213C002364S1002502013female7.9945214C002365S1003042013male7.9945215C002366S1003042013male7.9945216C002367S1003162013female7.9945217C002368S1003652013male7.9945218C002369S1003652013male7.9945219C002370S1003652013female7.9945220C002371S1004322013female7.9945221C002372S1004322013male7.9945222C002373S1004812013male7.9945223C002374S1004812013male7.9945224C002375S1004812013female7.9945225C002376S1004932013female7.9945226C002377S1004932013female7.9945227C002378S1005472013male7.9945228C002379S1005472013male7.9945229C002380S1005472013male7.9945230C002381S1005472013female7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ndf1 = disallowmissing!(leftjoin(Score, Child; on=:Child))\n\n\n525,126 rows √ó 8 columnsChildTestscorezScoreSchoolCohortSexageStringStringFloat64Float64StringStringStringFloat641C002352S20_r5.263161.7913S1000672013male7.994522C002352BPT3.7-0.0622317S1000672013male7.994523C002352SLJ125.0-0.0336567S1000672013male7.994524C002352Star_r2.471461.46874S1000672013male7.994525C002352Run1053.00.331058S1000672013male7.994526C002353S20_r5.01.15471S1000672013male7.994527C002353BPT4.10.498354S1000672013male7.994528C002353SLJ116.0-0.498822S1000672013male7.994529C002353Star_r1.76778-0.9773S1000672013male7.9945210C002353Run1089.00.574056S1000672013male7.9945211C002354S20_r4.545450.0551481S1000672013male7.9945212C002354BPT3.90.218061S1000672013male7.9945213C002354SLJ111.0-0.757248S1000672013male7.9945214C002354Star_r1.98875-0.209186S1000672013male7.9945215C002354Run864.0-0.944681S1000672013male7.9945216C002355S20_r4.545450.0551481S1001222013female7.9945217C002355BPT3.0-1.04326S1001222013female7.9945218C002355SLJ114.0-0.602193S1001222013female7.9945219C002355Star_r1.84464-0.71013S1001222013female7.9945220C002355Run835.0-1.14043S1001222013female7.9945221C002356S20_r4.34783-0.422921S1001462013male7.9945222C002356BPT3.3-0.622817S1001462013male7.9945223C002356SLJ118.0-0.395452S1001462013male7.9945224C002356Star_r1.90682-0.493992S1001462013male7.9945225C002356Run860.0-0.97168S1001462013male7.9945226C002357S20_r4.34783-0.422921S1001462013male7.9945227C002357BPT4.30.778646S1001462013male7.9945228C002357SLJ130.00.224769S1001462013male7.9945229C002357Star_r1.99655-0.182076S1001462013male7.9945230C002357Run960.0-0.296686S1001462013male7.99452‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to disallowmissing! is because the join will create columns that allow for missing values but we know that we should not get missing values in the result. This call will fail if, for some reason, missing values were created."
  },
  {
    "objectID": "arrow.html#discovering-patterns-in-the-data",
    "href": "arrow.html#discovering-patterns-in-the-data",
    "title": "Notes on saved data files",
    "section": "Discovering patterns in the data",
    "text": "Discovering patterns in the data\nOne of the motivations for creating the Child table was be able to bin the ages according to the age of each child, not the age of each Child-Test combination. Not all children have all 5 test results. We can check the number of results by grouping on :Child and evaluate the number of rows in each group.\n\nnobsChild = combine(groupby(Score, :Child), nrow => :ntest)\n\n\n108,295 rows √ó 2 columnsChildntestStringInt641C00235252C00235353C00235454C00235555C00235656C00235757C00235858C00235949C002360510C002361411C002362512C002363513C002364514C002365515C002366516C002367517C002368518C002369519C002370520C002371421C002372522C002373523C002374524C002375525C002376526C002377527C002378528C002379529C002380530C0023815‚ãÆ‚ãÆ‚ãÆ\n\n\nNow create a table of the number of children with 1, 2, ‚Ä¶, 5 test scores.\n\ncombine(groupby(nobsChild, :ntest), nrow)\n\n\n5 rows √ó 2 columnsntestnrowInt64Int6411462227293317394488365596529\n\n\nA natural question at this point is whether there is something about those students who have few observations. For example, are they from only a few schools?\nOne approach to examining properties like is to add the number of observations for each child to the :Child table. Later we can group the table according to this :ntest to look at properties of :Child by :ntest.\n\ngdf = groupby(\n  disallowmissing!(leftjoin(Child, nobsChild; on=:Child)), :ntest\n)\n\n\nGroupedDataFrame with 5 groups based on key: ntestFirst Group (462 rows): ntest = 1ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002452S1011752013male7.9945212C002625S1033292013male7.9945213C002754S1048142013female7.9945214C003269S1022582012female7.9972615C003599S1058432012female7.9972616C003807S1007542011male8.017C003985S1029452011male8.018C004086S1042552011male8.019C004657S1014002014male8.03833110C005036S1059092014male8.03833111C005440S1010232019male8.05202112C005523S1018252019female8.05202113C005697S1036152019male8.05202114C005759S1046322019female8.05202115C005810S1049542019female8.05202116C005835S1050532019male8.05202117C005854S1054052019male8.05202118C006550S1033292013male8.0794119C006760S1051812013female8.0794120C007031S1132442013male8.0794121C007050S1001952012female8.08214122C007305S1023502012male8.08214123C007828S1114052012female8.08214124C008698S1049172016female8.09309125C008707S1022712016male8.09582126C009596S1034212014female8.1232127C009651S1037062014female8.1232128C009879S1059092014female8.1232129C010203S1026602016male8.12594130C010204S1026602016male8.125941‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆLast Group (96529 rows): ntest = 5ChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002360S1001952013female7.9945259C002362S1002372013female7.99452510C002363S1002372013female7.99452511C002364S1002502013female7.99452512C002365S1003042013male7.99452513C002366S1003042013male7.99452514C002367S1003162013female7.99452515C002368S1003652013male7.99452516C002369S1003652013male7.99452517C002370S1003652013female7.99452518C002372S1004322013male7.99452519C002373S1004812013male7.99452520C002374S1004812013male7.99452521C002375S1004812013female7.99452522C002376S1004932013female7.99452523C002377S1004932013female7.99452524C002378S1005472013male7.99452525C002379S1005472013male7.99452526C002380S1005472013male7.99452527C002381S1005472013female7.99452528C002382S1005472013female7.99452529C002383S1005842013female7.99452530C002384S1005962013male7.994525‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAre the sexes represented more-or-less equally?\n\ncombine(groupby(first(gdf), :Sex), nrow => :nchild)\n\n\n2 rows √ó 2 columnsSexnchildStringInt641male2302female232\n\n\n\ncombine(groupby(last(gdf), :Sex), nrow => :nchild)\n\n\n2 rows √ó 2 columnsSexnchildStringInt641male475522female48977\n\n\nWhat about the distribution of ages?\n\n\"\"\"\n    ridgeplot!(ax::Axis, df::AbstractDataFrame, densvar::Symbol, group::Symbol; normalize=false)\n    ridgeplot!(f::Figure, args...; pos=(1,1) kwargs...)\n    ridgeplot(args...; kwargs...)\nCreate a \"ridge plot\".\nA ridge plot is stacked plot of densities for a given variable (`densvar`) grouped by a different variable (`group`). Because densities can very widely in scale, it is sometimes useful to `normalize` the densities so that each density has a maximum of 1.\nThe non-mutating method creates a Figure before calling the method for Figure.\nThe method for Figure places the ridge plot in the grid position specified by `pos`, default is (1,1).\n\"\"\"\nfunction ridgeplot!(\n  ax::Axis,\n  df::AbstractDataFrame,\n  densvar::Symbol,\n  group::Symbol;\n  normalize=false,\n)\n  # `normalize` makes it so that the max density is always 1\n  # `normalize` works on the density not the area/mass\n  gdf = groupby(df, group)\n  dens = combine(gdf, densvar => kde => :kde)\n  sort!(dens, group)\n  spacing = normalize ? 1.0 : 0.9 * maximum(dens[!, :kde]) do val\n    return maximum(val.density)\n  end\n\n  nticks = length(gdf)\n\n  for (idx, row) in enumerate(eachrow(dens))\n    dd = if normalize\n      row.kde.density ./ maximum(row.kde.density)\n    else\n      row.kde.density\n    end\n\n    offset = idx * spacing\n\n    lower = Node(Point2f.(row.kde.x, offset))\n    upper = Node(Point2f.(row.kde.x, dd .+ offset))\n    band!(ax, lower, upper; color=(:black, 0.3))\n    lines!(ax, upper; color=(:black, 1.0))\n  end\n\n  ax.yticks[] = (\n    1:spacing:(nticks * spacing), string.(dens[!, group])\n  )\n  ylims!(ax, 0, (nticks + 2) * spacing)\n  ax.xlabel[] = string(densvar)\n  ax.ylabel[] = string(group)\n\n  return ax\nend\n\n\nfunction ridgeplot!(f::Figure, args...; pos=(1, 1), kwargs...)\n  ridgeplot!(Axis(f[pos...]), args...; kwargs...)\n  return f\nend\n\n\n\"\"\"\n    ridgeplot(args...; kwargs...)\nSee [ridgeplot!](@ref).\n\"\"\"\nfunction ridgeplot(args...; kwargs...)\n  return ridgeplot!(Figure(), args...; kwargs...)\nend\n\n\nridgeplot(parent(gdf), :age, :ntest)\n\n\nparent(gdf)\n\n\n108,295 rows √ó 6 columnsChildSchoolCohortSexagentestStringStringStringStringFloat64Int641C002352S1000672013male7.9945252C002353S1000672013male7.9945253C002354S1000672013male7.9945254C002355S1001222013female7.9945255C002356S1001462013male7.9945256C002357S1001462013male7.9945257C002358S1001462013male7.9945258C002359S1001832013female7.9945249C002360S1001952013female7.99452510C002361S1002132013male7.99452411C002362S1002372013female7.99452512C002363S1002372013female7.99452513C002364S1002502013female7.99452514C002365S1003042013male7.99452515C002366S1003042013male7.99452516C002367S1003162013female7.99452517C002368S1003652013male7.99452518C002369S1003652013male7.99452519C002370S1003652013female7.99452520C002371S1004322013female7.99452421C002372S1004322013male7.99452522C002373S1004812013male7.99452523C002374S1004812013male7.99452524C002375S1004812013female7.99452525C002376S1004932013female7.99452526C002377S1004932013female7.99452527C002378S1005472013male7.99452528C002379S1005472013male7.99452529C002380S1005472013male7.99452530C002381S1005472013female7.994525‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ"
  },
  {
    "objectID": "arrow.html#reading-arrow-files-in-other-languages",
    "href": "arrow.html#reading-arrow-files-in-other-languages",
    "title": "Notes on saved data files",
    "section": "Reading Arrow files in other languages",
    "text": "Reading Arrow files in other languages\nThere are Arrow implementations for R (the arrow package) and for Python (pyarrow).\n#| eval: false\nimport pyarrow.feather: read_table\nread_table(\"./data/fggk21.arrow\")\n#| eval: false\nlibrary(\"arrow\")\nfggk21 <- read_feather(\"./data/fggk21.arrow\")\nnrow(fggk21)"
  },
  {
    "objectID": "glmm.html",
    "href": "glmm.html",
    "title": "Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used"
  },
  {
    "objectID": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "Generalized linear mixed models",
    "section": "Matrix notation for the sleepstudy model",
    "text": "Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n\n180 rows √ó 3 columnssubjdaysreactionStringInt8Float641S3080249.562S3081258.7053S3082250.8014S3083321.445S3084356.8526S3085414.697S3086382.2048S3087290.1499S3088430.58510S3089466.35311S3090222.73412S3091205.26613S3092202.97814S3093204.70715S3094207.71616S3095215.96217S3096213.6318S3097217.72719S3098224.29620S3099237.31421S3100199.05422S3101194.33223S3102234.3224S3103232.84225S3104229.30726S3105220.45827S3106235.42128S3107255.75129S3108261.01230S3109247.515‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nMinimizing 57    Time: 0:00:00 ( 7.35 ms/it)\n\n\nLinear mixed model fit by maximum likelihood\n \n\n\nreaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n\n\n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n\n\n\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51068 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                Coef.  Std. Error      z  Pr(>|z|)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, Œ≤, has 2 elements and the fixed-effects model matrix, X, is of size 180 √ó 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ‚ãÆ\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.Œ≤\n\n2-element Vector{Float64}:\n 251.40510484848417\n  10.467285959595715\n\n\n\nm1.X\n\n180√ó2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ‚ãÆ    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1‚Äôs.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The ‚Äúestimates‚Äù (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158188821572576 -40.04844169634357 ‚Ä¶ 0.7232620708173191 12.118907835512482; 9.075511758123813 -8.644079444480187 ‚Ä¶ -0.9710526399373302 1.310698060178722]\n\n\n\nfirst(m1.b)   # only one grouping factor\n\n2√ó18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   ‚Ä¶  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, Z, for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(first(m1.reterms))\n\n180√ó36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ‚ãÆ              ‚ãÆ              ‚ãÆ        ‚ã±     ‚ãÆ              ‚ãÆ              ‚ãÆ\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  ‚Ä¶  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.Œ≤ + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725613\n 215.00307618237164\n   ‚ãÆ\n 328.09823347656857\n 337.5944667962269\n 263.52401268399666\n 275.3019967037711\n 287.07998072354553\n 298.85796474331994\n 310.6359487630944\n 322.41393278286887\n 334.1919168026433\n 345.9699008224177\n 357.74788484219215\n 369.5258688619666\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22092373064143\n 273.76372144836097\n 293.3065191660805\n 312.8493168838\n 332.39211460151955\n 351.93491231923906\n 371.47771003695857\n 391.0205077546781\n 410.5633054723977\n 430.1061031901172\n 211.35666315214058\n 213.17986966725616\n 215.00307618237167\n   ‚ãÆ\n 328.09823347656857\n 337.594466796227\n 263.52401268399666\n 275.30199670377107\n 287.0799807235455\n 298.85796474331994\n 310.6359487630944\n 322.4139327828688\n 334.1919168026432\n 345.9699008224177\n 357.74788484219215\n 369.52586886196656\n\n\nIn symbols we would write the linear predictor expression as \\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\] where \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 √ó 2 and \\(\\bf Z\\) is of size 180 √ó 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 √ó 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 √ó 2 and all the same.\nRecall that this symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.Œ∏\n\n3-element Vector{Float64}:\n 0.9292213219958606\n 0.018168376087695465\n 0.22264487473562505\n\n\n\nŒª = only(m1.Œª)  # with multiple grouping factors there will be multiple Œª's\n\n2√ó2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ‚ãÖ \n 0.0181684  0.222645\n\n\n\nŒ£ = varest(m1) * (Œª * Œª')\n\n2√ó2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\n\nVarCorr(m1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51068\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "Generalized linear mixed models",
    "section": "Linear predictors in LMMs and GLMMs",
    "text": "Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as \\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\] may seem like over-mathematization (or ‚Äúoverkill‚Äù, if you prefer) relative to expressions like \\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\] but this more abstract form is necessary for generalizations.\nThe way that I read the first form is :::{.callout} The conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\nSo the only things that differ in the distributions of the \\(y_i\\)‚Äôs are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "glmm.html#generalized-linear-mixed-models",
    "href": "glmm.html#generalized-linear-mixed-models",
    "title": "Generalized linear mixed models",
    "section": "Generalized Linear Mixed Models",
    "text": "Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e.¬†yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i‚Äôth response is again determined by the i‚Äôth element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of ‚Äúsuccess‚Äù for the i‚Äôth response and must be between 0 and 1. We can‚Äôt guarantee that the i‚Äôth component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren‚Äôt as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function, \\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\] (it‚Äôs called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic \\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\] This is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(Œ∑) = inv(increment(exp(-Œ∑)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is \\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\] where \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don‚Äôt have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet ‚ÄúBeauty is truth, truth beauty - that is all ye know on earth and all ye need to know.‚Äù)\n\nAn example of a Bernoulli GLMM\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n\n1,934 rows √ó 5 columnsdisturbanlivchageuseStringStringStringFloat64String1D01Y3+18.44N2D01Y0-5.56N3D01Y21.44N4D01Y3+8.44N5D01Y0-13.56N6D01Y0-11.56N7D01Y3+18.44N8D01Y3+-3.56N9D01Y1-5.56N10D01Y3+1.44N11D01Y0-11.56Y12D01Y0-2.56N13D01Y1-4.56N14D01Y3+5.44N15D01Y3+-0.56N16D01Y3+4.44Y17D01Y0-5.56N18D01Y3+-0.56Y19D01Y1-6.56Y20D01Y2-3.56N21D01Y0-4.56N22D01Y0-9.56N23D01Y3+2.44N24D01Y22.44Y25D01Y1-4.56Y26D01Y3+14.44N27D01Y0-6.56Y28D01Y1-3.56Y29D01Y1-5.56Y30D01Y1-1.56Y‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n\n60 rows √ó 2 columnsdistnrowStringInt641D011172D02203D0324D04305D05396D06657D07188D08379D092310D101311D112112D122913D132414D1411815D152216D162017D172418D184719D192620D201521D211822D222023D231524D241425D256726D261327D274428D284929D293230D3061‚ãÆ‚ãÆ‚ãÆ\n\n\nThe information recorded included woman‚Äôs age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, drawn using lattice graphics in R, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20‚Äôs to early 30‚Äôs) and low again for older women (late 30‚Äôs to early 40‚Äôs in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn‚Äôt mean that there is no ‚Äúage effect‚Äù, it only means that there is no significant linear effect for age.\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist => Grouping(),\n  :urban => HelmertCoding(),\n  :children => HelmertCoding(),\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 186   Time: 0:00:00 ( 2.41 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n<1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7021\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n<1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n<1e-08\n\n\n\nlivch: 1\n0.8151\n0.1622\n5.02\n<1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n<1e-06\n\n\n\nlivch: 3+\n0.9153\n0.1858\n4.93\n<1e-06\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant. We usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229100\n0.478644\n\n\n\n\n\nNotice that there is no ‚Äúresidual‚Äù variance being estimated. This is because the Bernoulli distribution doesn‚Äôt have a scale parameter.\n\n\nConvert livch to a binary factor\n\n@transform!(contra, :children = :livch ‚â† \"0\")\n\n\n1,934 rows √ó 6 columnsdisturbanlivchageusechildrenStringStringStringFloat64StringBool1D01Y3+18.44N12D01Y0-5.56N03D01Y21.44N14D01Y3+8.44N15D01Y0-13.56N06D01Y0-11.56N07D01Y3+18.44N18D01Y3+-3.56N19D01Y1-5.56N110D01Y3+1.44N111D01Y0-11.56Y012D01Y0-2.56N013D01Y1-4.56N114D01Y3+5.44N115D01Y3+-0.56N116D01Y3+4.44Y117D01Y0-5.56N018D01Y3+-0.56Y119D01Y1-6.56Y120D01Y2-3.56N121D01Y0-4.56N022D01Y0-9.56N023D01Y3+2.44N124D01Y22.44Y125D01Y1-4.56Y126D01Y3+14.44N127D01Y0-6.56Y028D01Y1-3.56Y129D01Y1-5.56Y130D01Y1-1.56Y1‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 138   Time: 0:00:00 ( 2.04 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows √ó 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm272364.922379.182418.152379.242gm182372.462388.732433.272388.81\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children. There is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\nAt present the calculation of the geomdof as sum(influence(m)) is not correctly defined in our code for a GLMM so we need to do some more work before we can examine those values.\n\n\nUsing urban&dist as a grouping factor\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 143   Time: 0:00:00 ( 1.13 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm372353.822368.482407.462368.542gm272364.922379.182418.152379.243gm182372.462388.732433.272388.81\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. This reinforces the idea that a simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6065\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age¬≤ (written abs2(age)) in the model and we have a significant interaction age & children in the model.\n\n\nPredictions for some subgroups\nFor a ‚Äútypical‚Äù district (random effect near zero) the predictions on the linear predictor scale for a woman whose age is near the centering value (i.e.¬†centered age of zero) are:\n\nusing Effects\ndesign = Dict(\n  :children => [true, false], :urban => [\"Y\", \"N\"], :age => [0.0]\n)\npreds = effects(design, gm3)\n\n‚îå Info: Precompiling Effects [8f03c58b-bd97-4933-a826-f71b64d2cca2]\n‚îî @ Base loading.jl:1662\n\n\n\n4 rows √ó 7 columnschildrenageurbanuse: YerrlowerupperBoolFloat64StringFloat64Float64Float64Float64110.0Y0.6585760.150530.5080460.809106200.0Y-0.5543250.230477-0.784802-0.323848310.0N-0.1286160.113017-0.241633-0.0155985400.0N-1.341520.221575-1.56309-1.11994\n\n\nConverting these Œ∑ values to probabilities yields\n\nlogistic.(preds[!, \"use: Y\"])\n\n4-element Vector{Float64}:\n 0.6589404445793394\n 0.3648614994485519\n 0.46789030652047003\n 0.2072606855141492"
  },
  {
    "objectID": "glmm.html#summarizing-the-results",
    "href": "glmm.html#summarizing-the-results",
    "title": "Generalized linear mixed models",
    "section": "Summarizing the results",
    "text": "Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  },
  {
    "objectID": "kb07.html",
    "href": "kb07.html",
    "title": "Bootstrapping a fitted model",
    "section": "",
    "text": "Begin by loading the packages to be used.\nProvide a short alias for AlgebraOfGraphics."
  },
  {
    "objectID": "kb07.html#data-set-and-model",
    "href": "kb07.html#data-set-and-model",
    "title": "Bootstrapping a fitted model",
    "section": "Data set and model",
    "text": "Data set and model\nThe kb07 data (Kronm√ºller & Barr, 2007) are one of the datasets provided by the MixedModels package.\n\nkb07 = MixedModels.dataset(:kb07)\n\nArrow.Table with 1789 rows, 7 columns, and schema:\n :subj      String\n :item      String\n :spkr      String\n :prec      String\n :load      String\n :rt_trunc  Int16\n :rt_raw    Int16\n\n\nConvert the table to a DataFrame for summary.\n\nkb07 = DataFrame(kb07)\ndescribe(kb07)\n\n\n7 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1subjS030S1030String2itemI01I320String3spkrnewold0String4precbreakmaintain0String5loadnoyes0String6rt_trunc2182.25791940.051710Int167rt_raw2226.245791940.0159230Int16\n\n\nThe experimental factors; spkr, prec, and load, are two-level factors. The EffectsCoding contrast is used with these to create a \\(\\pm1\\) encoding. Furthermore, Grouping constrasts are assigned to the subj and item factors. This is not a contrast per-se but an indication that these factors will be used as grouping factors for random effects and, therefore, there is no need to create a contrast matrix. For large numbers of levels in a grouping factor, an attempt to create a contrast matrix may cause memory overflow.\nIt is not important in these cases but a good practice in any case.\n\ncontrasts = merge(\n  Dict(nm => EffectsCoding() for nm in (:spkr, :prec, :load)),\n  Dict(nm => Grouping() for nm in (:subj, :item)),\n);\n\nThe display of an initial model fit\n\nkbm01 = let\n  form = @formula(\n    rt_trunc ~\n      1 +\n      spkr * prec * load +\n      (1 + spkr + prec + load | subj) +\n      (1 + spkr + prec + load | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 722   Time: 0:00:01 ( 2.41 ms/it)\n  objective:  28637.1393507629\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_subj\nœÉ_item\n\n\n\n\n(Intercept)\n2181.6750\n77.3042\n28.22\n<1e-99\n301.8457\n362.1769\n\n\nspkr: old\n67.7490\n18.2846\n3.71\n0.0002\n43.0723\n40.5401\n\n\nprec: maintain\n-333.9206\n47.1549\n-7.08\n<1e-11\n62.1055\n246.8926\n\n\nload: yes\n78.7681\n19.5218\n4.03\n<1e-04\n65.1378\n42.1405\n\n\nspkr: old & prec: maintain\n-21.9634\n15.8063\n-1.39\n0.1647\n\n\n\n\nspkr: old & load: yes\n18.3838\n15.8063\n1.16\n0.2448\n\n\n\n\nprec: maintain & load: yes\n4.5334\n15.8063\n0.29\n0.7743\n\n\n\n\nspkr: old & prec: maintain & load: yes\n23.6051\n15.8063\n1.49\n0.1353\n\n\n\n\nResidual\n668.5074\n\n\n\n\n\n\n\n\n\n\ndoes not include the estimated correlations of the random effects.\nThe VarCorr extractor displays these.\n\nVarCorr(kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nsubj\n(Intercept)\n91110.8185\n301.8457\n\n\n\n\n\n\nspkr: old\n1855.2224\n43.0723\n+0.78\n\n\n\n\n\nprec: maintain\n3857.0961\n62.1055\n-0.59\n+0.02\n\n\n\n\nload: yes\n4242.9381\n65.1378\n+0.36\n+0.82\n+0.53\n\n\nitem\n(Intercept)\n131172.1058\n362.1769\n\n\n\n\n\n\nspkr: old\n1643.4957\n40.5401\n+0.44\n\n\n\n\n\nprec: maintain\n60955.9375\n246.8926\n-0.69\n+0.35\n\n\n\n\nload: yes\n1775.8252\n42.1405\n+0.32\n+0.23\n-0.15\n\n\nResidual\n\n446902.1445\n668.5074\n\n\n\n\n\n\n\n\nNone of the two-factor or three-factor interaction terms in the fixed-effects are significant. In the random-effects terms only the scalar random effects and the prec random effect for item appear to be warranted, leading to the reduced formula\n\nkbm02 = let\n  form = @formula(\n    rt_trunc ~\n      1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 93    Time: 0:00:00 ( 1.20 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(kbm02)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.240\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\nThese two models are nested and can be compared with a likelihood-ratio test.\n\nMixedModels.likelihoodratiotest(kbm02, kbm01)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + (1 | subj) + (1 + prec | item)\n9\n28664\n\n\n\n\n\nrt_trunc ~ 1 + spkr + prec + load + spkr & prec + spkr & load + prec & load + spkr & prec & load + (1 + spkr + prec + load | subj) + (1 + spkr + prec + load | item)\n29\n28637\n27\n20\n0.1436\n\n\n\n\n\nThe p-value of approximately 14% leads us to prefer the simpler model, kbm02, to the more complex, kbm01."
  },
  {
    "objectID": "kb07.html#a-bootstrap-sample",
    "href": "kb07.html#a-bootstrap-sample",
    "title": "Bootstrapping a fitted model",
    "section": "A bootstrap sample",
    "text": "A bootstrap sample\nCreate a bootstrap sample of a few thousand parameter estimates from the reduced model. The pseudo-random number generator is initialized to a fixed value for reproducibility.\n\nRandom.seed!(1234321)\nhide_progress = true\nkbm02samp = parametricbootstrap(2000, kbm02; hide_progress);\n\nOne of the uses of such a sample is to form ‚Äúconfidence intervals‚Äù on the parameters by obtaining the shortest interval that covers a given proportion (95%, by default) of the sample.\n\nDataFrame(shortestcovint(kbm02samp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2028.012337.922Œ≤missingspkr: old38.43199.59443Œ≤missingprec: maintain-439.321-245.8644Œ≤missingload: yes46.0262107.5115œÉitem(Intercept)261.196448.516œÉitemprec: maintain175.489312.0387œÅitem(Intercept), prec: maintain-0.89799-0.4455958œÉsubj(Intercept)228.099357.7899œÉresidualmissing655.249701.497\n\n\nA sample like this can be used for more than just creating an interval because it approximates the distribution of the estimator. For the fixed-effects parameters the estimators are close to being normally distributed, Figure¬†1.\n\n\nCode\ndraw(\n  data(kbm02samp.Œ≤) * mapping(:Œ≤; color=:coefname) * AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†1: Comparative densities of the fixed-effects coefficients in kbm02samp\n\n\n\n\n\n\nCode\ndraw(\n  data(\n    filter(\n      :column => ==(Symbol(\"(Intercept)\")), DataFrame(kbm02samp.œÉs)\n    ),\n  ) *\n  mapping(:œÉ; color=:group) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†2: Density plot of bootstrap samples standard deviation of random effects\n\n\n\n\n\n\nCode\ndraw(\n  data(filter(:type => ==(\"œÅ\"), DataFrame(kbm02samp.allpars))) *\n  mapping(:value => \"Correlation\"; color=:names) *\n  AOG.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†3: Density plot of correlation parameters in bootstrap sample from model kbm02"
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "",
    "text": "We take the kwdyz11.arrow dataset (Kliegl et al., 2010) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15.arrow from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl."
  },
  {
    "objectID": "kwdyz11.html#packages",
    "href": "kwdyz11.html#packages",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Packages",
    "text": "Packages\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\nusing Random\nusing StatsBase\nusing Statistics\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "href": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Read data, compute and plot densities and means",
    "text": "Read data, compute and plot densities and means\n\n\nCode\ndat = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 2, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt,\n    :rt => (x -> log.(x)) => :lrt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS01S610CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3rt370.426150.1358.6705.70Float644lrt5.8865.01135.882216.559190Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results and exhibited the same problems of model identification (see Kliegl et al., 2010).\nComparative density plots of all response times by cue-target relation, Figure¬†1, show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density(),\n)\n\n\n\n\n\nFigure¬†1: Comparative density plots of log reaction time for different cue-target relations.\n\n\n\n\nAn alternative visualization without overlap of the conditions can be accomplished with ridge plots.\nTo be done\nFor the next set of plots we average subjects‚Äô data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\n\n\n244 rows √ó 5 columnsSubjCTRnrt_mlrt_mCat‚Ä¶Cat‚Ä¶Int64Float64Float641S01val330413.3326.012722S01sod48437.7216.06823S01dos47443.3666.082694S01dod45434.3166.060795S02val333365.8995.87516S02sod47396.1495.959167S02dos46439.4876.069498S02dod48441.0426.068839S03val336371.4465.9048910S03sod46446.8546.0946411S03dos48471.3026.1428712S03dod47476.5326.1570613S04val336403.1815.9912414S04sod48446.0756.0949915S04dos48458.3046.1200516S04dod48441.0546.0820517S05val310358.7145.8411718S05sod44409.155.9493419S05dos45457.4936.1005320S05dod45472.1386.1336121S06val334362.8645.8795322S06sod48368.1235.8961723S06dos48383.0945.9360124S06dod48367.9735.8897925S07val326407.4975.9844726S07sod48459.06.1092727S07dos42486.5026.1772928S07dod47457.6176.1047129S08val333308.1025.7181530S08sod48323.8755.76388‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\n\n\nFigure¬†2: Comparative boxplots of log response time by cue-target relation.\n\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nA better alternative to the boxplot is a dotplot. It also displays subjects‚Äô condition means.\nTo be done"
  },
  {
    "objectID": "kwdyz11.html#linear-mixed-model",
    "href": "kwdyz11.html#linear-mixed-model",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Linear mixed model",
    "text": "Linear mixed model\n\ncontrasts = Dict(\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n  :Subj => Grouping(),\n)\nm1 = let\n  form = @formula(lrt ~ 1 + CTR + (1 + CTR | Subj))\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 318   Time: 0:00:00 ( 1.82 ms/it)\n  objective:  -12782.373740637588\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n5.9358\n0.0185\n320.53\n<1e-99\n0.1441\n\n\nCTR: sod\n0.0878\n0.0084\n10.48\n<1e-24\n0.0582\n\n\nCTR: dos\n0.0366\n0.0062\n5.92\n<1e-08\n0.0274\n\n\nCTR: dod\n-0.0086\n0.0060\n-1.43\n0.1515\n0.0249\n\n\nResidual\n0.1920\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.0207652\n0.1441013\n\n\n\n\n\n\nCTR: sod\n0.0033847\n0.0581778\n+0.48\n\n\n\n\n\nCTR: dos\n0.0007530\n0.0274414\n-0.24\n-0.15\n\n\n\n\nCTR: dod\n0.0006221\n0.0249410\n+0.30\n+0.93\n-0.43\n\n\nResidual\n\n0.0368543\n0.1919748\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nLMM m1 is not fully supported by the data; it is overparameterized. This is also visible in the PCA: only three, not four PCS are needed to account for all the variance and covariance in the random-effect structure. The problem is the +.93 CP for spatial sod and attraction dod effects.\n\nfirst(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.48   1.0     .      .\n CTR: dos     -0.24  -0.15   1.0     .\n CTR: dod      0.3    0.93  -0.43   1.0\n\nNormalized cumulative variances:\n[0.5886, 0.8095, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2    PC3    PC4\n (Intercept)  -0.4   0.04   0.9    0.17\n CTR: sod     -0.6   0.4   -0.16  -0.68\n CTR: dos      0.33  0.91   0.06   0.23\n CTR: dod     -0.61  0.08  -0.41   0.68"
  },
  {
    "objectID": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "href": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Diagnostic plots of LMM residuals",
    "text": "Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\nResidual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f => \"Fitted values\", :r => \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\nFigure¬†3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted log response time\", :r => \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†4: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n\nQ-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)\n\n\n\n\n\n\nObserved and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure¬†5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value => \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure¬†5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian"
  },
  {
    "objectID": "kwdyz11.html#conditional-modes",
    "href": "kwdyz11.html#conditional-modes",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Conditional modes",
    "text": "Conditional modes\nNow we move on to visualizations that are based on model parameters and subjects‚Äô data, that is ‚Äúpredictions‚Äù of the LMM for subject‚Äôs GM and experimental effects. Three important plots are\n\nOverlay\nCaterpillar\nShrinkage\n\n\nOverlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done\n\n\nCaterpillar plot\nThe caterpillar plot, Figure¬†6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n\n\nFigure¬†6: Prediction intervals on the random effects for Subj in model m1\n\n\n\n\n\n\nShrinkage plot\nFigure¬†7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n\n\nFigure¬†7: Shrinkage plot of the conditional means of the random effects for model m1"
  },
  {
    "objectID": "kwdyz11.html#parametric-bootstrap",
    "href": "kwdyz11.html#parametric-bootstrap",
    "title": "RePsychLing Kliegl et al.¬†(2010)",
    "section": "Parametric bootstrap",
    "text": "Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\nGenerate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1; hide_progress=true)\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n\n10 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)5.9339221Œ≤missingCTR: sod0.086421731Œ≤missingCTR: dos0.048889141Œ≤missingCTR: dod-0.012183551œÉSubj(Intercept)0.13293761œÉSubjCTR: sod0.049735871œÅSubj(Intercept), CTR: sod0.60494281œÉSubjCTR: dos0.027881991œÅSubj(Intercept), CTR: dos-0.254336101œÅSubjCTR: sod, CTR: dos0.0550768\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters\n\n37500\n\n\n\n\nShortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nDataFrame(shortestcovint(samp))\n\n\n\n15 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)5.899175.972562Œ≤missingCTR: sod0.07193110.1045883Œ≤missingCTR: dos0.02511790.04916734Œ≤missingCTR: dod-0.02071780.002683055œÉSubj(Intercept)0.1165650.168576œÉSubjCTR: sod0.04551810.07080337œÅSubj(Intercept), CTR: sod0.2437220.7129558œÉSubjCTR: dos0.009642950.04088249œÅSubj(Intercept), CTR: dos-0.9211810.24702610œÅSubjCTR: sod, CTR: dos-0.7247280.47764611œÉSubjCTR: dod0.01441330.038505712œÅSubj(Intercept), CTR: dod-0.132490.74037913œÅSubjCTR: sod, CTR: dod0.5763690.99999514œÅSubjCTR: dos, CTR: dod-0.8856910.43604115œÉresidualmissing0.1905590.193644\n\n\n\n\nComparative density plots of bootstrapped parameter estimates\n\nResidual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\", ismissing(:names))) *\n  mapping(:value => \"Residual standard deviation\") *\n  density();\n)\n\n\n\n\n\nFigure¬†8: ?(caption)\n\n\n\n\n\n\nFixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not inlcude its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept)\" => \"grand mean\",\n]\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => renamer(labels) => \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nVariance components (VCs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => renamer(labels) => \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†10: Comparative density plots of the variance components for model m1\n\n\n\n\nThe VC are all very nicely defined.\n\n\nCorrelation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" => \"GM, spatial\",\n    \"(Intercept), CTR: dos\" => \"GM, object\",\n    \"CTR: sod, CTR: dos\" => \"spatial, object\",\n    \"(Intercept), CTR: dod\" => \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" => \"object, attraction\",\n  ]\n  draw(\n    data(@subset(dat2, :type == \"œÅ\")) *\n    mapping(\n      :value => \"Correlation\";\n      color=:names => renamer(labels) => \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\nFigure¬†11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015)."
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2010) (see also script mmt_kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visiable in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2010). To keep matters somewhat simpler and comparable we ignore them in this script.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2010), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2010), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions. Therefore, the code in this script is largely the same as the one in kwdyz.jl.\nThere will be another vignette modelling the additional experimental manipulations of target size and orientation of cue rectangle. This analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); they were also used in a paper of GAMMs (Baayen et al., 2017). Data and R scripts are also available in R-package RePsychLing. Here we provide some of the corresponding analyses with MixedModels.jl and a much wider variety of visualizations of LMM results. A MixedModels.jl-based analysis focusing on the complex experimental design and the analysis of a complex random-effect structure for this design is in script kkl15_complex.jl."
  },
  {
    "objectID": "kkl15.html#packages",
    "href": "kkl15.html#packages",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Packages",
    "text": "Packages\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nusing Random\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\nusing MixedModelsMakie: qqnorm\nusing MixedModelsMakie: ridgeplot\nusing MixedModelsMakie: scatter\nconst datadir = joinpath(@__DIR__, \"data\")\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"png\")"
  },
  {
    "objectID": "kkl15.html#read-data-compute-and-plot-means",
    "href": "kkl15.html#read-data-compute-and-plot-means",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Read data, compute and plot means",
    "text": "Read data, compute and plot means\n\ndat = @chain \"kkl15.arrow\" begin\n  joinpath(datadir, _)\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 3, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt => (x -> log.(x)) => :lrt,\n    :rt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n\n4 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3lrt5.644245.01215.622556.619380Float644rt293.147150.22276.594749.4810Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results.\nComparative density plots of all response times by cue-target relation show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†1: Compartive density plots of log response time by condition\n\n\n\n\nBoxplots of the mean of log response time by subject under the different conditions show an outlier value under three of the four conditions; they are from the same subject.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\ndescribe(dat_subj)\n\n\n5 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3n156.2944964.04480Int644rt_m308.223208.194304.862584.710Float645lrt_m5.69085.332265.698486.361410Float64\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\n\n\nFigure¬†2: Comparative boxplots of mean response by subject under different conditions\n\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nWe remove the outlier subject and replot, but we model the data points in dat and check whether this subject appears as an outlier in the caterpillar plot of conditional modes.\n\n\nCode\nlet\n  dat_subj2 = @subset(dat_subj, :rt_m < 510)\n  boxplot(\n    dat_subj2.CTR.refs,\n    dat_subj2.lrt_m;\n    orientation=:horizontal,\n    show_notch=true,\n    axis=(;\n      yticks=(\n        1:4,\n        [\n          \"valid cue\",\n          \"same obj/diff pos\",\n          \"diff obj/same pos\",\n          \"diff obj/diff pos\",\n        ],\n      ),\n    ),\n    figure=(; resolution=(800, 300)),\n  )\nend\n\n\n\n\n\nFigure¬†3: Comparative boxplots of mean response by subject under different conditions without outlier\n\n\n\n\nA better alternative to the boxplot is often a dotplot, because it also displays subjects‚Äô condition means.\nTo be done\nFor the next set of plots we average subjects‚Äô data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_cond = combine(\n  groupby(dat_subj, :CTR),\n  :n => length => :N,\n  :lrt_m => mean => :lrt_M,\n  :lrt_m => std => :lrt_SD,\n  :lrt_m => (x -> std(x) / sqrt(length(x))) => :lrt_SE,\n)\n\n\n4 rows √ó 5 columnsCTRNlrt_Mlrt_SDlrt_SECat‚Ä¶Int64Float64Float64Float641val865.614430.158050.01704292sod865.688360.1913950.02063863dos865.729430.1910270.02059894dod865.730990.216280.023322\n\n\nWe can also look at correlations plots based on the four condition means. There are actually two correlation matrices which have correspondences in alternative parameterizatios of the LMM random-effect structure. One matrix is based on the four measures. If you think of the four measures as test scores, this matrix is the usual correlation matrix. The second matrix contains correlations between the Grand Mean (GM) and the three effects defined with the contrasts for the four levels of the condition factor in the next chunk.\nTo this end, we\n\nuse the unstack() command to convert data from long to wide format,\ncompute the GM and the three experimental effects.\nplot the correlation matrix for four measures/scores, and\nplot the correlation matrix for GM and three effects\n\n\ndat_subj_w = @chain dat_subj begin\n  unstack(:Subj, :CTR, :rt_m)\n  disallowmissing!\n  @transform(\n    :GM = (:val + :sod + :dos + :dod) ./ 4,\n    :spatial = :sod - :val,\n    :object = :dos - :sod,\n    :attraction = :dod - :dos,\n  )\nend\ndescribe(dat_subj_w)\n\n\n9 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2val283.688216.35286.376513.530Float643sod306.75213.444305.092562.0150Float644dos319.896215.787317.527584.710Float645dod322.561208.194315.511555.2060Float646GM308.223217.819309.381553.8650Float647spatial23.0621-47.203620.227287.78730Float648object13.1456-13.827311.790361.04240Float649attraction2.66497-43.8703-1.1188763.53470Float64\n\n\n\n#@df dat_subj_w StatsPlots.corrplot(cols(2:5), grid = false, compact=false)\n\n\n#@df dat_subj_w StatsPlots.corrplot(cols(6:9), grid = false, compact=false)\n\n\n\n\n\n\n\nNote\n\n\n\nTwo of the theoreticsally irrelevant within-subject effect correlations have a different sign than the corresponding, non-significant CPs in the LMM; they are negative here, numerically positive in the LMM. This occurs only very rarely in the case of ecological correlations. However, as they are not significant according to shortest coverage interval, it may not be that relevant either. It is the case both for effects based on log-transformed and raw reaction times."
  },
  {
    "objectID": "kkl15.html#linear-mixed-model",
    "href": "kkl15.html#linear-mixed-model",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Linear mixed model",
    "text": "Linear mixed model\n\ncontrasts = Dict(\n  :Subj => Grouping(),\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n)\nm1 = let\n  form = @formula lrt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 231   Time: 0:00:00 ( 2.07 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n5.6907\n0.0199\n286.42\n<1e-99\n0.1839\n\n\nCTR: sod\n0.0740\n0.0080\n9.30\n<1e-19\n0.0688\n\n\nCTR: dos\n0.0409\n0.0038\n10.74\n<1e-26\n0.0011\n\n\nCTR: dod\n0.0016\n0.0057\n0.28\n0.7771\n0.0387\n\n\nResidual\n0.1971\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.03382755\n0.18392268\n\n\n\n\n\n\nCTR: sod\n0.00472960\n0.06877208\n+0.56\n\n\n\n\n\nCTR: dos\n0.00000126\n0.00112159\n-0.05\n+0.80\n\n\n\n\nCTR: dod\n0.00149685\n0.03868912\n+0.60\n+0.66\n+0.36\n\n\nResidual\n\n0.03884575\n0.19709325\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\n\nonly(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.56   1.0     .      .\n CTR: dos     -0.05   0.8    1.0     .\n CTR: dod      0.6    0.66   0.36   1.0\n\nNormalized cumulative variances:\n[0.6327, 0.9136, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4\n (Intercept)  -0.41   0.66  -0.47   0.42\n CTR: sod     -0.6   -0.18  -0.34  -0.7\n CTR: dos     -0.43  -0.69  -0.07   0.58\n CTR: dod     -0.53   0.25   0.81   0.0\n\n\nWe note that the critical correlation parameter between spatial (sod) and attraction (dod) is now estimated at .66 ‚Äì not that close to the 1.0 boundary that caused singularity in Kliegl et al. (2010). However, the LMM based on log reaction times is still singular. Let‚Äôs check for untransformed reaction times.\n\nm1_rt = let\n  form = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4147\n48.05\n<1e-99\n59.3745\n\n\nCTR: sod\n23.0720\n2.6415\n8.73\n<1e-17\n22.8504\n\n\nCTR: dos\n13.0855\n1.4583\n8.97\n<1e-18\n6.8329\n\n\nCTR: dod\n2.6860\n2.0608\n1.30\n0.1925\n15.1026\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1_rt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n3525.32539\n59.37445\n\n\n\n\n\n\nCTR: sod\n522.13947\n22.85037\n+0.66\n\n\n\n\n\nCTR: dos\n46.68842\n6.83289\n+0.35\n+0.15\n\n\n\n\nCTR: dod\n228.08774\n15.10257\n+0.53\n+0.65\n+0.30\n\n\nResidual\n\n4254.25039\n65.22461\n\n\n\n\n\n\n\n\n\nissingular(m1_rt)\n\nfalse\n\n\nFor untransformed reaction times, we see the model is not singular."
  },
  {
    "objectID": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "href": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Diagnostic plots of LMM residuals",
    "text": "Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\nResidual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nscatter(fitted(m1), residuals(m1))\n\n\n\n\n\nFigure¬†4: Residuals versus fitted values for model m1\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted values from m1\", :r => \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\n\n\nFigure¬†5: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n\nQ-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\n\n\nCode\nqqnorm(m1; qqline=:none)\n\nFigure¬†6: ?(caption)\n\n\n\n\n\nCode\nqqnorm(m1_rt; qqline=:none)\n\nFigure¬†7: ?(caption)\n\n\n\n\nObserved and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is narrower than expected from a normal distribution. We can see this in this plot. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = (;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=repeat([\"residual\", \"normal\"]; inner=n),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure¬†8: Kernel density plot of the standardized residuals for model m1 versus a standard normal"
  },
  {
    "objectID": "kkl15.html#conditional-modes",
    "href": "kkl15.html#conditional-modes",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Conditional modes",
    "text": "Conditional modes\n\nCaterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n\n\nFigure¬†9: Prediction intervals of the subject random effects in model m1\n\n\n\n\nWhen we order the conditional modes for GM, that is (Intercept), the outlier subject S113 becomes visible; the associated experimental effects are not unusual.\n\n\nCode\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=1)\n\n\n\n\n\nFigure¬†10: Prediction intervals of the subject random effects in model m1 ordered by mean response\n\n\n\n\nThe catepillar plot also reveals that credibilty intervals are much shorter for subjects‚Äô Grand Means, shown in (Intercept), than the subjects‚Äô experimental effects, because the latter are based on difference scores not means. Moreover, credibility intervals are shorter for the first spatial effect sod than the other two effects, because the spatial effect involves the valid condition which yielded three times as many trials than the other three conditions. Consequently, the spatial effect is more reliable. Unfortunately, due to differences in scaling of the x-axis of the panels this effect must be inferred. One option to reveal this difference is to reparameterize the LMM such model parameters estimate the conditional modes for the levels of condition rather than the contrast-based effects. This is accomplished by replacing the 1 in the random effect term with 0, as shown next.\n\nm1L = let\n  form = @formula rt ~ 1 + CTR + (0 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4146\n48.05\n<1e-99\n\n\n\nCTR: sod\n23.0720\n2.6415\n8.73\n<1e-17\n60.1729\n\n\nCTR: dos\n13.0855\n1.4583\n8.97\n<1e-18\n62.4723\n\n\nCTR: dod\n2.6860\n2.0609\n1.30\n0.1925\n71.5378\n\n\nCTR: val\n\n\n\n\n47.2871\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1L)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\nCTR: val\n2236.0678\n47.2871\n\n\n\n\n\n\nCTR: sod\n3620.7745\n60.1729\n+0.94\n\n\n\n\n\nCTR: dos\n3902.7873\n62.4723\n+0.93\n+0.99\n\n\n\n\nCTR: dod\n5117.6611\n71.5378\n+0.89\n+0.98\n+0.98\n\n\nResidual\n\n4254.2488\n65.2246\n\n\n\n\n\n\n\n\nThe caterpillar plot for levels shows the effect of the number of trials on credibility intervals; they are obviously much shorter for the valid condition. Note that this effect is not visible in a repeated-measure ANOVA with four condition means per subject as input.\n\n\nCode\n@chain m1L begin\n  ranefinfo\n  only\n  caterpillar!(Figure(; resolution=(800, 1000)), _; orderby=1)\nend\n\n\n\n\n\nFigure¬†11: Prediction intervals of the subject random effects in model m1L\n\n\n\n\n\n\nShrinkage plot\n\nLog-transformed reaction times (LMM m1)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1)\n\n\n\n\n\nFigure¬†12: Shrinkage plots of the subject random effects in model m1L\n\n\n\n\nThree of the CPs are imploded, but not the theoretically critical ones. These implosions did not occur (or were not as visible) for raw reaction times.\n\n\nRaw reaction times (LMM m1_rt)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1_rt)\n\n\n\n\n\nFigure¬†13: Shrinkage plots of the subject random effects in model m1_rt\n\n\n\n\nThe implosion is for three CP visualizations is not observed for raw reaction times. Interesting."
  },
  {
    "objectID": "kkl15.html#parametric-bootstrap",
    "href": "kkl15.html#parametric-bootstrap",
    "title": "RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "Parametric bootstrap",
    "text": "Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\nGenerate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1);\n\n\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n10 rows √ó 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411Œ≤missing(Intercept)5.6919121Œ≤missingCTR: sod0.07775131Œ≤missingCTR: dos0.040674441Œ≤missingCTR: dod-0.0024932651œÉSubj(Intercept)0.1825461œÉSubjCTR: sod0.062357271œÅSubj(Intercept), CTR: sod0.67746481œÉSubjCTR: dos0.0039113991œÅSubj(Intercept), CTR: dos-0.290374101œÅSubjCTR: sod, CTR: dos0.507145\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters\n\n37500\n\n\n\n\nShortest coverage interval\n\nDataFrame(shortestcovint(samp))\n\n\n15 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)5.64875.727752Œ≤missingCTR: sod0.0579340.08897943Œ≤missingCTR: dos0.03291840.0477164Œ≤missingCTR: dod-0.009701970.01267655œÉSubj(Intercept)0.1533530.2075776œÉSubjCTR: sod0.05657480.07997687œÅSubj(Intercept), CTR: sod0.388260.716538œÉSubjCTR: dos0.001016460.01795729œÅSubj(Intercept), CTR: dos-0.9999990.92042210œÅSubjCTR: sod, CTR: dos-0.8843110.99998811œÉSubjCTR: dod0.02772640.048685912œÅSubj(Intercept), CTR: dod0.3928010.8300813œÅSubjCTR: sod, CTR: dod0.4451640.89084214œÅSubjCTR: dos, CTR: dod-0.8345880.93672615œÉresidualmissing0.1958950.198225\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\nFigure¬†14: Ridge plot of fixed-effects bootstrap samples from model m1L\n\n\n\n\n\n\nComparative density plots of bootstrapped parameter estimates\n\nResidual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"residual\")) *\n  mapping(:value => \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\nFigure¬†15: Kernel density estimate from bootstrap samples of the residual standard deviation for model m1L\n\n\n\n\n\n\nFixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not include its density here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" => \"GM\",\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept), CTR: sod\" => \"GM, spatial\",\n  \"(Intercept), CTR: dos\" => \"GM, object\",\n  \"CTR: sod, CTR: dos\" => \"spatial, object\",\n  \"(Intercept), CTR: dod\" => \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" => \"object, attraction\",\n])\ndraw(\n  data(@subset(dat2, :type == \"Œ≤\" && :names ‚â† \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => rn => \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†16: Kernel density estimate from bootstrap samples of the fixed effects for model m1L\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÉ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => rn => \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†17: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThe VC are all very nicely defined.\n\n\nCorrelation parameters (CPs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"œÅ\")) *\n  mapping(\n    :value => \"Correlation\";\n    color=:names => rn => \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure¬†18: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl.\nThe three remaining CPs are not well defined for log-transformed reaction times; they only fit noise and should be removed. It is also possible that fitting the complex experimental design (including target size and rectangle orientation) will lead to more acceptable estimates. The corresponding plot based on LMM m1_rt for raw reaction times still shows them with very wide distributions, but acceptable."
  },
  {
    "objectID": "shrinkageplot.html",
    "href": "shrinkageplot.html",
    "title": "More on shrinkage plots",
    "section": "",
    "text": "I have stated that the likelihood criterion used to fit linear mixed-effects can be considered as balancing fidelity to the data (i.e.¬†fits the observed data well) versus model complexity.\nThis is similar to some of the criterion used in Machine Learning (ML), except that the criterion for LMMs has a rigorous mathematical basis.\nIn the shrinkage plot we consider the values of the random-effects coefficients for the fitted values of the model versus those from a model in which there is no penalty for model complexity.\nIf there is strong subject-to-subject variation then the model fit will tend to values of the random effects similar to those without a penalty on complexity.\nIf the random effects term is not contributing much (i.e.¬†it is ‚Äúinert‚Äù) then the random effects will be shrunk considerably towards zero in some directions.\nLoad the kb07 data set (don‚Äôt tell Reinhold that I used these data)."
  },
  {
    "objectID": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "href": "shrinkageplot.html#expressing-the-covariance-of-random-effects",
    "title": "More on shrinkage plots",
    "section": "Expressing the covariance of random effects",
    "text": "Expressing the covariance of random effects\nEarlier today we mentioned that the parameters being optimized are from a ‚Äúmatrix square root‚Äù of the covariance matrix for the random effects. There is one such lower triangular matrix for each grouping factor.\n\nl1 = first(m1.Œª)   # Cholesky factor of relative covariance for subj\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.451522    ‚ãÖ          ‚ãÖ          ‚ãÖ \n  0.0502576  0.0403171   ‚ãÖ          ‚ãÖ \n -0.0552328  0.0725482  0.0177998   ‚ãÖ \n  0.0351353  0.0845401  0.0333557  0.0\n\n\nNotice the zero on the diagonal. A triangular matrix with zeros on the diagonal is singular.\n\nl2 = last(m1.Œª)    # this one is not singular\n\n4√ó4 LowerTriangular{Float64, Matrix{Float64}}:\n  0.541769    ‚ãÖ            ‚ãÖ           ‚ãÖ \n  0.0267884  0.0544051     ‚ãÖ           ‚ãÖ \n -0.253071   0.268983     0.0          ‚ãÖ \n  0.019981   0.00612443  -0.00380246  0.05935\n\n\nTo regenerate the covariance matrix we need to know that the covariance is not the square of l1, it is l1 * l1' (so that the result is symmetric) and multiplied by œÉÃÇ¬≤\n\nŒ£‚ÇÅ = varest(m1) .* (l1 * l1')\n\n4√ó4 Matrix{Float64}:\n  91110.8   10141.3     -11145.2     7089.81\n  10141.3    1855.22        66.6142  2312.37\n -11145.2      66.6142    3857.1     2139.02\n   7089.81   2312.37      2139.02    4242.94\n\n\n\ndiag(Œ£‚ÇÅ)  # compare to the variance column in the VarCorr output\n\n4-element Vector{Float64}:\n 91110.81853641519\n  1855.222443908261\n  3857.096133983496\n  4242.938057034487\n\n\n\nsqrt.(diag(Œ£‚ÇÅ))\n\n4-element Vector{Float64}:\n 301.8456866288057\n  43.07229322787749\n  62.10552418250325\n  65.13783890362411"
  },
  {
    "objectID": "shrinkageplot.html#shrinkage-plots",
    "href": "shrinkageplot.html#shrinkage-plots",
    "title": "More on shrinkage plots",
    "section": "Shrinkage plots",
    "text": "Shrinkage plots\n\n\nCode\nshrinkageplot(m1)\n\n\n\n\n\nFigure¬†1: Shrinkage plot of model m1\n\n\n\n\nThe upper left panel shows the perfect negative correlation for those two components of the random effects.\n\nshrinkageplot(m1, :item)\n\n\n\n\n\nX1 = Int.(m1.X')\n\n8√ó1789 Matrix{Int64}:\n  1   1   1   1   1  1   1   1   1   1  ‚Ä¶   1   1   1   1   1   1   1  1   1\n -1   1   1  -1  -1  1   1  -1  -1   1      1  -1  -1   1   1  -1  -1  1   1\n -1   1  -1   1  -1  1  -1   1  -1   1     -1   1  -1   1  -1   1  -1  1  -1\n  1  -1  -1  -1  -1  1   1   1   1  -1      1   1   1  -1  -1  -1  -1  1   1\n  1   1  -1  -1   1  1  -1  -1   1   1     -1  -1   1   1  -1  -1   1  1  -1\n -1  -1  -1   1   1  1   1  -1  -1  -1  ‚Ä¶   1  -1  -1  -1  -1   1   1  1   1\n -1  -1   1  -1   1  1  -1   1  -1  -1     -1   1  -1  -1   1  -1   1  1  -1\n  1  -1   1   1  -1  1  -1  -1   1  -1     -1  -1   1  -1   1   1  -1  1  -1\n\n\n\nX1 * X1'\n\n8√ó8 Matrix{Int64}:\n 1789    -1    -1     3    -3     1     1     3\n   -1  1789    -3     1    -1     3     3     1\n   -1    -3  1789     1    -1     3     3     1\n    3     1     1  1789     3    -1    -1    -3\n   -3    -1    -1     3  1789     1     1     3\n    1     3     3    -1     1  1789    -3    -1\n    1     3     3    -1     1    -3  1789    -1\n    3     1     1    -3     3    -1    -1  1789"
  },
  {
    "objectID": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "href": "shrinkageplot.html#how-to-interpret-a-shrinkage-plot",
    "title": "More on shrinkage plots",
    "section": "How to interpret a shrinkage plot",
    "text": "How to interpret a shrinkage plot\n\nExtreme shrinkage (shrunk to a line or to a point) is easy to interpret - the term is not providing benefit and can be removed.\nWhen the range of the blue dots (shrunk values) is comparable to those of the red dots (unshrunk) it indicates that the term after shrinkage is about as strong as without shrinkage.\nBy itself, this doesn‚Äôt mean that the term is important. In some ways you need to get a feeling for the absolute magnitude of the random effects in addition to the relative magnitude.\nSmall magnitude and small relative magnitude indicate you can drop that term"
  },
  {
    "objectID": "shrinkageplot.html#conclusions-from-these-plots",
    "href": "shrinkageplot.html#conclusions-from-these-plots",
    "title": "More on shrinkage plots",
    "section": "Conclusions from these plots",
    "text": "Conclusions from these plots\n\nOnly the intercept for the subj appears to be contributing explanatory power\nFor the item both the intercept and the spkr appear to be contributing\n\n\nm2 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec * spkr * load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\nMinimizing 81    Time: 0:00:00 ( 1.30 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.7582\n77.4709\n28.16\n<1e-99\n364.7286\n298.1109\n\n\nprec: maintain\n-333.8582\n47.4629\n-7.03\n<1e-11\n252.6687\n\n\n\nspkr: old\n67.8114\n16.0526\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.6849\n16.0525\n4.90\n<1e-06\n\n\n\n\nprec: maintain & spkr: old\n-21.8802\n16.0525\n-1.36\n0.1729\n\n\n\n\nprec: maintain & load: yes\n4.4710\n16.0526\n0.28\n0.7806\n\n\n\n\nspkr: old & load: yes\n18.3214\n16.0526\n1.14\n0.2537\n\n\n\n\nprec: maintain & spkr: old & load: yes\n23.5219\n16.0525\n1.47\n0.1428\n\n\n\n\nResidual\n678.9318\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m2)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133026.917\n364.729\n\n\n\n\nprec: maintain\n63841.496\n252.669\n-0.70\n\n\nsubj\n(Intercept)\n88870.081\n298.111\n\n\n\nResidual\n\n460948.432\n678.932\n\n\n\n\n\n\n\n\nCode\nshrinkageplot(m2)\n\n\n\n\n\nFigure¬†2: Shrinkage plot of model m2\n\n\n\n\n\nm3 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 | subj) + (1 + prec | item)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m3)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\nrng = Random.seed!(1234321);\n\n\nm3btstrp = parametricbootstrap(rng, 2000, m3);\n\n\nDataFrame(shortestcovint(m3btstrp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2013.952319.532Œ≤missingprec: maintain-429.807-241.4293Œ≤missingspkr: old35.333995.72724Œ≤missingload: yes47.067111.045œÉitem(Intercept)267.788452.96œÉitemprec: maintain171.547314.7027œÅitem(Intercept), prec: maintain-0.89308-0.4570848œÉsubj(Intercept)235.921364.7179œÉresidualmissing657.736703.054\n\n\n\nridgeplot(m3btstrp)\n\n\n\n\nFigure¬†3: Ridge plot of the fixed-effects coefficients from the bootstrap sample\n\n\n\n\n\n\nridgeplot(m3btstrp; show_intercept=false)\nFigure¬†4: ?(caption)\n\n\n\nm4 = let\n  form = @formula(\n    rt_trunc ~\n      1 + prec + spkr + load + (1 + prec | item) + (1 | subj)\n  )\n  fit(MixedModel, form, kb07; contrasts)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n2181.8526\n77.4681\n28.16\n<1e-99\n364.7125\n298.0259\n\n\nprec: maintain\n-333.7906\n47.4472\n-7.03\n<1e-11\n252.5212\n\n\n\nspkr: old\n67.8790\n16.0785\n4.22\n<1e-04\n\n\n\n\nload: yes\n78.5904\n16.0785\n4.89\n<1e-05\n\n\n\n\nResidual\n680.0319\n\n\n\n\n\n\n\n\n\n\n\nm4bstrp = parametricbootstrap(rng, 2000, m4);\n\n\nridgeplot(m4bstrp; show_intercept=false)\n\n\n\n\n\nDataFrame(shortestcovint(m4bstrp))\n\n\n9 rows √ó 5 columnstypegroupnameslowerupperStringString?String?Float64Float641Œ≤missing(Intercept)2008.722319.82Œ≤missingprec: maintain-433.808-248.8583Œ≤missingspkr: old35.472997.95764Œ≤missingload: yes47.0078108.4375œÉitem(Intercept)261.52444.4266œÉitemprec: maintain177.437318.8467œÅitem(Intercept), prec: maintain-0.898508-0.4773468œÉsubj(Intercept)229.031356.4079œÉresidualmissing656.919701.946\n\n\n\nVarCorr(m4)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nitem\n(Intercept)\n133015.244\n364.713\n\n\n\n\nprec: maintain\n63766.936\n252.521\n-0.70\n\n\nsubj\n(Intercept)\n88819.437\n298.026\n\n\n\nResidual\n\n462443.388\n680.032\n\n\n\n\n\n\n\n\nCode\nlet mods = [m1, m2, m4]\n  DataFrame(;\n    geomdof=(sum ‚àò leverage).(mods),\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows √ó 6 columnsgeomdofnpardevianceAICBICAICcFloat64Int64Float64Float64Float64Float641131.552928637.128695.128854.328696.12107.5431328658.528684.528755.828684.73103.478928663.928681.928731.328682.0\n\n\n\nscatter(fitted(m4), residuals(m4))\n\n\n\n\nFigure¬†5: Residuals versus fitted values for model m4"
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "",
    "text": "Ths script uses a subset of data reported in F√ºhner, Golle, Granacher, & Kliegl (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. (Fuenher2021?)\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollement, that is in their ninth year of life in the third grade. To avoid delays associated with model fitting we work with a reduced data set and less complex models than those in the reference publication. The script requires only a few changes to specify the more complex models in the paper.\nThe script is structured in three main sections:"
  },
  {
    "objectID": "contrasts_fggk21.html#setup",
    "href": "contrasts_fggk21.html#setup",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "1. Setup",
    "text": "1. Setup\n\n1.0 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing AlgebraOfGraphics: linear\nusing Arrow\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nProgressMeter.ijulia_behavior(:clear);\n\n\n\n\n1.1 Readme for ‚Äò./data/fggk21.rds‚Äô\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: ‚ÄúGirls‚Äù (n=55,086), ‚ÄúBoys‚Äù (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units\n\n\n\n1.2 Preprocessing\n\n1.2.1 Read data\n\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n\n7 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1Cohort201120190String2SchoolS100043S8002000String3ChildC002352C1179660String4Sexfemalemale0String5age8.560737.994528.558529.106090Float646TestBPTStar_r0String7score226.1411.141524.651161530.00Float64\n\n\n\n\n1.2.3 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\nbegin\n  dat = @chain df begin\n    @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n    @groupby(:Test, :Sex)\n    combine(x -> x[sample(1:nrow(x), 500), :])\n  end\nend\n\n\n5,000 rows √ó 7 columnsTestSexCohortSchoolChildagescoreStringStringStringStringStringFloat64Float641S20_rBoys2018S101205C0902958.854214.545452S20_rBoys2017S100780C0779408.739223.846153S20_rBoys2012S105223C0997568.917184.545454S20_rBoys2016S101096C0844068.799454.255325S20_rBoys2017S101618C0540168.533884.444446S20_rBoys2016S105491C0865908.815884.651167S20_rBoys2017S111521C1145619.059554.545458S20_rBoys2018S101229C1010038.939084.347839S20_rBoys2013S102295C0978058.91173.9215710S20_rBoys2017S100766C0772188.722794.5454511S20_rBoys2019S106598C0576568.555784.4444412S20_rBoys2015S102258C0810728.761124.5454513S20_rBoys2017S110206C0294578.309384.3478314S20_rBoys2017S110188C1019598.939084.5454515S20_rBoys2011S101552C0513308.503764.0816316S20_rBoys2011S104991C0803108.750174.6511617S20_rBoys2011S101059C0038348.04.6511618S20_rBoys2018S111430C0537258.525675.4054119S20_rBoys2017S104024C0482168.473654.761920S20_rBoys2018S101400C1168749.106094.8780521S20_rBoys2016S105594C0943518.878853.5087722S20_rBoys2019S104220C0286248.30395.1282123S20_rBoys2014S105685C1033078.955515.024S20_rBoys2019S101369C0470958.470914.761925S20_rBoys2012S100912C0694088.66534.0816326S20_rBoys2012S102003C0883228.832314.761927S20_rBoys2017S100845C0386318.394253.703728S20_rBoys2014S103950C0547508.542094.6511629S20_rBoys2016S106677C0103478.125944.3478330S20_rBoys2011S106410C0422118.418894.08163‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\n\n1.2.2 Transformations\n\nbegin\n  transform!(dat, :age, :age => (x -> x .- 8.5) => :a1) # centered age (linear)\n  select!(groupby(dat, :Test), :, :score => zscore => :zScore) # z-score\nend\n\n\n5,000 rows √ó 9 columnsTestSexCohortSchoolChildagescorea1zScoreStringStringStringStringStringFloat64Float64Float64Float641S20_rBoys2018S101205C0902958.854214.545450.3542090.1132692S20_rBoys2017S100780C0779408.739223.846150.23922-1.608043S20_rBoys2012S105223C0997568.917184.545450.417180.1132694S20_rBoys2016S101096C0844068.799454.255320.299452-0.600895S20_rBoys2017S101618C0540168.533884.444440.0338809-0.1353646S20_rBoys2016S105491C0865908.815884.651160.315880.3734677S20_rBoys2017S111521C1145619.059554.545450.5595480.1132698S20_rBoys2018S101229C1010038.939084.347830.439083-0.3731879S20_rBoys2013S102295C0978058.91173.921570.411704-1.4224110S20_rBoys2017S100766C0772188.722794.545450.2227930.11326911S20_rBoys2019S106598C0576568.555784.444440.0557837-0.13536412S20_rBoys2015S102258C0810728.761124.545450.2611230.11326913S20_rBoys2017S110206C0294578.309384.34783-0.190623-0.37318714S20_rBoys2017S110188C1019598.939084.545450.4390830.11326915S20_rBoys2011S101552C0513308.503764.081630.00376454-1.0284116S20_rBoys2011S104991C0803108.750174.651160.2501710.37346717S20_rBoys2011S101059C0038348.04.65116-0.50.37346718S20_rBoys2018S111430C0537258.525675.405410.02566742.2300119S20_rBoys2017S104024C0482168.473654.7619-0.02635180.64605520S20_rBoys2018S101400C1168749.106094.878050.6060920.9319421S20_rBoys2016S105594C0943518.878853.508770.37885-2.4384922S20_rBoys2019S104220C0286248.30395.12821-0.1960991.5476923S20_rBoys2014S105685C1033078.955515.00.455511.2321224S20_rBoys2019S101369C0470958.470914.7619-0.02908970.64605525S20_rBoys2012S100912C0694088.66534.081630.165298-1.0284126S20_rBoys2012S102003C0883228.832314.76190.3323070.64605527S20_rBoys2017S100845C0386318.394253.7037-0.105749-1.9586728S20_rBoys2014S103950C0547508.542094.651160.04209450.37346729S20_rBoys2016S106677C0103478.125944.34783-0.374059-0.37318730S20_rBoys2011S106410C0422118.418894.08163-0.0811088-1.02841‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\n\nbegin\n  dat2 = combine(\n    groupby(dat, [:Test, :Sex]),\n    :score => mean,\n    :score => std,\n    :zScore => mean,\n    :zScore => std,\n  )\nend\n\n\n10 rows √ó 6 columnsTestSexscore_meanscore_stdzScore_meanzScore_stdStringStringFloat64Float64Float64Float641S20_rBoys4.573170.4182760.1815011.029572BPTBoys3.98120.6914640.3333270.966393SLJBoys129.72619.73620.1648251.016894Star_rBoys2.086410.2893840.1119831.039095RunBoys1038.51155.5990.2363051.041316S20_rGirls4.42570.380233-0.1815010.9359317BPTGirls3.50420.657897-0.3333270.9194768SLJGirls123.32818.5501-0.1648250.9557729Star_rGirls2.024030.263789-0.1119830.94718510RunGirls967.89134.131-0.2363050.897645\n\n\n\n\n1.2.3 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566."
  },
  {
    "objectID": "contrasts_fggk21.html#contrast-coding",
    "href": "contrasts_fggk21.html#contrast-coding",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "2. Contrast coding",
    "text": "2. Contrast coding\nContrast coding is part of StatsModels.jl. Here is the primary author‚Äôs (i.e., Dave Kleinschmidt‚Äôs documentation of Modeling Categorical Data.\nThe random factors Child, School, and Cohort are assigned a Grouping contrast. This contrast is needed when the number of groups (i.e., units, levels) is very large. This is the case for Child (i.e., the 108,925 children in the full and probably also the 11,566 children in the reduced data set). The assignment is not necessary for the typical sample size of experiments. However, we use this coding of random factors irrespective of the number of units associated with them to be transparent about the distinction between random and fixed factors.\nA couple of general remarks about the following examples. First, all contrasts defined in this tutorial return an estimate of the Grand Mean (GM) in the intercept, that is they are so-called sum-to-zero contrasts. In both Julia and R the default contrast is Dummy coding which is not a sum-to-zero contrast, but returns the mean of the reference (control) group - unfortunately for (quasi-)experimentally minded scientists.\nSecond, The factor Sex has only two levels. We use EffectCoding (also known as Sum coding in R) to estimate the difference of the levels from the Grand Mean. Unlike in R, the default sign of the effect is for the second level (base is the first, not the last level), but this can be changed with the base kwarg in the command. Effect coding is a sum-to-zero contrast, but when applied to factors with more than two levels does not yield orthogonal contrasts.\nFinally, contrasts for the five levels of the fixed factor Test represent the hypotheses about differences between them. In this tutorial, we use this factor to illustrate various options.\nWe (initially) include only Test as fixed factor and Child as random factor. More complex LMMs can be specified by simply adding other fixed or random factors to the formula.\n\n2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small ‚Äú2-1‚Äù effect ‚Äúcorrelates‚Äù negatively with a larger ‚Äú3-2‚Äù effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  ),\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :Child  => Grouping()\n  :School => Grouping()\n  :Test   => SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Cohort => Grouping()\n  :Sex    => EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nf_ovi_1 = @formula zScore ~ 1 + Test + (1 | Child);\n\n\nm_ovi_SeqDiff_1 = fit(MixedModel, f_ovi_1, dat; contrasts=contr1)\n\nMinimizing 15    Time: 0:00:00 (19.81 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n0.0006\n0.0142\n0.04\n0.9666\n0.6174\n\n\nTest: Star_r\n-0.0001\n0.0445\n-0.00\n0.9979\n\n\n\nTest: S20_r\n-0.0003\n0.0445\n-0.01\n0.9954\n\n\n\nTest: SLJ\n0.0010\n0.0445\n0.02\n0.9815\n\n\n\nTest: BPT\n-0.0029\n0.0446\n-0.07\n0.9473\n\n\n\nResidual\n0.7860\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = let\n  form = @formula zScore ~ 1 + Test * a1 + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0127\n0.0145\n-0.88\n0.3812\n0.6119\n\n\nTest: Star_r\n-0.0168\n0.0453\n-0.37\n0.7100\n\n\n\nTest: S20_r\n0.0045\n0.0452\n0.10\n0.9202\n\n\n\nTest: SLJ\n0.0042\n0.0450\n0.09\n0.9255\n\n\n\nTest: BPT\n-0.0225\n0.0453\n-0.50\n0.6197\n\n\n\na1\n0.2246\n0.0492\n4.56\n<1e-05\n\n\n\nTest: Star_r & a1\n0.2502\n0.1550\n1.61\n0.1065\n\n\n\nTest: S20_r & a1\n-0.0081\n0.1523\n-0.05\n0.9577\n\n\n\nTest: SLJ & a1\n-0.0645\n0.1536\n-0.42\n0.6747\n\n\n\nTest: BPT & a1\n0.2822\n0.1538\n1.83\n0.0665\n\n\n\nResidual\n0.7863\n\n\n\n\n\n\n\n\n\nThe difference between older and younger childrend is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = let\n  form = @formula zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0106\n0.0141\n-0.75\n0.4530\n0.5593\n\n\nTest: Star_r\n-0.0159\n0.0443\n-0.36\n0.7189\n\n\n\nTest: S20_r\n0.0045\n0.0442\n0.10\n0.9192\n\n\n\nTest: SLJ\n0.0007\n0.0440\n0.02\n0.9870\n\n\n\nTest: BPT\n-0.0174\n0.0442\n-0.39\n0.6944\n\n\n\na1\n0.1916\n0.0481\n3.98\n<1e-04\n\n\n\nSex: Boys\n0.2008\n0.0139\n14.48\n<1e-46\n\n\n\nTest: Star_r & a1\n0.2508\n0.1520\n1.65\n0.0989\n\n\n\nTest: S20_r & a1\n-0.0078\n0.1495\n-0.05\n0.9586\n\n\n\nTest: SLJ & a1\n-0.0183\n0.1503\n-0.12\n0.9032\n\n\n\nTest: BPT & a1\n0.2150\n0.1504\n1.43\n0.1528\n\n\n\nTest: Star_r & Sex: Boys\n-0.1317\n0.0435\n-3.03\n0.0025\n\n\n\nTest: S20_r & Sex: Boys\n0.0673\n0.0435\n1.55\n0.1221\n\n\n\nTest: SLJ & Sex: Boys\n-0.0075\n0.0434\n-0.17\n0.8626\n\n\n\nTest: BPT & Sex: Boys\n0.1615\n0.0434\n3.72\n0.0002\n\n\n\nResidual\n0.7960\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role ‚Äì as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls‚Äô ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\nf_ovi = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Child)\nm_ovi_SeqDiff = fit(MixedModel, f_ovi, dat; contrasts=contr1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0141\n-0.76\n0.4461\n0.5622\n\n\nTest: Star_r\n-0.0140\n0.0444\n-0.31\n0.7529\n\n\n\nTest: S20_r\n0.0012\n0.0444\n0.03\n0.9780\n\n\n\nTest: SLJ\n0.0028\n0.0441\n0.06\n0.9490\n\n\n\nTest: BPT\n-0.0158\n0.0442\n-0.36\n0.7218\n\n\n\na1\n0.1929\n0.0481\n4.01\n<1e-04\n\n\n\nSex: Boys\n0.2009\n0.0141\n14.22\n<1e-45\n\n\n\nTest: Star_r & a1\n0.2531\n0.1520\n1.66\n0.0960\n\n\n\nTest: S20_r & a1\n-0.0077\n0.1495\n-0.05\n0.9590\n\n\n\nTest: SLJ & a1\n-0.0192\n0.1502\n-0.13\n0.8981\n\n\n\nTest: BPT & a1\n0.2201\n0.1504\n1.46\n0.1435\n\n\n\nTest: Star_r & Sex: Boys\n-0.1259\n0.0444\n-2.84\n0.0045\n\n\n\nTest: S20_r & Sex: Boys\n0.0590\n0.0444\n1.33\n0.1833\n\n\n\nTest: SLJ & Sex: Boys\n-0.0007\n0.0441\n-0.01\n0.9881\n\n\n\nTest: BPT & Sex: Boys\n0.1676\n0.0442\n3.79\n0.0002\n\n\n\na1 & Sex: Boys\n0.0063\n0.0481\n0.13\n0.8965\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.1075\n0.1520\n-0.71\n0.4794\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.1537\n0.1495\n1.03\n0.3040\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.1387\n0.1502\n-0.92\n0.3558\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.0963\n0.1504\n-0.64\n0.5222\n\n\n\nResidual\n0.7937\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics.\n\n\n2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiosrespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performats also depends on endurance. HeC1 yields a measure of Coordination ‚Äúcorrected‚Äù for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate ‚Äúorthogonal‚Äù than ‚Äúnon-orthogonal‚Äù random-effect structures.\n\ncontr2 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = fit(MixedModel, f_ovi, dat; contrasts=contr2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0141\n-0.76\n0.4461\n0.5622\n\n\nTest: Star_r\n-0.0070\n0.0222\n-0.31\n0.7529\n\n\n\nTest: S20_r\n-0.0019\n0.0128\n-0.15\n0.8805\n\n\n\nTest: SLJ\n-0.0003\n0.0090\n-0.03\n0.9774\n\n\n\nTest: BPT\n-0.0033\n0.0070\n-0.47\n0.6378\n\n\n\na1\n0.1929\n0.0481\n4.01\n<1e-04\n\n\n\nSex: Boys\n0.2009\n0.0141\n14.22\n<1e-45\n\n\n\nTest: Star_r & a1\n0.1265\n0.0760\n1.66\n0.0960\n\n\n\nTest: S20_r & a1\n0.0396\n0.0434\n0.91\n0.3614\n\n\n\nTest: SLJ & a1\n0.0150\n0.0308\n0.49\n0.6258\n\n\n\nTest: BPT & a1\n0.0530\n0.0238\n2.23\n0.0257\n\n\n\nTest: Star_r & Sex: Boys\n-0.0630\n0.0222\n-2.84\n0.0045\n\n\n\nTest: S20_r & Sex: Boys\n-0.0013\n0.0128\n-0.10\n0.9185\n\n\n\nTest: SLJ & Sex: Boys\n-0.0008\n0.0090\n-0.09\n0.9276\n\n\n\nTest: BPT & Sex: Boys\n0.0330\n0.0070\n4.71\n<1e-05\n\n\n\na1 & Sex: Boys\n0.0063\n0.0481\n0.13\n0.8965\n\n\n\nTest: Star_r & a1 & Sex: Boys\n-0.0538\n0.0760\n-0.71\n0.4794\n\n\n\nTest: S20_r & a1 & Sex: Boys\n0.0333\n0.0434\n0.77\n0.4428\n\n\n\nTest: SLJ & a1 & Sex: Boys\n-0.0180\n0.0308\n-0.59\n0.5579\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.0301\n0.0238\n-1.27\n0.2058\n\n\n\nResidual\n0.7937\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent.\n\n\n2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1 0 0 0\n       0 -1 +1 0 0\n       0 0 -1 +1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = fit(MixedModel, f_ovi, dat; contrasts=contr3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0141\n-0.76\n0.4461\n0.5622\n\n\nTest: BPT-other\n-0.0661\n0.1403\n-0.47\n0.6378\n\n\n\nTest: Star-End\n-0.0140\n0.0444\n-0.31\n0.7529\n\n\n\nTest: S20-Star\n0.0012\n0.0444\n0.03\n0.9780\n\n\n\nTest: SLJ-S20\n0.0028\n0.0441\n0.06\n0.9490\n\n\n\na1\n0.1929\n0.0481\n4.01\n<1e-04\n\n\n\nSex: Boys\n0.2009\n0.0141\n14.22\n<1e-45\n\n\n\nTest: BPT-other & a1\n1.0604\n0.4754\n2.23\n0.0257\n\n\n\nTest: Star-End & a1\n0.2531\n0.1520\n1.66\n0.0960\n\n\n\nTest: S20-Star & a1\n-0.0077\n0.1495\n-0.05\n0.9590\n\n\n\nTest: SLJ-S20 & a1\n-0.0192\n0.1502\n-0.13\n0.8981\n\n\n\nTest: BPT-other & Sex: Boys\n0.6606\n0.1403\n4.71\n<1e-05\n\n\n\nTest: Star-End & Sex: Boys\n-0.1259\n0.0444\n-2.84\n0.0045\n\n\n\nTest: S20-Star & Sex: Boys\n0.0590\n0.0444\n1.33\n0.1833\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0007\n0.0441\n-0.01\n0.9881\n\n\n\na1 & Sex: Boys\n0.0063\n0.0481\n0.13\n0.8965\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n-0.6014\n0.4754\n-1.27\n0.2058\n\n\n\nTest: Star-End & a1 & Sex: Boys\n-0.1075\n0.1520\n-0.71\n0.4794\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1537\n0.1495\n1.03\n0.3040\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1387\n0.1502\n-0.92\n0.3558\n\n\n\nResidual\n0.7937\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 +1 0 0 0\n      0 -1 +1 0 0\n      0 0 -1 +1 0\n      0 0 0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = fit(MixedModel, f_ovi, dat; contrasts=contr1b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0141\n-0.76\n0.4461\n0.5622\n\n\nTest: Star-Run\n-0.0140\n0.0444\n-0.31\n0.7529\n\n\n\nTest: S20-Star\n0.0012\n0.0444\n0.03\n0.9780\n\n\n\nTest: SLJ-S20\n0.0028\n0.0441\n0.06\n0.9490\n\n\n\nTest: BPT-SLJ\n-0.0158\n0.0442\n-0.36\n0.7218\n\n\n\na1\n0.1929\n0.0481\n4.01\n<1e-04\n\n\n\nSex: Boys\n0.2009\n0.0141\n14.22\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.2531\n0.1520\n1.66\n0.0960\n\n\n\nTest: S20-Star & a1\n-0.0077\n0.1495\n-0.05\n0.9590\n\n\n\nTest: SLJ-S20 & a1\n-0.0192\n0.1502\n-0.13\n0.8981\n\n\n\nTest: BPT-SLJ & a1\n0.2201\n0.1504\n1.46\n0.1435\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1259\n0.0444\n-2.84\n0.0045\n\n\n\nTest: S20-Star & Sex: Boys\n0.0590\n0.0444\n1.33\n0.1833\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0007\n0.0441\n-0.01\n0.9880\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1676\n0.0442\n3.79\n0.0002\n\n\n\na1 & Sex: Boys\n0.0063\n0.0481\n0.13\n0.8965\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1075\n0.1520\n-0.71\n0.4794\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1537\n0.1495\n1.03\n0.3040\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1387\n0.1502\n-0.92\n0.3558\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.0963\n0.1504\n-0.64\n0.5222\n\n\n\nResidual\n0.7937\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 86    Time: 0:00:00 ( 5.85 ms/it)\n  objective:  13892.725596292521\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0141\n-0.76\n0.4458\n0.6072\n\n\nTest: Star-Run\n-0.0192\n0.0451\n-0.42\n0.6709\n0.0000\n\n\nTest: S20-Star\n0.0020\n0.0443\n0.05\n0.9632\n0.6415\n\n\nTest: SLJ-S20\n0.0069\n0.0433\n0.16\n0.8738\n0.3895\n\n\nTest: BPT-SLJ\n-0.0182\n0.0439\n-0.42\n0.6777\n0.0000\n\n\na1\n0.1940\n0.0482\n4.03\n<1e-04\n\n\n\nSex: Boys\n0.2008\n0.0141\n14.19\n<1e-44\n\n\n\nTest: Star-Run & a1\n0.2616\n0.1548\n1.69\n0.0910\n\n\n\nTest: S20-Star & a1\n-0.0095\n0.1493\n-0.06\n0.9491\n\n\n\nTest: SLJ-S20 & a1\n-0.0223\n0.1478\n-0.15\n0.8802\n\n\n\nTest: BPT-SLJ & a1\n0.2207\n0.1492\n1.48\n0.1393\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1320\n0.0451\n-2.93\n0.0034\n\n\n\nTest: S20-Star & Sex: Boys\n0.0625\n0.0443\n1.41\n0.1583\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0010\n0.0433\n0.02\n0.9823\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1668\n0.0439\n3.80\n0.0001\n\n\n\na1 & Sex: Boys\n0.0054\n0.0482\n0.11\n0.9104\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.1010\n0.1548\n-0.65\n0.5141\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1565\n0.1493\n1.05\n0.2945\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1479\n0.1478\n-1.00\n0.3171\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.0896\n0.1492\n-0.60\n0.5483\n\n\n\nResidual\n0.6659\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\nMinimizing 2847      Time: 0:00:18 ( 6.65 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0113\n0.0140\n-0.81\n0.4190\n\n\n\nTest: Star-Run\n-0.0252\n0.0445\n-0.57\n0.5706\n\n\n\nTest: S20-Star\n0.0000\n0.0448\n0.00\n0.9994\n\n\n\nTest: SLJ-S20\n0.0092\n0.0442\n0.21\n0.8348\n\n\n\nTest: BPT-SLJ\n-0.0183\n0.0437\n-0.42\n0.6753\n\n\n\na1\n0.1908\n0.0478\n3.99\n<1e-04\n\n\n\nSex: Boys\n0.2008\n0.0140\n14.32\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.2540\n0.1531\n1.66\n0.0970\n\n\n\nTest: S20-Star & a1\n-0.0130\n0.1513\n-0.09\n0.9313\n\n\n\nTest: SLJ-S20 & a1\n-0.0055\n0.1508\n-0.04\n0.9711\n\n\n\nTest: BPT-SLJ & a1\n0.2240\n0.1485\n1.51\n0.1316\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1338\n0.0445\n-3.01\n0.0026\n\n\n\nTest: S20-Star & Sex: Boys\n0.0676\n0.0448\n1.51\n0.1310\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0039\n0.0442\n-0.09\n0.9298\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1646\n0.0437\n3.77\n0.0002\n\n\n\na1 & Sex: Boys\n0.0108\n0.0478\n0.23\n0.8212\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.0796\n0.1531\n-0.52\n0.6029\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1491\n0.1513\n0.99\n0.3242\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1538\n0.1508\n-1.02\n0.3078\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.0874\n0.1485\n-0.59\n0.5560\n\n\n\nTest: BPT\n\n\n\n\n0.9361\n\n\nTest: SLJ\n\n\n\n\n0.9828\n\n\nTest: Star_r\n\n\n\n\n0.9915\n\n\nTest: Run\n\n\n\n\n0.9762\n\n\nTest: S20_r\n\n\n\n\n0.9728\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = let\n  f_cpx_0 = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, f_cpx_0, dat; contrasts=contr1b)\nend\n\nMinimizing 2847      Time: 0:00:19 ( 6.81 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0113\n0.0140\n-0.81\n0.4190\n\n\n\nTest: Star-Run\n-0.0252\n0.0445\n-0.57\n0.5706\n\n\n\nTest: S20-Star\n0.0000\n0.0448\n0.00\n0.9994\n\n\n\nTest: SLJ-S20\n0.0092\n0.0442\n0.21\n0.8348\n\n\n\nTest: BPT-SLJ\n-0.0183\n0.0437\n-0.42\n0.6753\n\n\n\na1\n0.1908\n0.0478\n3.99\n<1e-04\n\n\n\nSex: Boys\n0.2008\n0.0140\n14.32\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.2540\n0.1531\n1.66\n0.0970\n\n\n\nTest: S20-Star & a1\n-0.0130\n0.1513\n-0.09\n0.9313\n\n\n\nTest: SLJ-S20 & a1\n-0.0055\n0.1508\n-0.04\n0.9711\n\n\n\nTest: BPT-SLJ & a1\n0.2240\n0.1485\n1.51\n0.1316\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1338\n0.0445\n-3.01\n0.0026\n\n\n\nTest: S20-Star & Sex: Boys\n0.0676\n0.0448\n1.51\n0.1310\n\n\n\nTest: SLJ-S20 & Sex: Boys\n-0.0039\n0.0442\n-0.09\n0.9298\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1646\n0.0437\n3.77\n0.0002\n\n\n\na1 & Sex: Boys\n0.0108\n0.0478\n0.23\n0.8212\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.0796\n0.1531\n-0.52\n0.6029\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1491\n0.1513\n0.99\n0.3242\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1538\n0.1508\n-1.02\n0.3078\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.0874\n0.1485\n-0.59\n0.5560\n\n\n\nTest: BPT\n\n\n\n\n0.9361\n\n\nTest: SLJ\n\n\n\n\n0.9828\n\n\nTest: Star_r\n\n\n\n\n0.9915\n\n\nTest: Run\n\n\n\n\n0.9762\n\n\nTest: S20_r\n\n\n\n\n0.9728\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.95300989\n0.97622226\n\n\n\n\n\n\n\nTest: Star_r\n0.98316797\n0.99154827\n+0.64\n\n\n\n\n\n\nTest: S20_r\n0.94634960\n0.97280502\n-0.58\n+0.16\n\n\n\n\n\nTest: SLJ\n0.96595991\n0.98283259\n+0.12\n+0.32\n+0.55\n\n\n\n\nTest: BPT\n0.87624919\n0.93608183\n-0.06\n-0.62\n-0.41\n+0.01\n\n\nResidual\n\n0.00000000\n0.00003703\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .\n Test: Star_r   0.64   1.0     .      .      .\n Test: S20_r   -0.58   0.16   1.0     .      .\n Test: SLJ      0.12   0.32   0.55   1.0     .\n Test: BPT     -0.06  -0.62  -0.41   0.01   1.0\n\nNormalized cumulative variances:\n[0.4139, 0.7688, 0.9747, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.26   0.67  -0.24  -0.15  -0.63\n Test: Star_r  -0.63   0.27   0.05   0.62   0.38\n Test: S20_r   -0.35  -0.64  -0.01   0.31  -0.61\n Test: SLJ     -0.39  -0.24  -0.74  -0.41   0.28\n Test: BPT      0.51   0.06  -0.63   0.58  -0.05,)\n\n\n\nf_cpx_1 = @formula(\n  zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child)\n)\nm_cpx_1_SeqDiff =\nfit(MixedModel, f_cpx_1, dat; contrasts=contr1b)\n\nMinimizing 2188      Time: 0:00:15 ( 7.20 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0105\n0.0141\n-0.74\n0.4570\n0.4905\n\n\nTest: Star-Run\n-0.0284\n0.0441\n-0.64\n0.5203\n0.7764\n\n\nTest: S20-Star\n-0.0015\n0.0449\n-0.03\n0.9726\n1.1587\n\n\nTest: SLJ-S20\n0.0187\n0.0442\n0.42\n0.6730\n0.6963\n\n\nTest: BPT-SLJ\n-0.0241\n0.0437\n-0.55\n0.5809\n1.1219\n\n\na1\n0.1937\n0.0480\n4.04\n<1e-04\n\n\n\nSex: Boys\n0.2018\n0.0141\n14.35\n<1e-45\n\n\n\nTest: Star-Run & a1\n0.2697\n0.1518\n1.78\n0.0757\n\n\n\nTest: S20-Star & a1\n-0.0234\n0.1517\n-0.15\n0.8774\n\n\n\nTest: SLJ-S20 & a1\n-0.0106\n0.1511\n-0.07\n0.9438\n\n\n\nTest: BPT-SLJ & a1\n0.2287\n0.1488\n1.54\n0.1243\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1404\n0.0441\n-3.18\n0.0015\n\n\n\nTest: S20-Star & Sex: Boys\n0.0675\n0.0449\n1.50\n0.1330\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0029\n0.0442\n0.07\n0.9476\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1625\n0.0437\n3.72\n0.0002\n\n\n\na1 & Sex: Boys\n0.0056\n0.0480\n0.12\n0.9066\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n-0.0803\n0.1518\n-0.53\n0.5967\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n0.1715\n0.1517\n1.13\n0.2582\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n-0.1851\n0.1511\n-1.23\n0.2204\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.0685\n0.1488\n-0.46\n0.6452\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .      .      .\n Test: Star-Run   0.57   1.0     .      .      .\n Test: S20-Star  -0.07   0.42   1.0     .      .\n Test: SLJ-S20    0.28  -0.53  -0.15   1.0     .\n Test: BPT-SLJ   -0.61  -0.61  -0.17   0.22   1.0\n\nNormalized cumulative variances:\n[0.4715, 0.7527, 0.9248, 0.9997, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4    PC5\n (Intercept)     -0.43   0.61  -0.02  -0.34   0.57\n Test: Star-Run  -0.61  -0.14   0.03  -0.49  -0.6\n Test: S20-Star  -0.27  -0.41  -0.83   0.08   0.25\n Test: SLJ-S20    0.27   0.64  -0.54   0.03  -0.48\n Test: BPT-SLJ    0.54  -0.19  -0.12  -0.8    0.13,)\n\n\n\n\n2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principle components of the published LMM based on test scores, not test effects (contrasts) - coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 0 0 0 +1\n      -3 +2 +2 +2 -3\n      0 +2 -1 -1 0\n      0 0 +1 -1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = fit(MixedModel, f_cpx_1, dat; contrasts=contr4)\n\nMinimizing 2044      Time: 0:00:14 ( 7.10 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0113\n0.0141\n-0.80\n0.4245\n0.6105\n\n\nTest: c5.1\n-0.0272\n0.0432\n-0.63\n0.5287\n1.4819\n\n\nTest: c234.15\n-0.0080\n0.1686\n-0.05\n0.9622\n0.4014\n\n\nTest: c2.34\n-0.0069\n0.0785\n-0.09\n0.9304\n1.8141\n\n\nTest: c3.4\n-0.0032\n0.0450\n-0.07\n0.9435\n1.5667\n\n\na1\n0.1919\n0.0482\n3.98\n<1e-04\n\n\n\nSex: Boys\n0.2002\n0.0141\n14.17\n<1e-44\n\n\n\nTest: c5.1 & a1\n0.4478\n0.1485\n3.02\n0.0026\n\n\n\nTest: c234.15 & a1\n0.0789\n0.5795\n0.14\n0.8917\n\n\n\nTest: c2.34 & a1\n0.0312\n0.2645\n0.12\n0.9061\n\n\n\nTest: c3.4 & a1\n0.0250\n0.1532\n0.16\n0.8705\n\n\n\nTest: c5.1 & Sex: Boys\n0.0975\n0.0432\n2.26\n0.0239\n\n\n\nTest: c234.15 & Sex: Boys\n-0.8622\n0.1686\n-5.11\n<1e-06\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1354\n0.0785\n-1.72\n0.0846\n\n\n\nTest: c3.4 & Sex: Boys\n0.0048\n0.0450\n0.11\n0.9152\n\n\n\na1 & Sex: Boys\n0.0101\n0.0482\n0.21\n0.8339\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.1810\n0.1485\n-1.22\n0.2229\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3079\n0.5795\n0.53\n0.5953\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.1363\n0.2645\n-0.52\n0.6063\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.1367\n0.1532\n0.89\n0.3722\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3726884\n0.6104821\n\n\n\n\n\n\n\nTest: c5.1\n2.1960619\n1.4819116\n-0.06\n\n\n\n\n\n\nTest: c234.15\n0.1611427\n0.4014258\n+0.50\n-0.14\n\n\n\n\n\nTest: c2.34\n3.2908760\n1.8140772\n+0.35\n-0.23\n-0.59\n\n\n\n\nTest: c3.4\n2.4544981\n1.5666838\n-0.14\n+0.20\n+0.05\n-0.24\n\n\nResidual\n\n0.0000000\n0.0000079\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1     -0.06   1.0     .      .      .\n Test: c234.15   0.5   -0.14   1.0     .      .\n Test: c2.34     0.35  -0.23  -0.59   1.0     .\n Test: c3.4     -0.14   0.2    0.05  -0.24   1.0\n\nNormalized cumulative variances:\n[0.3371, 0.6485, 0.842, 1.0, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)     0.06   0.68   0.53   0.06  -0.5\n Test: c5.1     -0.24  -0.36   0.71  -0.52   0.19\n Test: c234.15  -0.55   0.56  -0.08  -0.02   0.62\n Test: c2.34     0.71   0.1    0.31   0.23   0.58\n Test: c3.4     -0.37  -0.29   0.33   0.82  -0.0,)\n\n\nThere is a numerical interaction with a z-value > 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger ‚Äúunfitness‚Äù gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related ‚Äúfitness‚Äù gains of girls than boys (i.e., in Run_r).\n\ncontr4b = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => HypothesisCoding(\n      [\n        0.49 -0.04 0.20 0.03 -0.85\n        0.70 -0.56 -0.21 -0.13 0.37\n        0.31 0.68 -0.56 -0.35 0.00\n        0.04 0.08 0.61 -0.78 0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    ),\n  ),\n);\n\n\nm_cpx_1_PC_2 = fit(MixedModel, f_cpx_1, dat; contrasts=contr4b)\n\nMinimizing 2611      Time: 0:00:18 ( 7.12 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0125\n0.0141\n-0.88\n0.3784\n0.6226\n\n\nTest: c5.1\n0.0363\n0.0268\n1.36\n0.1750\n0.8772\n\n\nTest: c234.15\n0.0254\n0.0284\n0.89\n0.3711\n0.8887\n\n\nTest: c12.34\n-0.0030\n0.0314\n-0.10\n0.9233\n0.9881\n\n\nTest: c3.4\n-0.0082\n0.0314\n-0.26\n0.7934\n0.6360\n\n\na1\n0.1868\n0.0487\n3.83\n0.0001\n\n\n\nSex: Boys\n0.1937\n0.0141\n13.69\n<1e-41\n\n\n\nTest: c5.1 & a1\n-0.2985\n0.1046\n-2.85\n0.0043\n\n\n\nTest: c234.15 & a1\n-0.0924\n0.1076\n-0.86\n0.3908\n\n\n\nTest: c12.34 & a1\n-0.0661\n0.1078\n-0.61\n0.5399\n\n\n\nTest: c3.4 & a1\n0.0317\n0.1074\n0.29\n0.7683\n\n\n\nTest: c5.1 & Sex: Boys\n-0.1118\n0.0268\n-4.17\n<1e-04\n\n\n\nTest: c234.15 & Sex: Boys\n0.1300\n0.0284\n4.58\n<1e-05\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0196\n0.0314\n-0.62\n0.5325\n\n\n\nTest: c3.4 & Sex: Boys\n0.0194\n0.0314\n0.62\n0.5362\n\n\n\na1 & Sex: Boys\n0.0141\n0.0487\n0.29\n0.7714\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1498\n0.1046\n1.43\n0.1521\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.0183\n0.1076\n-0.17\n0.8652\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0540\n0.1078\n-0.50\n0.6163\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0805\n0.1074\n0.75\n0.4535\n\n\n\nResidual\n0.0001\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.38757281\n0.62255346\n\n\n\n\n\n\n\nTest: c5.1\n0.76941569\n0.87716344\n+0.59\n\n\n\n\n\n\nTest: c234.15\n0.78984628\n0.88873296\n-0.13\n-0.79\n\n\n\n\n\nTest: c12.34\n0.97633678\n0.98809756\n-0.03\n+0.24\n+0.14\n\n\n\n\nTest: c3.4\n0.40445223\n0.63596559\n-0.15\n-0.18\n+0.24\n-0.02\n\n\nResidual\n\n0.00000001\n0.00007995\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1      0.59   1.0     .      .      .\n Test: c234.15  -0.13  -0.79   1.0     .      .\n Test: c12.34   -0.03   0.24   0.14   1.0     .\n Test: c3.4     -0.15  -0.18   0.24  -0.02   1.0\n\nNormalized cumulative variances:\n[0.43, 0.6472, 0.8301, 1.0, 1.0]\n\nComponent loadings\n                  PC1   PC2    PC3    PC4    PC5\n (Intercept)    -0.43  0.11  -0.41   0.71   0.36\n Test: c5.1     -0.66  0.16  -0.16  -0.16  -0.7\n Test: c234.15   0.55  0.31  -0.05   0.54  -0.56\n Test: c12.34   -0.07  0.92   0.24  -0.17   0.26\n Test: c3.4      0.28  0.16  -0.86  -0.38   0.07,)\n\n\n\nf_zcp_1 = @formula(zScore ~ 1 + Test*a1*Sex + zerocorr(1 + Test | Child))\nm_zcp_1_PC_2 = fit(MixedModel, f_zcp_1, dat; contrasts=contr4b)\n\nMinimizing 715   Time: 0:00:02 ( 4.02 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0103\n0.0143\n-0.72\n0.4694\n0.6839\n\n\nTest: c5.1\n0.0192\n0.0307\n0.63\n0.5305\n0.6938\n\n\nTest: c234.15\n0.0073\n0.0316\n0.23\n0.8161\n0.7049\n\n\nTest: c12.34\n0.0025\n0.0321\n0.08\n0.9372\n0.9300\n\n\nTest: c3.4\n-0.0064\n0.0312\n-0.20\n0.8376\n0.7452\n\n\na1\n0.1881\n0.0487\n3.86\n0.0001\n\n\n\nSex: Boys\n0.1921\n0.0143\n13.44\n<1e-40\n\n\n\nTest: c5.1 & a1\n-0.2944\n0.1048\n-2.81\n0.0050\n\n\n\nTest: c234.15 & a1\n-0.0981\n0.1081\n-0.91\n0.3640\n\n\n\nTest: c12.34 & a1\n-0.0654\n0.1086\n-0.60\n0.5470\n\n\n\nTest: c3.4 & a1\n0.0364\n0.1065\n0.34\n0.7327\n\n\n\nTest: c5.1 & Sex: Boys\n-0.1006\n0.0307\n-3.28\n0.0010\n\n\n\nTest: c234.15 & Sex: Boys\n0.1388\n0.0316\n4.40\n<1e-04\n\n\n\nTest: c12.34 & Sex: Boys\n-0.0236\n0.0321\n-0.74\n0.4617\n\n\n\nTest: c3.4 & Sex: Boys\n0.0168\n0.0312\n0.54\n0.5905\n\n\n\na1 & Sex: Boys\n0.0110\n0.0487\n0.23\n0.8214\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.1588\n0.1048\n1.52\n0.1298\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.0004\n0.1081\n0.00\n0.9972\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n-0.0607\n0.1086\n-0.56\n0.5760\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.0770\n0.1065\n0.72\n0.4700\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_zcp_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.46766669\n0.68386160\n\n\n\n\n\n\n\nTest: c5.1\n0.48137676\n0.69381320\n.\n\n\n\n\n\n\nTest: c234.15\n0.49692054\n0.70492591\n.\n.\n\n\n\n\n\nTest: c12.34\n0.86487697\n0.92998762\n.\n.\n.\n\n\n\n\nTest: c3.4\n0.55537296\n0.74523350\n.\n.\n.\n.\n\n\nResidual\n\n0.00000000\n0.00000268\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_zcp_1_PC_2, m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13346\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13474\n-128\n10\nNaN"
  },
  {
    "objectID": "contrasts_fggk21.html#other-topics",
    "href": "contrasts_fggk21.html#other-topics",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "3. Other topics",
    "text": "3. Other topics\n\n3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13897.476150404531\n 13897.476150404531\n 13897.476150404531\n\n\n\n\n3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\nbegin\n  f_Child = @formula zScore ~\n    1 + Test * a1 * Sex + (1 + Test | Child)\n  m_Child_SDC = fit(MixedModel, f_Child, dat; contrasts=contr1)\n  m_Child_HeC = fit(MixedModel, f_Child, dat; contrasts=contr2)\n  m_Child_HyC = fit(MixedModel, f_Child, dat; contrasts=contr3)\n  m_Child_PCA = fit(MixedModel, f_Child, dat; contrasts=contr4)\nend\n\nMinimizing 2044      Time: 0:00:14 ( 7.05 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0113\n0.0141\n-0.80\n0.4245\n0.6105\n\n\nTest: c5.1\n-0.0272\n0.0432\n-0.63\n0.5287\n1.4819\n\n\nTest: c234.15\n-0.0080\n0.1686\n-0.05\n0.9622\n0.4014\n\n\nTest: c2.34\n-0.0069\n0.0785\n-0.09\n0.9304\n1.8141\n\n\nTest: c3.4\n-0.0032\n0.0450\n-0.07\n0.9435\n1.5667\n\n\na1\n0.1919\n0.0482\n3.98\n<1e-04\n\n\n\nSex: Boys\n0.2002\n0.0141\n14.17\n<1e-44\n\n\n\nTest: c5.1 & a1\n0.4478\n0.1485\n3.02\n0.0026\n\n\n\nTest: c234.15 & a1\n0.0789\n0.5795\n0.14\n0.8917\n\n\n\nTest: c2.34 & a1\n0.0312\n0.2645\n0.12\n0.9061\n\n\n\nTest: c3.4 & a1\n0.0250\n0.1532\n0.16\n0.8705\n\n\n\nTest: c5.1 & Sex: Boys\n0.0975\n0.0432\n2.26\n0.0239\n\n\n\nTest: c234.15 & Sex: Boys\n-0.8622\n0.1686\n-5.11\n<1e-06\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1354\n0.0785\n-1.72\n0.0846\n\n\n\nTest: c3.4 & Sex: Boys\n0.0048\n0.0450\n0.11\n0.9152\n\n\n\na1 & Sex: Boys\n0.0101\n0.0482\n0.21\n0.8339\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.1810\n0.1485\n-1.22\n0.2229\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.3079\n0.5795\n0.53\n0.5953\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.1363\n0.2645\n-0.52\n0.6063\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.1367\n0.1532\n0.89\n0.3722\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.27939638\n0.52857959\n\n\n\n\n\n\n\nTest: Star_r\n0.98602207\n0.99298644\n+0.44\n\n\n\n\n\n\nTest: S20_r\n1.03745970\n1.01855766\n+0.05\n+0.33\n\n\n\n\n\nTest: SLJ\n0.61103602\n0.78168793\n+0.16\n-0.79\n-0.10\n\n\n\n\nTest: BPT\n1.19939056\n1.09516691\n-0.55\n-0.14\n+0.08\n-0.18\n\n\nResidual\n\n0.00000000\n0.00001633\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.19722352\n0.44409855\n\n\n\n\n\n\n\nTest: Star_r\n0.29121692\n0.53964518\n+0.40\n\n\n\n\n\n\nTest: S20_r\n0.18434120\n0.42934974\n-0.08\n+0.71\n\n\n\n\n\nTest: SLJ\n0.04917003\n0.22174316\n+0.33\n-0.27\n+0.06\n\n\n\n\nTest: BPT\n0.07317694\n0.27051236\n-0.51\n-0.41\n+0.15\n+0.21\n\n\nResidual\n\n0.00000000\n0.00000874\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.42414521\n0.65126432\n\n\n\n\n\n\n\nTest: BPT-other\n1.91109652\n1.38242415\n+0.98\n\n\n\n\n\n\nTest: Star-End\n1.07468948\n1.03667231\n+0.34\n+0.16\n\n\n\n\n\nTest: S20-Star\n1.53733440\n1.23989290\n-0.31\n-0.32\n+0.21\n\n\n\n\nTest: SLJ-S20\n1.18886096\n1.09034901\n+0.33\n+0.50\n-0.77\n-0.40\n\n\nResidual\n\n0.00000000\n0.00003056\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.3726884\n0.6104821\n\n\n\n\n\n\n\nTest: c5.1\n2.1960619\n1.4819116\n-0.06\n\n\n\n\n\n\nTest: c234.15\n0.1611427\n0.4014258\n+0.50\n-0.14\n\n\n\n\n\nTest: c2.34\n3.2908760\n1.8140772\n+0.35\n-0.23\n-0.59\n\n\n\n\nTest: c3.4\n2.4544981\n1.5666838\n-0.14\n+0.20\n+0.05\n-0.24\n\n\nResidual\n\n0.0000000\n0.0000079\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nbegin\n  f_Child0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  m_Child_SDC0 = fit(MixedModel, f_Child0, dat; contrasts=contr1)\n  m_Child_HeC0 = fit(MixedModel, f_Child0, dat; contrasts=contr2)\n  m_Child_HyC0 = fit(MixedModel, f_Child0, dat; contrasts=contr3)\n  m_Child_PCA0 = fit(MixedModel, f_Child0, dat; contrasts=contr4)\nend\n\nMinimizing 618   Time: 0:00:02 ( 4.12 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Child\n\n\n\n\n(Intercept)\n-0.0108\n0.0142\n-0.76\n0.4469\n0.7223\n\n\nTest: c5.1\n-0.0234\n0.0436\n-0.54\n0.5922\n1.2608\n\n\nTest: c234.15\n0.0049\n0.1695\n0.03\n0.9771\n0.6768\n\n\nTest: c2.34\n-0.0129\n0.0778\n-0.17\n0.8678\n2.0020\n\n\nTest: c3.4\n-0.0054\n0.0444\n-0.12\n0.9039\n1.1475\n\n\na1\n0.1945\n0.0483\n4.03\n<1e-04\n\n\n\nSex: Boys\n0.1999\n0.0142\n14.10\n<1e-44\n\n\n\nTest: c5.1 & a1\n0.4454\n0.1500\n2.97\n0.0030\n\n\n\nTest: c234.15 & a1\n0.1372\n0.5809\n0.24\n0.8133\n\n\n\nTest: c2.34 & a1\n0.0432\n0.2620\n0.17\n0.8689\n\n\n\nTest: c3.4 & a1\n0.0362\n0.1515\n0.24\n0.8111\n\n\n\nTest: c5.1 & Sex: Boys\n0.0989\n0.0436\n2.27\n0.0234\n\n\n\nTest: c234.15 & Sex: Boys\n-0.8435\n0.1695\n-4.98\n<1e-06\n\n\n\nTest: c2.34 & Sex: Boys\n-0.1236\n0.0778\n-1.59\n0.1120\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0044\n0.0444\n-0.10\n0.9209\n\n\n\na1 & Sex: Boys\n0.0060\n0.0483\n0.12\n0.9008\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.1902\n0.1500\n-1.27\n0.2049\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.2167\n0.5809\n0.37\n0.7091\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.1570\n0.2620\n-0.60\n0.5490\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n0.1444\n0.1515\n0.95\n0.3406\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13893\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13416\n476\n10\n<1e-95\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13324\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13390\n-66\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13398\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13461\n-63\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nœá¬≤\nœá¬≤-dof\nP(>œá¬≤)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13402\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13398\n4\n10\n0.9637\n\n\n\n\n\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nbegin\n  zcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\n  gof_summary = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM,\n      dof=dof.(mods),\n      deviance=deviance.(mods),\n      AIC=aic.(mods),\n      BIC=bic.(mods),\n    ),\n    :deviance,\n  )\nend\n\n\n4 rows √ó 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641HeC02613324.113376.113545.52HyC02613398.013450.013619.53PCA02613401.913453.913623.34SDC02613892.713944.714114.2\n\n\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM.\n\n\n3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula zScore ~\n  1 + Test * a1 * Sex + (1 + Test + a1 | School);\nm_School_SeqDiff = fit(MixedModel, f_School, dat; contrasts=contr1);\nm_School_Helmert = fit(MixedModel, f_School, dat; contrasts=contr2);\nm_School_Hypo = fit(MixedModel, f_School, dat; contrasts=contr3);\nm_School_PCA = fit(MixedModel, f_School, dat; contrasts=contr4);\n\nMinimizing 740   Time: 0:00:00 ( 0.79 ms/it)\n  objective:  13821.183521613766\n\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.039662\n0.199154\n\n\n\n\n\n\n\n\nTest: Star_r\n0.160407\n0.400509\n+0.02\n\n\n\n\n\n\n\nTest: S20_r\n0.171273\n0.413851\n-0.05\n-0.85\n\n\n\n\n\n\nTest: SLJ\n0.216100\n0.464865\n-0.12\n+0.33\n-0.70\n\n\n\n\n\nTest: BPT\n0.103593\n0.321859\n-0.28\n-0.54\n+0.64\n-0.75\n\n\n\n\na1\n0.039032\n0.197564\n-0.26\n+0.35\n-0.69\n+0.53\n-0.02\n\n\nResidual\n\n0.853881\n0.924057\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0396656\n0.1991621\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0400990\n0.2002473\n+0.02\n\n\n\n\n\n\n\nTest: S20_r\n0.0078362\n0.0885222\n-0.06\n-0.57\n\n\n\n\n\n\nTest: SLJ\n0.0067759\n0.0823159\n-0.20\n+0.16\n-0.65\n\n\n\n\n\nTest: BPT\n0.0018799\n0.0433578\n-0.65\n-0.63\n+0.13\n+0.04\n\n\n\n\na1\n0.0390215\n0.1975386\n-0.26\n+0.35\n-0.81\n+0.32\n+0.34\n\n\nResidual\n\n0.8538781\n0.9240553\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.039676\n0.199188\n\n\n\n\n\n\n\n\nTest: BPT-other\n0.754466\n0.868600\n-0.65\n\n\n\n\n\n\n\nTest: Star-End\n0.160574\n0.400716\n+0.02\n-0.63\n\n\n\n\n\n\nTest: S20-Star\n0.171495\n0.414120\n-0.05\n+0.39\n-0.85\n\n\n\n\n\nTest: SLJ-S20\n0.216005\n0.464764\n-0.12\n-0.02\n+0.33\n-0.70\n\n\n\n\na1\n0.039008\n0.197505\n-0.26\n+0.34\n+0.35\n-0.69\n+0.53\n\n\nResidual\n\n0.853780\n0.924002\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.039604\n0.199006\n\n\n\n\n\n\n\n\nTest: c5.1\n0.026217\n0.161916\n-1.00\n\n\n\n\n\n\n\nTest: c234.15\n1.932392\n1.390105\n+0.24\n-0.24\n\n\n\n\n\n\nTest: c2.34\n0.366909\n0.605730\n+0.14\n-0.14\n+0.68\n\n\n\n\n\nTest: c3.4\n0.217656\n0.466536\n+0.14\n-0.14\n-0.25\n-0.19\n\n\n\n\na1\n0.037097\n0.192607\n-0.29\n+0.29\n-0.05\n+0.56\n-0.56\n\n\nResidual\n\n0.854044\n0.924145\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nbegin\n  f_School0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School)\n  m_School_SDC0 = fit(MixedModel, f_School0, dat; contrasts=contr1)\n  m_School_HeC0 = fit(MixedModel, f_School0, dat; contrasts=contr2)\n  m_School_HyC0 = fit(MixedModel, f_School0, dat; contrasts=contr3)\n  m_School_PCA0 = fit(MixedModel, f_School0, dat; contrasts=contr4)\n  #\n  zcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods2 = [\n    m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n  ]\n  gof_summary2 = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM2,\n      dof=dof.(mods2),\n      deviance=deviance.(mods2),\n      AIC=aic.(mods2),\n      BIC=bic.(mods2),\n    ),\n    :deviance,\n  )\nend\n\n\n4 rows √ó 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641PCA02713839.813893.814069.72HeC02713843.413897.414073.33HyC02713854.713908.714084.74SDC02713859.313913.314089.2\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting."
  },
  {
    "objectID": "contrasts_fggk21.html#thats-it",
    "href": "contrasts_fggk21.html#thats-it",
    "title": "Mixed Models Tutorial: Contrast Coding",
    "section": "5. That‚Äôs it",
    "text": "5. That‚Äôs it\nThat‚Äôs it for this tutorial. It is time to try your own contrast coding. You can use these data; there are many alternatives to set up hypotheses for the five tests. Of course and even better, code up some contrasts for data of your own.\nHave fun!"
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Code\nusing Arrow\nusing Chain\nusing DataFrames\nusing MixedModels\nusing ProgressMeter\nusing StatsBase\nusing StatsModels\nusing StatsModels: ContrastsCoding\n\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "contrasts_kwdyz11.html#sec-data",
    "href": "contrasts_kwdyz11.html#sec-data",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Example data",
    "text": "Example data\nWe take the KWDYZ dataset (Kliegl et al., 2010). This is an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Interestingly, a different theoretical perspective, derived from feature overlap, leads to a different set of contrasts. Can the results refute one of the theoretical perspectives?\nWe also have a dataset from a replication and extension of this study (Kliegl, Kuschela, & Laubrock, 2015). Both data sets are available in R-package RePsychLing"
  },
  {
    "objectID": "contrasts_kwdyz11.html#sec-preprocessing",
    "href": "contrasts_kwdyz11.html#sec-preprocessing",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Preprocessing",
    "text": "Preprocessing\n\ndat1 = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select!(:subj => :Subj, :tar => :CTR, :rt)\nend\ncellmeans = combine(\n  groupby(dat1, [:CTR]),\n  :rt => mean,\n  :rt => std,\n  :rt => length,\n  :rt => (x -> std(x) / sqrt(length(x))) => :rt_semean,\n)\n\n\n4 rows √ó 5 columnsCTRrt_meanrt_stdrt_lengthrt_semeanStringFloat64Float64Int64Float641val358.03283.4581201410.5880692sod391.26792.66228631.731773dos405.14692.689328431.738374dod402.395.391428631.78278"
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "SeqDiffCoding",
    "text": "SeqDiffCoding\nThis contrast corresponds to MASS::contr.sdif() in R.\n\nform = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\nlevels = [\"val\", \"sod\", \"dos\", \"dod\"]\nm1 = let\n  contrasts = Dict(\n    :CTR => SeqDiffCoding(; levels), :Subj => Grouping()\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\nMinimizing 206   Time: 0:00:00 ( 2.19 ms/it)\n  objective:  325809.5493494844\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0918\n54.96\n<1e-99\n55.2038\n\n\nCTR: sod\n33.7817\n3.2875\n10.28\n<1e-24\n23.2495\n\n\nCTR: dos\n13.9852\n2.3057\n6.07\n<1e-08\n10.7538\n\n\nCTR: dod\n-2.7469\n2.2145\n-1.24\n0.2148\n9.5132\n\n\nResidual\n69.8348"
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "HypothesisCoding",
    "text": "HypothesisCoding\nThis contrast corresponds to MASS::contr.sdif() in R. A general solution (not inverse of last contrast)\n\nm1b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        0 -1 1 0\n        0 0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0920\n54.95\n<1e-99\n55.2051\n\n\nCTR: spt\n33.7817\n3.2872\n10.28\n<1e-24\n23.2470\n\n\nCTR: obj\n13.9852\n2.3058\n6.07\n<1e-08\n10.7543\n\n\nCTR: grv\n2.7469\n2.2141\n1.24\n0.2147\n9.5085\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nControlling the ordering of levels for contrasts:\n\nkwarg levels to order the levels; the first is set as the baseline.\nkwarg base= to fix the baseline level.\n\nThe assignment of random factors such as Subj to Grouping() is only necessary when the sample size is very large and leads to an out-of-memory error; it is included only in the first example for reference."
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "DummyCoding",
    "text": "DummyCoding\nThi contrast corresponds to contr.treatment() in R\n\nm2 = let\n  contrasts = Dict(:CTR => DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n358.0914\n6.1541\n58.19\n<1e-99\n47.9111\n\n\nCTR: dod\n45.0200\n4.3636\n10.32\n<1e-24\n32.2914\n\n\nCTR: dos\n47.7669\n3.5566\n13.43\n<1e-40\n25.5369\n\n\nCTR: sod\n33.7817\n3.2873\n10.28\n<1e-24\n23.2474\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nThis contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM."
  },
  {
    "objectID": "contrasts_kwdyz11.html#ychycaeitcoding",
    "href": "contrasts_kwdyz11.html#ychycaeitcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "YchycaeitCoding",
    "text": "YchycaeitCoding\nThe contrasts returned by DummyCoding may be what you want. Can‚Äôt we have them, but also the GM rather than the mean of the base level? Yes, we can! I call this ‚ÄúYou can have your cake and it eat, too‚Äù-Coding (YchycaeitCoding).\n\nm2b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0917\n54.96\n<1e-99\n55.2029\n\n\nCTR: sod\n33.7817\n3.2876\n10.28\n<1e-24\n23.2504\n\n\nCTR: dos\n47.7669\n3.5569\n13.43\n<1e-40\n25.5396\n\n\nCTR: dod\n45.0200\n4.3641\n10.32\n<1e-24\n32.2955\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nJust relevel the factor or move the column with -1s for a different base."
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding---corresponds-to-contr.sum-in-r",
    "href": "contrasts_kwdyz11.html#effectscoding---corresponds-to-contr.sum-in-r",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "EffectsCoding - corresponds to contr.sum() in R",
    "text": "EffectsCoding - corresponds to contr.sum() in R\n\nm3 = let\n  contrasts = Dict(:CTR => EffectsCoding(; base=\"dod\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n<1e-99\n55.2007\n\n\nCTR: dos\n16.1248\n1.4404\n11.19\n<1e-28\n7.3321\n\n\nCTR: sod\n2.1396\n1.3338\n1.60\n0.1087\n6.0078\n\n\nCTR: val\n-31.6422\n2.6422\n-11.98\n<1e-32\n19.9501\n\n\nResidual\n69.8348"
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "HelmertCoding",
    "text": "HelmertCoding\n\nm4 = let\n  contrasts = Dict(:CTR => HelmertCoding())\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0907\n54.96\n<1e-99\n55.1949\n\n\nCTR: dos\n1.3735\n1.1071\n1.24\n0.2148\n4.7550\n\n\nCTR: sod\n-4.2039\n0.6842\n-6.14\n<1e-09\n3.3484\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n<1e-32\n6.6502\n\n\nResidual\n69.8349"
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Reverse HelmertCoding",
    "text": "Reverse HelmertCoding\n\nm4b = let\n  levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR => HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0907\n54.96\n<1e-99\n55.1949\n\n\nCTR: dos\n1.3735\n1.1071\n1.24\n0.2148\n4.7550\n\n\nCTR: sod\n-4.2039\n0.6842\n-6.14\n<1e-09\n3.3484\n\n\nCTR: val\n-10.5474\n0.8808\n-11.98\n<1e-32\n6.6502\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nHelmert contrasts are othogonal."
  },
  {
    "objectID": "contrasts_kwdyz11.html#anovacoding---anova-contrasts-are-orthogonal.",
    "href": "contrasts_kwdyz11.html#anovacoding---anova-contrasts-are-orthogonal.",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "AnovaCoding - Anova contrasts are orthogonal.",
    "text": "AnovaCoding - Anova contrasts are orthogonal.\n\nA(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n<1e-99\n55.2001\n\n\nCTR: A\n59.0052\n5.1829\n11.38\n<1e-29\n36.2105\n\n\nCTR: B\n31.0348\n4.6752\n6.64\n<1e-10\n31.7158\n\n\nCTR: AxB\n-36.5287\n3.0929\n-11.81\n<1e-31\n16.0071\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\nA(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1\n\n\nA(2) x B(2) x C(3)\nTO BE DONE"
  },
  {
    "objectID": "contrasts_kwdyz11.html#nestedcoding",
    "href": "contrasts_kwdyz11.html#nestedcoding",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "NestedCoding",
    "text": "NestedCoding\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\nm8 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1 0 0\n        0 0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0909\n54.96\n<1e-99\n55.1967\n\n\nCTR: do_so\n59.0052\n5.1824\n11.39\n<1e-29\n36.2060\n\n\nCTR: spt\n33.7817\n3.2872\n10.28\n<1e-24\n23.2467\n\n\nCTR: grv\n2.7470\n2.2143\n1.24\n0.2148\n9.5110\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "href": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Other orthogonal contrasts",
    "text": "Other orthogonal contrasts\nFor factors with more than four levels there are many options for specifying orthogonal contrasts as long as one proceeds in a top-down strictly hiearchical fashion.\nSuppose you have a factor with seven levels and let‚Äôs ignore shifting colummns. In this case, you have six options for the first contrast, that is 6 vs.¬†1, 5 vs.2 , 4 vs.¬†3, 3 vs.¬†4, 2 vs.¬†5, and 1 vs.¬†6 levels. Then, you specify orthogonal contrasts for partitions with more than 2 elements and so on. That is, you don‚Äôt specify a contrast that crosses an earlier partition line.\nIn the following example, after an initial 4 vs 3 partitioning of levels, we specify AnovaCoding for the left and HelmertCoding for the right partition.\n\ncontrasts = Dict(\n  :CTR => HypothesisCoding(\n    [\n      -1/4 -1/4 -1/4 -1/4 +1/3 +1/3 +1/3\n      -1/2 -1/2 +1/2 +1/2 0 0 0\n      -1/2 +1/2 -1/2 +1/2 0 0 0\n      +1/2 -1/2 -1/2 +1/2 0 0 0\n      0 0 0 0 -1 +1 0\n      0 0 0 0 -1/2 -1/2 1\n    ];\n    levels=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\"],\n    labels=[\"c567.1234\", \"B\", \"C\", \"BxC\", \"c6.5\", \"c6.56\"],\n  ),\n);\n\nThere are two rules that hold for all orthogonal contrasts:\n\nThe weights within rows sum to zero.\nFor all pairs of rows, the sum of the products of weights in the same columns sums to zero."
  },
  {
    "objectID": "contrasts_kwdyz11.html#standard-contrasts",
    "href": "contrasts_kwdyz11.html#standard-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Standard contrasts",
    "text": "Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "href": "contrasts_kwdyz11.html#exotic-contrasts-rk-well",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)",
    "text": "‚ÄúExotic‚Äù contrasts (rk: well ‚Ä¶)\nWe also provide HelmertCoding and SeqDiffCoding (corresponding to base R‚Äôs contr.helmert() and MASS::contr.sdif())."
  },
  {
    "objectID": "contrasts_kwdyz11.html#manual-contrasts",
    "href": "contrasts_kwdyz11.html#manual-contrasts",
    "title": "Contrast Coding of Visual Attention Effects",
    "section": "Manual contrasts",
    "text": "Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it‚Äôs good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they‚Äôll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information)."
  },
  {
    "objectID": "largescaledesigned.html",
    "href": "largescaledesigned.html",
    "title": "SMLP2022",
    "section": "",
    "text": "Load the packages to be used.\n\n\nCode\nusing AlgebraOfGraphics\nusing Arrow\nusing CairoMakie\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing Effects\nusing MixedModels\nusing MixedModelsMakie\nusing ProgressMeter\nusing StandardizedPredictors\nusing StatsBase\n\ndatadir(paths::AbstractString...) = joinpath(@__DIR__, \"data\", paths...)\nCairoMakie.activate!(; type=\"svg\");\nProgressMeter.ijulia_behavior(:clear);\n\n\nThe English Lexicon Project (Balota et al., 2007) was a large-scale multicenter study to examine properties of English words. It incorporated both a lexical decision task and a word recognition task. Different groups of subjects participated in the different tasks."
  },
  {
    "objectID": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "href": "largescaledesigned.html#trial-level-data-from-the-ldt",
    "title": "SMLP2022",
    "section": "Trial-level data from the LDT",
    "text": "Trial-level data from the LDT\nIn the lexical decision task the study participant is shown a character string, under carefully controlled conditions, and responds according to whether they identify the string as a word or not. Two responses are recorded: whether the choice of word/non-word is correct and the time that elapsed between exposure to the string and registering a decision.\nSeveral covariates, some relating to the subject and some relating to the target, were recorded. Initially we consider only the trial-level data.\n\nldttrial = Arrow.Table(datadir(\"ELP_ldt_trial.arrow\"))\n\nArrow.Table with 2745952 rows, 5 columns, and schema:\n :subj  Int16\n :seq   Int16\n :acc   Union{Missing, Bool}\n :rt    Int16\n :item  String\n\nwith metadata given by a Base.ImmutableDict{String, String} with 3 entries:\n  \"title\"     => \"Trial-level data from Lexical Discrimination Task in the Engl‚Ä¶\n  \"reference\" => \"Balota et al. (2007), The English Lexicon Project, Behavior R‚Ä¶\n  \"source\"    => \"https://osf.io/n63s2\"\n\n\nThe two response variables are acc - the accuracy of the response - and rt, the response time in milliseconds. There is one trial-level covariate, seq, the sequence number of the trial within subj. Each subject participated in two sessions on different days, with 2000 trials recorded on the first day.\nNotice the metadata with a citation and a URL for the OSF project.\nWe convert to a DataFrame and add a Boolean column s2 which is true for trials in the second session.\n\nldttrial = @transform!(DataFrame(ldttrial), :s2 = :seq > 2000)\ndescribe(ldttrial)\n\n\n6 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6s20.40712800.010Bool"
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialexplore",
    "href": "largescaledesigned.html#sec-ldtinitialexplore",
    "title": "SMLP2022",
    "section": "Initial data exploration",
    "text": "Initial data exploration\nFrom the basic summary of ldttrial we can see that there are some questionable response times ‚Äî negative values and values over 32 seconds.\nBecause of obvious outliers we will use the median response time, which is not strongly influenced by outliers, rather than the mean response time when summarizing by item or by subject.\nAlso, there are missing values of the accuracy. We should check if these are associated with particular subjects or particular items.\n\nSummaries by item\nTo summarize by item we group the trials by item and use combine to produce the various summary statistics. As we will create similar summaries by subject, we incorporate an ‚Äòi‚Äô in the names of these summaries (and an ‚Äòs‚Äô in the name of the summaries by subject) to be able to identify the grouping used.\n\nbyitem = @chain ldttrial begin\n  groupby(:item)\n  @combine(\n    :ni = length(:acc),               # no. of obs\n    :imiss = count(ismissing, :acc),  # no. of missing acc\n    :iacc = count(skipmissing(:acc)), # no. of accurate\n    :imedianrt = median(:rt),\n  )\n  @transform!(\n    :wrdlen = Int8(length(:item)),\n    :ipropacc = :iacc / :ni\n  )\nend\n\n\n80,962 rows √ó 7 columnsitemniimissiaccimedianrtwrdlenipropaccStringInt64Int64Int64Float64Int8Float641a35026743.010.7428572e35019824.010.5428573aah34021770.530.6176474aal34032702.530.9411765Aaron33031625.050.9393946Aarod33023810.050.696977aback34015710.050.4411768ahack34034662.051.09abacus34017671.560.510alacus34029640.060.85294111abandon34032641.070.94117612acandon34033725.570.97058813abandoned34031667.590.91176514adandoned34011760.590.32352915abandoning34034662.0101.016abantoning34030848.5100.88235317abandonment35035734.0111.018apandonment35030817.0110.85714319abase34123750.550.67647120abose34023805.550.67647121abasement33017850.090.51515222afasement33030649.090.90909123abash32022727.550.687524adash32025784.550.7812525abate34024687.050.70588226abape34032675.050.94117627abated34023775.060.67647128agated34014897.560.41176529abbess3407837.560.20588230abbass34028788.060.823529‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nIt can be seen that the items occur in word/nonword pairs and the pairs are sorted alphabetically by the word in the pair (ignoring case). We can add the word/nonword status for the items as\n\nbyitem.isword = isodd.(eachindex(byitem.item))\ndescribe(byitem)\n\n\n8 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1itemAarodzuss0String2ni33.91663034.0370Int643imiss0.016921500.020Int644iacc29.0194031.0370Int645imedianrt753.069458.0737.51691.00Float646wrdlen7.998818.0210Int87ipropacc0.8556160.00.9117651.00Float648isword0.500.510Bool\n\n\nThis table shows that some of the items were never identified correctly. These are\n\nfilter(:iacc => iszero, byitem)\n\n\n9 rows √ó 8 columnsitemniimissiaccimedianrtwrdlenipropacciswordStringInt64Int64Int64Float64Int8Float64Bool1baobab3400616.560.012haulage3400708.570.013leitmotif3500688.090.014miasmal3500774.070.015peahen3400684.060.016plosive3400663.070.017plugugly3300709.080.018poshest3400740.070.019servo3300697.050.01\n\n\nNotice that these are all words but somewhat obscure words such that none of the subjects exposed to the word identified it correctly.\nWe can incorporate characteristics like wrdlen and isword back into the original trial table with a ‚Äúleft join‚Äù. This operation joins two tables by values in a common column. It is called a left join because the left (or first) table takes precedence, in the sense that every row in the left table is present in the result. If there is no matching row in the second table then missing values are inserted for the columns from the right table in the result.\n\ndescribe(\n  leftjoin!(\n    ldttrial,\n    select(byitem, :item, :wrdlen, :isword);\n    on=:item,\n  ),\n)\n\n\n8 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6s20.40712800.010Bool7wrdlen7.9983518.0210Union{Missing, Int8}8isword0.49999500.010Union{Missing, Bool}\n\n\nNotice that the wrdlen and isword variables in this table allow for missing values, because they are derived from the second argument, but there are no missing values for these variables. If there is no need to allow for missing values, there is a slight advantage in disallowing them in the element type, because the code to check for and handle missing values is not needed.\nThis could be done separately for each column or for the whole data frame, as in\n\ndescribe(disallowmissing!(ldttrial; error=false))\n\n\n8 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64Type1subj409.311409.08160Int162seq1687.2111687.033740Int163acc0.8560401.011370Union{Missing, Bool}4rt846.325-16160732.0320610Int165itemAarodzuss0String6s20.40712800.010Bool7wrdlen7.9983518.0210Int88isword0.49999500.010Bool\n\n\n\n\n\n\n\n\nNamed argument ‚Äúerror‚Äù\n\n\n\n\n\nThe named argument error=false is required because there is one column, acc, that does incorporate missing values. If error=false were not given then the error thrown when trying to disallowmissing on the acc column would be propagated and the top-level call would fail.\n\n\n\nA barchart of the word length counts, Figure¬†1, shows that the majority of the items are between 3 and 14 characters.\n\n\nCode\nlet\n  wlen = 1:21\n  draw(\n    data((; wrdlen=wlen, count=counts(byitem.wrdlen, wlen))) *\n    mapping(:wrdlen => \"Length of word\", :count) *\n    visual(BarPlot),\n  )\nend\n\n\n\n\n\nFigure¬†1: Histogram of word lengths in the items used in the lexical decision task.\n\n\n\n\nTo examine trends in accuracy by word length we create a plot of the response versus word length using just a scatterplot smoother. It would not be meaningful to plot the raw data because that would just provide horizontal lines at \\(\\pm 1\\). Instead we add the smoother to show the trend and omit the raw data points.\nThe resulting plot, Figure¬†2, shows the accuracy of identifying words is more-or-less constant at around 84%, but accuracy decreases with increasing word length for the nonwords.\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc))) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :acc => \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†2: Smoothed curves of accuracy versus word length in the lexical decision task.\n\n\n\n\nFigure¬†2 may be a bit misleading because the largest discrepancies in proportion of accurate identifications of words and nonwords occur for the longest words, of which there are few. Over 95% of the words are between 4 and 13 characters in length\n\ncount(x -> 4 ‚â§ x ‚â§ 13, byitem.wrdlen) / nrow(byitem)\n\n0.9654899829549666\n\n\nIf we restrict the smoother curves to this range, as in Figure¬†3,\n\n\nCode\ndraw(\n  data(@subset(ldttrial, !ismissing(:acc), 4 ‚â§ :wrdlen ‚â§ 13)) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :acc => \"Accuracy\";\n    color=:isword,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†3: Smoothed curves of accuracy versus word length in the range 4 to 13 characters in the lexical decision task.\n\n\n\n\nthe differences are less dramatic.\nAnother way to visualize these results is by plotting the proportion accurate versus word-length separately for words and non-words with the area of each marker proportional to the number of observations for that combinations (Figure¬†4).\n\n\nCode\nlet\n  itemsummry = combine(\n    groupby(byitem, [:wrdlen, :isword]),\n    :ni => sum,\n    :imiss => sum,\n    :iacc => sum,\n  )\n  @transform!(\n    itemsummry,\n    :iacc_mean = :iacc_sum / (:ni_sum - :imiss_sum)\n  )\n  @transform!(itemsummry, :msz = sqrt((:ni_sum - :imiss_sum) / 800))\n  draw(\n    data(itemsummry) * mapping(\n      :wrdlen => \"Word length\",\n      :iacc_mean => \"Proportion accurate\";\n      color=:isword,\n      markersize=:msz,\n    );\n    figure=(; resolution=(800, 450)),\n  )\nend\n\n\n\n\n\nFigure¬†4: Proportion of accurate trials in the LDT versus word length separately for words and non-words. The area of the marker is proportional to the number of observations represented.\n\n\n\n\nThe pattern in the range of word lengths with non-negligible counts (there are points in the plot down to word lengths of 1 and up to word lengths of 21 but these points are very small) is that the accuracy for words is nearly constant at about 84% and the accuracy fof nonwords is slightly higher until lengths of 13, at which point it falls off a bit.\n\n\nSummaries by subject\nA summary of accuracy and median response time by subject\n\nbysubj = @chain ldttrial begin\n  groupby(:subj)\n  @combine(\n    :ns = length(:acc),               # no. of obs\n    :smiss = count(ismissing, :acc),  # no. of missing acc\n    :sacc = count(skipmissing(:acc)), # no. of accurate\n    :smedianrt = median(:rt),\n  )\n  @transform!(:spropacc = :sacc / :ns)\nend\n\n\n814 rows √ó 6 columnssubjnssmisssaccsmedianrtspropaccInt16Int64Int64Int64Float64Float6411337403158554.00.93598122337213031960.00.89887333337233006813.00.89145944337413062619.00.90752855337402574677.00.76289366337402927855.00.86751677337442877918.50.852697883372127311310.00.809905993374132669657.00.7910491010337402722757.00.8067581111337402894632.00.8577361212337442979692.00.88292813133374229801114.00.8832251414337412697603.00.7993481515337202957729.00.8769281616337402924710.00.8666271717337412947755.00.8734441818337402851617.00.8449911919337402890724.00.856552020337202905858.00.86150721213372030511041.00.9048042222337222756972.50.8173192323337432543629.50.7537052424337402995644.00.887672525337202988732.50.8861212626337403024830.00.89626627273374127741099.50.822172828337212898823.50.85943129293372030221052.50.8962043030337402946680.00.873148‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nshows some anomalies\n\ndescribe(bysubj)\n\n\n6 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolFloat64RealFloat64RealInt64DataType1subj409.3111409.58160Int162ns3373.4133703374.033740Int643smiss1.6830501.0220Int644sacc2886.3317272928.032860Int645smedianrt760.992205.0735.01804.00Float646spropacc0.8556130.5118550.8680310.9739180Float64\n\n\nFirst, some subjects are accurate on only about half of their trials, which is the proportion that would be expected from random guessing. A plot of the median response time versus proportion accurate, Figure¬†5, shows that the subjects with lower accuracy are some of the fastest responders, further indicating that these subjects are sacrificing accuracy for speed.\n\n\nCode\ndraw(\n  data(bysubj) *\n  mapping(\n    :spropacc => \"Proportion accurate\",\n    :smedianrt => \"Median response time (ms)\",\n  ) *\n  (smooth() + visual(Scatter));\n)\n\n\n\n\n\nFigure¬†5: Median response time versus proportion accurate by subject in the LDT.\n\n\n\n\nAs described in Balota et al. (2007), the participants performed the trials in blocks of 250 followed by a short break. During the break they were given feedback concerning accuracy and response latency in the previous block of trials. If the accuracy was less than 80% the participant was encouraged to improve their accuracy. Similarly, if the mean response latency was greater than 1000 ms, the participant was encouraged to decrease their response time. During the trials immediate feedback was given if the response was incorrect.\nNevertheless, approximately 15% of the subjects were unable to maintain 80% accuracy on their trials\n\ncount(<(0.8), bysubj.spropacc) / nrow(bysubj)\n\n0.15233415233415235\n\n\nand there is some association of faster response times with low accuracy. The majority of the subjects whose median response time is less than 500 ms. are accurate on less than 75% of their trials. Another way of characterizing the relationship is that none of the subjects with 90% accuracy or greater had a median response time less than 500 ms.\n\nminimum(@subset(bysubj, :spropacc > 0.9).smedianrt)\n\n505.0\n\n\nIt is common in analyses of response latency in a lexical discrimination task to consider only the latencies on correct identifications and to trim outliers. In Balota et al. (2007) a two-stage outlier removal strategy was used; first removing responses less than 200 ms or greater than 3000 ms then removing responses more than three standard deviations from the participant‚Äôs mean response.\nAs described in Section¬†2.2.3 we will analyze these data on a speed scale (the inverse of response time) using only the first-stage outlier removal of response latencies less than 200 ms or greater than 3000 ms. On the speed scale the limits are 0.333 per second up to 5 per second.\nTo examine the effects of the fast but inaccurate responders we will fit models to the data from all the participants and to the data from the 85% of participants who maintained an overall accuracy of 80% or greater.\n\npruned = @chain ldttrial begin\n  @subset(!ismissing(:acc), 200 ‚â§ :rt ‚â§ 3000,)\n  leftjoin!(select(bysubj, :subj, :spropacc); on=:subj)\n  dropmissing!\nend\nsize(pruned)\n\n(2714311, 9)\n\n\n\ndescribe(pruned)\n\n\n9 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64DataType1subj409.8021410.08160Int162seq1684.5611684.033740Int163acc0.85988401.010Bool4rt838.712200733.030000Int165itemAarodzuss0String6s20.4066300.010Bool7wrdlen7.9924418.0210Int88isword0.50012601.010Bool9spropacc0.8571690.5118550.8692950.9739180Float64\n\n\n\n\nChoice of response scale\nAs we have indicated, generally the response times are analyzed for the correct identifications only. Furthermore, unrealistically large or small response times are eliminated. For this example we only use the responses between 200 and 3000 ms.\nA density plot of the pruned response times, Figure¬†6, shows they are skewed to the right.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(:rt => \"Response time (ms.) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†6: Kernel density plot of the pruned response times (ms.) in the LDT.\n\n\n\n\nIn such cases it is common to transform the response to a scale such as the logarithm of the response time or to the speed of the response, which is the inverse of the response time.\nThe density of the response speed, in responses per second, is shown in Figure¬†7.\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :rt => (x -> 1000 / x) => \"Response speed (s‚Åª¬π) for correct responses\") *\n  AlgebraOfGraphics.density();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure¬†7: Kernel density plot of the pruned response speed in the LDT.\n\n\n\n\nFigure¬†6 and Figure¬†7 indicate that it may be more reasonable to establish a lower bound of 1/3 second (333 ms) on the response latency, corresponding to an upper bound of 3 per second on the response speed. However, only about one half of one percent of the correct responses have latencies in the range of 200 ms. to 333 ms.\n\ncount(\n  r -> !ismissing(r.acc) && 200 < r.rt < 333,\n  eachrow(ldttrial),\n) / count(!ismissing, ldttrial.acc)\n\n0.005867195806137328\n\n\nso the exact position of the lower cut-off point on the response latencies is unlikely to be very important.\n\n\n\n\n\n\nUsing inline transformations vs defining new columns\n\n\n\n\n\nIf you examine the code for (fit-elpldtspeeddens?), you will see that the conversion from rt to speed is done inline rather than creating and storing a new variable in the DataFrame.\nI prefer to keep the DataFrame simple with the integer variables (e.g.¬†:rt) if possible.\nI recommend using the StandardizedPredictors.jl capabilities to center numeric variables or convert to zscores.\n\n\n\n\n\nTransformation of response and the form of the model\nAs noted in Box & Cox (1964), a transformation of the response that produces a more Gaussian distribution often will also produce a simpler model structure. For example, Figure¬†8 shows the smoothed relationship between word length and response time for words and non-words separately,\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :rt => \"Response time (ms)\";\n    :color => :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure¬†8: Scatterplot smooths of response time versus word length in the LDT.\n\n\n\n\nand Figure¬†9 shows the similar relationships for speed\n\n\nCode\ndraw(\n  data(pruned) *\n  mapping(\n    :wrdlen => \"Word length\",\n    :rt => (x -> 1000/x) => \"Speed of response (s‚Åª¬π)\";\n    :color => :isword,\n  ) *\n  smooth();\n)\n\n\n\n\n\nFigure¬†9: Scatterplot smooths of response speed versus word length in the LDT.\n\n\n\n\nFor the most part the smoother lines in Figure¬†9 are reasonably straight. The small amount of curvature is associated with short word lengths, say less than 4 characters, of which there are comparatively few in the study.\nFigure¬†10 shows a ‚Äúviolin plot‚Äù - the empirical density of the response speed by word length separately for words and nonwords. The lines on the plot are fit by linear regression.\n\n\nCode\nlet\n  plt = data(@subset(pruned, :wrdlen > 3, :wrdlen < 14))\n  plt *= mapping(\n    :wrdlen => \"Word length\",\n    :rt => (x -> 1000/x) => \"Speed of response (s‚Åª¬π)\",\n    color=:isword,\n    side=:isword,\n  )\n  plt *= (visual(Violin) + linear(; interval=:confidence))\n  draw(plt, axis=(; limits=(nothing, (0.0, 2.8))))\nend\n\n\n\n\n\nFigure¬†10: Empirical density of response speed versus word length by word/non-word status, with lines fit by linear regression to each group."
  },
  {
    "objectID": "largescaledesigned.html#sec-ldtinitialmodel",
    "href": "largescaledesigned.html#sec-ldtinitialmodel",
    "title": "SMLP2022",
    "section": "Models with scalar random effects",
    "text": "Models with scalar random effects\nA major purpose of the English Lexicon Project is to characterize the items (words or nonwords) according to the observed accuracy of identification and to response latency, taking into account subject-to-subject variability, and to relate these to lexical characteristics of the items.\nIn Balota et al. (2007) the item response latency is characterized by the average response latency from the correct trials after outlier removal.\nMixed-effects models allow us greater flexibility and, we hope, precision in characterizing the items by controlling for subject-to-subject variability and for item characteristics such as word/nonword and item length.\nWe begin with a model that has scalar random effects for item and for subject and incorporates fixed-effects for word/nonword and for item length and for the interaction of these terms.\n\nEstablish the contrasts\nBecause there are a large number of items in the data set it is important to assign a Grouping() contrast to item (and, less importantly, to subj). For the isword factor we will use an EffectsCoding contrast with the base level as false. The non-words are assigned -1 in this contrast and the words are assigned +1. The wrdlen covariate is on its original scale but centered at 8 characters.\nThus the (Intercept) coefficient is the predicted speed of response for a typical subject and typical item (without regard to word/non-word status) of 8 characters.\nSet these contrasts\n\ncontrasts = Dict(\n  :subj => Grouping(),\n  :item => Grouping(),\n  :isword => EffectsCoding(; base=false),\n  :wrdlen => Center(8),\n)\n\nDict{Symbol, Any} with 4 entries:\n  :item   => Grouping()\n  :wrdlen => Center(8)\n  :isword => EffectsCoding(false, nothing)\n  :subj   => Grouping()\n\n\nand fit a first model with simple, scalar, random effects for subj and item.\n\nelm01 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  fit(MixedModel, form, pruned; contrasts)\nend\n\nMinimizing 53    Time: 0:00:06 ( 0.13  s/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n<1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n<1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nThe predicted response speed by word length and word/nonword status can be summarized as\n\neffects(Dict(:isword => [false, true], :wrdlen => 4:2:12), elm01)\n\n\n10 rows √ó 6 columnswrdlenisword1000 / rterrlowerupperInt64BoolFloat64Float64Float64Float641401.465550.009031111.456521.474582601.389470.008981241.380491.398453801.313380.008964591.304421.3223541001.23730.008981341.228321.2462851201.161210.009031291.152181.170256411.63510.00903111.626071.644137611.53670.008981241.527721.545698811.438310.008964591.429341.4472791011.339910.008981331.330921.34889101211.241510.009031281.232481.25054\n\n\nIf we restrict to only those subjects with 80% accuracy or greater the model becomes\n\nelm02 = let\n  form = @formula(\n    1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n  )\n  dat = @subset(pruned, :spropacc > 0.8)\n  fit(MixedModel, form, dat; contrasts)\nend\n\nMinimizing 53    Time: 0:00:03 (70.60 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3611\n0.0088\n153.98\n<1e-99\n0.1247\n0.2318\n\n\nisword: true\n0.0656\n0.0005\n133.73\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0444\n0.0002\n-222.65\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0057\n0.0002\n-28.73\n<1e-99\n\n\n\n\nResidual\n0.3342\n\n\n\n\n\n\n\n\n\n\n\neffects(Dict(:isword => [false, true], :wrdlen => 4:2:12), elm02)\n\n\n10 rows √ó 6 columnswrdlenisword1000 / rterrlowerupperInt64BoolFloat64Float64Float64Float641401.450360.008924661.441441.459292601.372970.0088711.36411.381843801.295570.008853081.286721.3044341001.218180.00887111.209311.2270551201.140780.008924861.131861.149716411.627350.008924651.618421.636277611.527020.0088711.518151.535898811.42670.008853071.417841.4355591011.326370.008871091.31751.33524101211.226050.008924831.217121.23497\n\n\nThe differences in the fixed-effects parameter estimates between a model fit to the full data set and one fit to the data from accurate responders only, are small.\nHowever, the random effects for the item, while highly correlated, are not perfectly correlated.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndisallowmissing!(\n  leftjoin!(\n    byitem,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:item]), [:item, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:item]), [:item, :elm02]);\n      on=:item,\n    ),\n    on=:item,\n  ),\n)\ndisallowmissing!(\n  leftjoin!(\n    bysubj,\n    leftjoin!(\n      rename!(DataFrame(raneftables(elm01)[:subj]), [:subj, :elm01]),\n      rename!(DataFrame(raneftables(elm02)[:subj]), [:subj, :elm02]);\n      on=:subj,\n    ),\n    on=:subj,\n  ); error=false,\n)\ndraw(\n  data(byitem) * mapping(\n    :elm01 => \"Conditional means of item random effects for model elm01\",\n    :elm02 => \"Conditional means of item random effects for model elm02\";\n    color=:isword,\n  );\n  axis=(; width=600, height=600),\n)\n\n\n\n\n\nFigure¬†11: Conditional means of scalar random effects for item in model elm01, fit to the pruned data, versus those for model elm02, fit to the pruned data with inaccurate subjects removed.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdjust the alpha on Figure¬†11.\n\n\nFigure¬†11 is exactly of the form that would be expected in a sample from a correlated multivariate Gaussian distribution. The correlation of the two sets of conditional means is about 96%.\n\ncor(Matrix(select(byitem, :elm01, :elm02)))\n\n2√ó2 Matrix{Float64}:\n 1.0       0.958655\n 0.958655  1.0\n\n\nThese models take only a few seconds to fit on a modern laptop computer, which is quite remarkable given the size of the data set and the number of random effects.\nThe amount of time to fit more complex models will be much greater so we may want to move those fits to more powerful server computers. We can split the tasks of fitting and analyzing a model between computers by saving the optimization summary after the model fit and later creating the MixedModel object followed by restoring the optsum object.\n\nsaveoptsum(\"./fits/elm01.json\", elm01);\n\n\nelm01a = restoreoptsum!(\n  let\n    form = @formula(\n      1000 / rt ~ 1 + isword * wrdlen + (1 | item) + (1 | subj)\n    )\n    MixedModel(form, pruned; contrasts)\n  end,\n  \"./fits/elm01.json\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nœÉ_item\nœÉ_subj\n\n\n\n\n(Intercept)\n1.3758\n0.0090\n153.69\n<1e-99\n0.1185\n0.2550\n\n\nisword: true\n0.0625\n0.0005\n131.35\n<1e-99\n\n\n\n\nwrdlen(centered: 8)\n-0.0436\n0.0002\n-225.38\n<1e-99\n\n\n\n\nisword: true & wrdlen(centered: 8)\n-0.0056\n0.0002\n-28.83\n<1e-99\n\n\n\n\nResidual\n0.3781\n\n\n\n\n\n\n\n\n\n\nOther covariates associated with the item are available as\n\nelpldtitem = DataFrame(Arrow.Table(datadir(\"ELP_ldt_item.arrow\")))\ndescribe(elpldtitem)\n\n\n9 rows √ó 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion‚Ä¶AnyUnion‚Ä¶AnyInt64Type1itemAarodzuss0String2Ortho_N1.5330901.0250Int83BG_Sum13938.41113026.059803177Union{Missing, Int32}4BG_Mean1921.255.51907.06910.0177Union{Missing, Float32}5BG_Freq_By_Pos2043.0801928.069854Union{Missing, Int16}6itemno40481.5140481.5809620Int327isword0.500.510Bool8wrdlen7.998818.0210Int89pairno20241.0120241.0404810Int32\n\n\nand those associated with the subject are\n\nelpldtsubj = DataFrame(Arrow.Table(datadir(\"ELP_ldt_subj.arrow\")))\ndescribe(elpldtsubj)\n\n\n20 rows √ó 7 columns (omitted printing of 2 columns)variablemeanminmedianmaxSymbolUnion‚Ä¶AnyAnyAny1subj409.3111409.58162univKansasWayne State3sexfm4DOB1938-06-071984-11-145MEQ44.493219.044.075.06vision5.5116906.077hearing5.8610106.078educatn8.89681112.0289ncorrct29.8505530.04010rawscor31.99251332.04011vocabAge17.812310.317.821.012shipTime3.086103.0913readTime2.502150.02.015.014preshlth5.4870806.0715pasthlth4.9298905.0716S1start2001-03-16T13:49:272001-10-16T11:38:28.5002003-07-29T18:48:4417S2start2001-03-19T10:00:352001-10-19T14:24:19.5002003-07-30T13:07:4518MEQstrt2001-03-22T18:32:002001-10-23T11:26:132003-07-30T14:30:4919filename101DATA.LDTData998.LDT20frstLangEnglishother\n\n\nFor the simple model elm01 the estimated standard deviation of the random effects for subject is greater than that of the random effects for item, a common occurrence. A caterpillar plot, Figure¬†12,\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm01, :subj),\n)\n\n\n\n\n\nFigure¬†12: Conditional means and 95% prediction intervals for subject random effects in elm01.\n\n\n\n\nshows definite distinctions between subjects because the widths of the prediction intervals are small compared to the range of the conditional modes. Also, there is at least one outlier with a conditional mode over 1.0.\nFigure¬†13 is the corresponding caterpillar plot for model elm02 fit to the data with inaccurate responders eliminated.\n\n\nCode\nqqcaterpillar!(\n  Figure(resolution=(800, 650)),\n  ranefinfo(elm02, :subj),\n)\n\n\n\n\n\nFigure¬†13: Conditional means and 95% prediction intervals for subject random effects in elm02."
  },
  {
    "objectID": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "href": "largescaledesigned.html#random-effects-from-the-simple-model-related-to-covariates",
    "title": "SMLP2022",
    "section": "Random effects from the simple model related to covariates",
    "text": "Random effects from the simple model related to covariates\nThe random effects ‚Äúestimates‚Äù (technically they are ‚Äúconditional means‚Äù) from the simple model elm01 provide a measure of how much the item or subject differs from the population. (We use elm01 because the main difference between elm01 and elm02 are that some subjects were dropped before fitting elm02.)\nFor the item its length and word/non-word status have already been incorporated in the model. At this point the subjects are just being treated as a homogeneous population.\nThe random effects conditional means have been extracted and incorporated in the byitem and bysubj tables. Now add selected demographic and item-specific measures.\n\nitemextended = leftjoin(\n  byitem,\n  select(elpldtitem, 1:5);\n  on = :item,\n)\nsubjextended = leftjoin(\n  bysubj,\n  select(elpldtsubj, 1:3, :vocabAge);\n  on=:subj,\n)\n\n\n814 rows √ó 11 columns (omitted printing of 2 columns)subjnssmisssaccsmedianrtspropaccelm01elm02univInt16Int64Int64Int64Float64Float64Float64Float64?String?11337403158554.00.9359810.4114590.426624Morehead22337213031960.00.898873-0.30907-0.293732Morehead33337233006813.00.891459-0.153078-0.139436Morehead44337413062619.00.9075280.2130470.22754Morehead55337402574677.00.7628930.0850349missingMorehead66337402927855.00.867516-0.207356-0.192651Morehead77337442877918.50.852697-0.182201-0.166357Morehead883372127311310.00.809905-0.541434-0.526828Morehead993374132669657.00.7910490.154926missingMorehead1010337402722757.00.806758-0.0541104-0.0403266Morehead1111337402894632.00.8577360.2177340.231618Morehead1212337442979692.00.8829280.0623510.0770981Morehead13133374229801114.00.883225-0.409761-0.3956Morehead1414337412697603.00.7993480.298338missingMorehead1515337202957729.00.876928-0.001062740.0142299Morehead1616337402924710.00.8666270.03671310.0518282Morehead1717337412947755.00.873444-0.0599943-0.0443423Morehead1818337402851617.00.8449910.2235690.238685Morehead1919337402890724.00.85655-0.0001906040.0147752Morehead2020337202905858.00.861507-0.169734-0.155883Morehead21213372030511041.00.904804-0.31294-0.299952Morehead2222337222756972.50.817319-0.286105-0.271354Morehead2323337432543629.50.7537050.25627missingMorehead2424337402995644.00.887670.1654410.180714Morehead2525337202988732.50.886121-0.0395191-0.0242025Morehead2626337403024830.00.896266-0.160068-0.145719Morehead27273374127741099.50.82217-0.32906-0.314104Morehead2828337212898823.50.859431-0.179577-0.164645Morehead29293372030221052.50.896204-0.408285-0.393552Morehead3030337402946680.00.8731480.08560580.0998801Morehead‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ‚ãÆ\n\n\nAs shown in ?@fig-fig-elm01vocabage, there does not seem to be a strong relationship between vocabulary age and speed of response by subject.\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :vocabAge, :sex))) *\n  mapping(\n    :vocabAge => \"Vocabulary age (yr) of subject\",\n    :elm01 => \"Random effect in model elm01\";\n    color=:sex,\n  ) * visual(Scatter)\n)\n\n\n\n\n\nFigure¬†14: Random effect for subject in model elm01 versus vocabulary age\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm01, :univ))) *\n  mapping(\n    :elm01 => \"Random effect in model elm01\";\n    color=:univ => \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\nFigure¬†15: Estimated density of random effects for subject in model elm01 by university\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(subjextended, :elm02, :univ))) *\n  mapping(\n    :elm02 => \"Random effect in model elm02 (accurate responders only)\";\n    color=:univ => \"University\",\n  ) * AlgebraOfGraphics.density()\n)\n\n\n\n\n\nFigure¬†16: Estimated density of random effects for subject in model elm02, fit to accurate responders only, by university\n\n\n\n\n\n\nCode\ndraw(\n  data(dropmissing(select(itemextended, :elm01, :BG_Mean, :isword))) *\n  mapping(\n    :BG_Mean => \"Mean bigram frequency\",\n    :elm01 => \"Random effect in model elm01\";\n    color=:isword,\n  ) * visual(Scatter)\n)\n\n\n\n\n\nFigure¬†17: Random effect in model elm01 versus mean bigram frequency, by word/nonword status"
  }
]