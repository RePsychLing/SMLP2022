---
title: "Ã–zge Bakay: Missing Verb Illusion"
subtitle: "RePsychLing SMLP2022"
author: "Reinhold Kliegl"
date: "2022-08-27 (last revised: `r format(Sys.time())`)"
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options: 
  chunk_output_type: console
jupyter: julia-1.8
---

# Background

The experiment investigates whether there is missing-verb illusion in Turkish center-embeddings and whether case similarity has an effect on the presence of illusion. The experiment has a 2x2 design where the factors are Grammaticality (Grammatical vs. Ungrammatical) and Case Similarity (High vs. Low). Both factors varied within-subject and within-item. The primary dependent variable is a 5-point rating. 

The analysis includes data from 56 subject (after exclusion of one subject with low accuracy) who rated 24 critical items (6 in each of the 2 x 2 conditions; nobs=1344. Subjects also rated 36 filler items that are excluded here, but might be used to assess rating bias. 

With 56 (or 57) subjects and 24 items, assignment of items to conditions could not be counterbalanced across subjects. The assignment was not random because each item was seen by 20, 13, 13, and 11 subjects across the four conditions.

The gaol of the analysis is to test effects with a cumulative link mixed model for ordinal rating data,  basically an integrated set of logistic regressions, taking into account clustering of ratings by subjects and by items. 

# Setup

First attach the _MixedModels.jl_ package and other packages for plotting.
The _CairoMakie.jl_ package allows the Makie graphics system [@Danisch2021] to generate high quality static images. Activate that package with the SVG (Scalable Vector Graphics) backend.

```{julia}
#| label: packages

using AlgebraOfGraphics
using Arrow
using CairoMakie       # graphics back-end
using CategoricalArrays
using Chain
using DataFrames
using DataFrameMacros  # simplified dplyr-like data wrangling 
using KernelDensity    # density estimation
using MixedModels
using MixedModelsMakie # diagnostic plots
using ProgressMeter
using Random           # random number generators
using RCall            # call R from Julia
using StatsModels

using AlgebraOfGraphics: boxplot
using AlgebraOfGraphics: density

using MixedModelsMakie: qqnorm
using MixedModelsMakie: ridgeplot
using MixedModelsMakie: scatter
using MixedModelsMakie: caterpillar

ProgressMeter.ijulia_behavior(:clear);
CairoMakie.activate!(; type="svg");
```

+ The data are available as an arrow file. 
+ Most of preprocessing was done with R in RStudio.
+ Order of factor levels should be checked.

```{julia}
df = DataFrame(Arrow.Table("./data/Bakay_rating.arrow"));
@transform!(df, :Subj = @bycol categorical(:Subj))
@transform!(df, :Item = @bycol categorical(:Item))
@transform!(df, :Case = @bycol categorical(:Case))
@transform!(df, :Gram = @bycol categorical(:Gram))
@transform!(df, :tr_c = :trial - 30);

describe(df)
```

# Figures

## Case x Gram

```{julia}
#| label: fig1
#| fig-cap: "Case x Grammaticality interaction"
#| message: false

RCall.ijulia_setdevice(MIME("image/svg+xml"); width=10, height=10.0)
@rput df;

R"""
suppressWarnings(suppressMessages(library(tidyverse)))

# respecting color vision deficiency
cbPalette <- c( "#0072B2", "#D55E00", "#009E73", "#CC79A7",
                "#F0E442", "#56B4E9", "#999999", "#E69F00")

table1 <- 
  df |> 
  group_by(Subj, Case, Gram) |> 
  summarise(N=n(), rating=mean(rating)) |> 
  ungroup() |> 
  group_by(Case, Gram) |> 
  summarise(N=n(), rating_M=mean(rating), 
            rating_SD=sd(rating), rating_SE=rating_SD/sqrt(N))
 fig1 <-           
  table1 |> 
  mutate(Case=fct_rev(Case)) |> 
  ggplot(aes(x=Case, y=rating_M, group=Gram, color=Gram)) +
  geom_point(position=position_dodge(width=.1)) +
  geom_line(position=position_dodge(width=.1)) +
  geom_errorbar(aes(ymax=rating_M + 2*rating_SE,
                    ymin=rating_M - 2*rating_SE), width=.05,
                position=position_dodge(width=.1)) +
  scale_color_manual("", labels=c("grammatical", "ungrammatical"), values=cbPalette)+
  scale_y_continuous("Rating", limits=c(1,5)) +
  theme_bw() + theme(legend.position = c(.99, .99), legend.justification = c(.99,.99))

print(fig1)
""";
```

## Trial

```{julia}
#| label: fig2
#| fig-cap: _Figure 2_. Trial main effect with default smooth() parameters.
#| message: false

R"""

fig2 <-
  df |> 
  ggplot(aes(x=trial, y=rating)) +
  geom_smooth() +
  xlab("Trial") + 
  scale_y_continuous("Rating", limits=c(1,5)) +
  theme_bw() + theme(legend.position = "top")

print(fig2)
""";
```

## Trial x Case x Gram

```{julia}
#| label: fig3
#| fig-cap: _Figure 3_. Trial effect for four conditions with default smooth() parameters.
#| message: false

R"""
fig3 <-
  df |> 
  ggplot(aes(x=trial, y=rating, group=Case, color=Case)) +
  geom_smooth() +
  xlab("Trial") + 
  scale_y_continuous("Rating", limits=c(1,5)) +
  scale_colour_manual("Condition", values=cbPalette) +
  theme_bw() + theme(legend.position = "top")

print(fig3)
""";
```

Not much evidence for a trial effect.

# LMM

## Contrasts

```{julia}
#| label: contrasts

contrasts = merge(
      Dict(:Case => EffectsCoding(base= "Low"; levels=["Low", "High"])),
      Dict(:Gram => EffectsCoding(base= "Ungra"; levels=["Ungra", "Gra"])),
      Dict(nm => Grouping() for nm in (:Subj, :Item))
   );
```

## Varying only GM

```{julia}
#| label: m0

 m0 = let 
    form = @formula(rating ~ 1 + tr_c + Case * Gram + (1|Subj) + (1|Item));
    fit(MixedModel, form, df; contrasts);
  end;
  
m0
```

Remove trial covariate

```{julia}
#| label: m1

 m1 = let 
    form = @formula(rating ~ 1 + Case * Gram + (1|Subj) + (1|Item));
    fit(MixedModel, form, df; contrasts);
  end;

m1
```

We don't need trial as a covariate. Probably, we could also drop the interaction at least from the random-effect structure.

## Individual and item differences in main effects

Relative to the factorial design this is not the maximal CLMM; we do not include Subj- and Item-related VCs for the interaction term.

```{julia}
#| label: m2

 m2 = let 
    form = @formula(rating ~ 1 + Case * Gram + (1 + Case + Gram |Subj) + (1 + Case + Gram |Item));
    fit(MixedModel, form, df; contrasts);
  end;
display(issingular(m2))
display(m2.PCA[:Subj])
display(m2.PCA[:Item])
display(lrtest(m1, m2))

m2
```

The random-effect structure is overparaemterized. Actually, the program crashed.

## Zero-correlation parameters CLMM

```{julia}
#| label: m3

 m3 = let 
    form = @formula(rating ~ 1 + Case * Gram + zerocorr(1 + Case + Gram | Subj) + zerocorr(1 + Case + Gram | Item));
    fit(MixedModel, form, df; contrasts);
  end;
display(issingular(m3))
display(m3.PCA[:Subj])
display(m3.PCA[:Item])
display(lrtest(m1, m3, m2))

m3
```

Some of the VCs are very close to zero. Program crashed, again.

## Parsimonious CLMM

```{julia}
#| label: m4

 m4 = let 
    form = @formula(rating ~ 1 + Case * Gram + zerocorr(1 + Gram | Subj) + zerocorr(1 + Case | Item));
    fit(MixedModel, form, df; contrasts)
  end;
display(issingular(m4))
display(lrtest(m1, m4, m3, m2))

m4
```

Looking good!

## Extending the parsimonious CLMM with CPs

```{julia}
#| label: m5
#| eval: true

 m5 = let 
    form = @formula(rating ~ 1 + Case * Gram + (1 + Gram | Subj) + (1 + Case | Item)) ;
    fit(MixedModel, form, df; contrasts)
  end;
display(issingular(m5))
display(lrtest(m4, m5))

m5
```

Looking also good, but the two CPs are not significant.

# Summary

+ There is evidence for significant individual differences in the effect of Gram and in the Grand Mean
+ There is evidence for significant item differences in the effect of Case and in the Grand Mean
+ There is no evidence for correlation parameters.
+ There are two significant main effects of Case and Gram, irrespective of the complexity of the random-effect struture.

# Appendix

```{julia}
versioninfo()
```

